{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70077b26-fdb8-4fee-bb8a-4c7766c2f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d543650-b3b3-4d0a-924f-6ec5839ee1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6239327f-ac6e-43a2-a160-b22aef24bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b8da5c-5a17-4a73-a7b4-d2ada7dc5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b6dea2-affa-4443-ace1-b847d3ce8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1b89a4f-5c39-4fa5-9280-c3ad64a743fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15222d1a-2e7a-49ee-b557-9af666408205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43405e68-f273-4117-8de3-adce92843589",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db4d2705-b290-4a4b-913e-a8976fdaa152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219d471-fb11-4a96-a5a8-7f545e6db5c4",
   "metadata": {},
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb4ffb3b-9a75-4e9a-9f30-9a9082b7a239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309813  [   64/10000]\n",
      "loss: 2.298416  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 14.8%, Avg loss: 2.280054 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.282422  [   64/10000]\n",
      "loss: 2.258436  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 2.247402 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.255635  [   64/10000]\n",
      "loss: 2.220612  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 17.9%, Avg loss: 2.214829 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.227987  [   64/10000]\n",
      "loss: 2.182131  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 20.5%, Avg loss: 2.180535 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.199603  [   64/10000]\n",
      "loss: 2.140970  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 25.9%, Avg loss: 2.144957 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.170191  [   64/10000]\n",
      "loss: 2.098530  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.108165 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.139725  [   64/10000]\n",
      "loss: 2.054893  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 37.9%, Avg loss: 2.070144 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.107846  [   64/10000]\n",
      "loss: 2.009885  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 2.030650 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.074241  [   64/10000]\n",
      "loss: 1.963027  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 1.989385 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.038620  [   64/10000]\n",
      "loss: 1.914197  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.946396 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.001680  [   64/10000]\n",
      "loss: 1.863991  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.902105 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.962996  [   64/10000]\n",
      "loss: 1.812804  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.856679 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.922212  [   64/10000]\n",
      "loss: 1.760764  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.810225 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.879704  [   64/10000]\n",
      "loss: 1.707778  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.762945 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.835617  [   64/10000]\n",
      "loss: 1.654248  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 1.715167 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.790087  [   64/10000]\n",
      "loss: 1.600484  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 1.667020 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.743318  [   64/10000]\n",
      "loss: 1.546913  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.618720 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.695795  [   64/10000]\n",
      "loss: 1.493807  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 1.570528 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.648003  [   64/10000]\n",
      "loss: 1.441606  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 1.522742 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.600047  [   64/10000]\n",
      "loss: 1.390364  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.475583 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.552187  [   64/10000]\n",
      "loss: 1.340281  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.429271 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.504693  [   64/10000]\n",
      "loss: 1.291574  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.384039 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.457956  [   64/10000]\n",
      "loss: 1.244358  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 1.340143 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.412393  [   64/10000]\n",
      "loss: 1.198950  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.297781 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.368240  [   64/10000]\n",
      "loss: 1.155581  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 1.257113 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.325647  [   64/10000]\n",
      "loss: 1.114403  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 1.218259 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.284842  [   64/10000]\n",
      "loss: 1.075452  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 1.181254 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.245915  [   64/10000]\n",
      "loss: 1.038552  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 1.146122 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.208869  [   64/10000]\n",
      "loss: 1.003877  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 1.112850 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.173709  [   64/10000]\n",
      "loss: 0.971292  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 1.081383 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 1.140409  [   64/10000]\n",
      "loss: 0.940704  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 1.051651 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 1.108842  [   64/10000]\n",
      "loss: 0.912027  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 1.023567 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 1.078934  [   64/10000]\n",
      "loss: 0.885137  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.997035 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.050572  [   64/10000]\n",
      "loss: 0.859903  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.971966 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.023675  [   64/10000]\n",
      "loss: 0.836207  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.948264 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.998143  [   64/10000]\n",
      "loss: 0.813967  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.925837 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.973887  [   64/10000]\n",
      "loss: 0.793071  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.904606 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.950828  [   64/10000]\n",
      "loss: 0.773412  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.884492 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.928851  [   64/10000]\n",
      "loss: 0.754887  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.865420 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.907901  [   64/10000]\n",
      "loss: 0.737430  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.847320 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.887932  [   64/10000]\n",
      "loss: 0.720969  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.830130 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.868882  [   64/10000]\n",
      "loss: 0.705410  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.813789 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.850738  [   64/10000]\n",
      "loss: 0.690690  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.798240 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.833420  [   64/10000]\n",
      "loss: 0.676727  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.783432 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.816864  [   64/10000]\n",
      "loss: 0.663476  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.769319 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.801021  [   64/10000]\n",
      "loss: 0.650885  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.755856 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.785866  [   64/10000]\n",
      "loss: 0.638908  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.743002 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.771358  [   64/10000]\n",
      "loss: 0.627542  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.730721 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.757470  [   64/10000]\n",
      "loss: 0.616717  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.718977 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.744161  [   64/10000]\n",
      "loss: 0.606380  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.707738 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.731380  [   64/10000]\n",
      "loss: 0.596503  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.696973 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.719125  [   64/10000]\n",
      "loss: 0.587044  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.686655 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.707360  [   64/10000]\n",
      "loss: 0.577980  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.676756 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.696042  [   64/10000]\n",
      "loss: 0.569285  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.667252 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.685161  [   64/10000]\n",
      "loss: 0.560953  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.658119 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.674703  [   64/10000]\n",
      "loss: 0.552956  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.649338 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.664639  [   64/10000]\n",
      "loss: 0.545268  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.640891 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.654953  [   64/10000]\n",
      "loss: 0.537867  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.632758 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.645650  [   64/10000]\n",
      "loss: 0.530744  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.624920 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.636697  [   64/10000]\n",
      "loss: 0.523869  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.617362 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.628053  [   64/10000]\n",
      "loss: 0.517241  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.610069 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.619708  [   64/10000]\n",
      "loss: 0.510828  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.603031 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.611631  [   64/10000]\n",
      "loss: 0.504644  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.596235 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.603837  [   64/10000]\n",
      "loss: 0.498658  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.589668 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.596300  [   64/10000]\n",
      "loss: 0.492867  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.583318 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.589014  [   64/10000]\n",
      "loss: 0.487250  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.577174 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.581959  [   64/10000]\n",
      "loss: 0.481819  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.571224 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.575134  [   64/10000]\n",
      "loss: 0.476552  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.565460 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.568519  [   64/10000]\n",
      "loss: 0.471424  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.559877 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.562087  [   64/10000]\n",
      "loss: 0.466458  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.554464 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.555845  [   64/10000]\n",
      "loss: 0.461612  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.549210 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.549795  [   64/10000]\n",
      "loss: 0.456920  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.544112 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.543924  [   64/10000]\n",
      "loss: 0.452367  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.539163 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.538221  [   64/10000]\n",
      "loss: 0.447961  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.534358 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.532677  [   64/10000]\n",
      "loss: 0.443677  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.529690 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.527304  [   64/10000]\n",
      "loss: 0.439563  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.525153 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.522092  [   64/10000]\n",
      "loss: 0.435583  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.520744 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.517040  [   64/10000]\n",
      "loss: 0.431713  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.516455 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.512135  [   64/10000]\n",
      "loss: 0.427892  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.512280 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.507382  [   64/10000]\n",
      "loss: 0.424147  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.508216 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.502766  [   64/10000]\n",
      "loss: 0.420497  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.504261 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.498255  [   64/10000]\n",
      "loss: 0.416943  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.500406 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.493840  [   64/10000]\n",
      "loss: 0.413491  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.496654 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.489533  [   64/10000]\n",
      "loss: 0.410126  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.492999 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.485323  [   64/10000]\n",
      "loss: 0.406863  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.489439 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.481218  [   64/10000]\n",
      "loss: 0.403685  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.485970 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.477216  [   64/10000]\n",
      "loss: 0.400575  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.482587 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.473320  [   64/10000]\n",
      "loss: 0.397546  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.479286 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.469549  [   64/10000]\n",
      "loss: 0.394598  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.476064 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.465883  [   64/10000]\n",
      "loss: 0.391715  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.472922 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.462298  [   64/10000]\n",
      "loss: 0.388905  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.469855 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.458797  [   64/10000]\n",
      "loss: 0.386172  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.466863 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.455386  [   64/10000]\n",
      "loss: 0.383515  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.463943 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.452058  [   64/10000]\n",
      "loss: 0.380915  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.461092 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.448807  [   64/10000]\n",
      "loss: 0.378387  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.458308 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.445632  [   64/10000]\n",
      "loss: 0.375924  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.455589 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.442536  [   64/10000]\n",
      "loss: 0.373526  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.452931 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.439510  [   64/10000]\n",
      "loss: 0.371190  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.450333 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.436562  [   64/10000]\n",
      "loss: 0.368912  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.447790 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.433678  [   64/10000]\n",
      "loss: 0.366701  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.445302 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.430863  [   64/10000]\n",
      "loss: 0.364536  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.442869 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.428119  [   64/10000]\n",
      "loss: 0.362417  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.440489 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.425447  [   64/10000]\n",
      "loss: 0.360347  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.438160 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.422842  [   64/10000]\n",
      "loss: 0.358335  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.435878 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.420293  [   64/10000]\n",
      "loss: 0.356364  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.433645 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.417797  [   64/10000]\n",
      "loss: 0.354442  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.431458 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.415352  [   64/10000]\n",
      "loss: 0.352563  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.429316 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.412960  [   64/10000]\n",
      "loss: 0.350741  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.427216 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.410622  [   64/10000]\n",
      "loss: 0.348958  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.425158 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.408329  [   64/10000]\n",
      "loss: 0.347208  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.423142 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.406083  [   64/10000]\n",
      "loss: 0.345504  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.421165 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.403884  [   64/10000]\n",
      "loss: 0.343843  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.419225 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.401731  [   64/10000]\n",
      "loss: 0.342204  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.417323 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.399616  [   64/10000]\n",
      "loss: 0.340615  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.415457 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.397553  [   64/10000]\n",
      "loss: 0.339066  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.413627 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.395536  [   64/10000]\n",
      "loss: 0.337555  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.411831 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.393556  [   64/10000]\n",
      "loss: 0.336072  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.410069 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.391611  [   64/10000]\n",
      "loss: 0.334636  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.408338 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.389708  [   64/10000]\n",
      "loss: 0.333231  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.406638 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.387844  [   64/10000]\n",
      "loss: 0.331863  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.404969 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.386014  [   64/10000]\n",
      "loss: 0.330534  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.403329 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.384214  [   64/10000]\n",
      "loss: 0.329231  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.401718 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.382451  [   64/10000]\n",
      "loss: 0.327978  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.400134 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.380720  [   64/10000]\n",
      "loss: 0.326754  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.398577 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.379021  [   64/10000]\n",
      "loss: 0.325565  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.397046 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.377356  [   64/10000]\n",
      "loss: 0.324400  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.395540 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.375718  [   64/10000]\n",
      "loss: 0.323256  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.394059 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.374110  [   64/10000]\n",
      "loss: 0.322136  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.392603 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.372534  [   64/10000]\n",
      "loss: 0.321030  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.391170 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.370987  [   64/10000]\n",
      "loss: 0.319943  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.389760 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.369466  [   64/10000]\n",
      "loss: 0.318888  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.388373 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.367979  [   64/10000]\n",
      "loss: 0.317850  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.387008 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.366519  [   64/10000]\n",
      "loss: 0.316823  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.385664 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.365080  [   64/10000]\n",
      "loss: 0.315827  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.384340 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.363667  [   64/10000]\n",
      "loss: 0.314839  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.383038 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.362269  [   64/10000]\n",
      "loss: 0.313872  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.381755 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.360899  [   64/10000]\n",
      "loss: 0.312917  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.380491 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.359557  [   64/10000]\n",
      "loss: 0.311989  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.379246 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.358236  [   64/10000]\n",
      "loss: 0.311086  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.378019 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.356944  [   64/10000]\n",
      "loss: 0.310189  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.376811 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.355678  [   64/10000]\n",
      "loss: 0.309311  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.375620 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.354441  [   64/10000]\n",
      "loss: 0.308455  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.374448 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.353226  [   64/10000]\n",
      "loss: 0.307616  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.373292 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.352027  [   64/10000]\n",
      "loss: 0.306788  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.372153 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.350843  [   64/10000]\n",
      "loss: 0.305974  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.371029 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.349678  [   64/10000]\n",
      "loss: 0.305170  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.369921 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.348529  [   64/10000]\n",
      "loss: 0.304394  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.368827 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.347404  [   64/10000]\n",
      "loss: 0.303625  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.367749 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.346293  [   64/10000]\n",
      "loss: 0.302871  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.366686 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.345202  [   64/10000]\n",
      "loss: 0.302124  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.365637 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.344134  [   64/10000]\n",
      "loss: 0.301396  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.364602 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.343078  [   64/10000]\n",
      "loss: 0.300678  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.363581 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.342038  [   64/10000]\n",
      "loss: 0.299964  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.362573 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.341011  [   64/10000]\n",
      "loss: 0.299263  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.361578 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.339999  [   64/10000]\n",
      "loss: 0.298573  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.360596 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.339000  [   64/10000]\n",
      "loss: 0.297891  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.359627 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.338017  [   64/10000]\n",
      "loss: 0.297221  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.358669 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.337046  [   64/10000]\n",
      "loss: 0.296560  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.357724 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.336087  [   64/10000]\n",
      "loss: 0.295915  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.356790 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.335142  [   64/10000]\n",
      "loss: 0.295277  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.355868 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.334211  [   64/10000]\n",
      "loss: 0.294652  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.354957 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.333291  [   64/10000]\n",
      "loss: 0.294032  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.354057 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.332385  [   64/10000]\n",
      "loss: 0.293426  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.353168 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.331492  [   64/10000]\n",
      "loss: 0.292838  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.352289 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.330613  [   64/10000]\n",
      "loss: 0.292244  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.351420 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.329744  [   64/10000]\n",
      "loss: 0.291663  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.350561 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.328885  [   64/10000]\n",
      "loss: 0.291087  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.349712 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.328040  [   64/10000]\n",
      "loss: 0.290514  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.348872 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.327207  [   64/10000]\n",
      "loss: 0.289956  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.348042 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.326387  [   64/10000]\n",
      "loss: 0.289409  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.347222 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.325580  [   64/10000]\n",
      "loss: 0.288874  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.346410 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.324785  [   64/10000]\n",
      "loss: 0.288337  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.345608 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.323998  [   64/10000]\n",
      "loss: 0.287802  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.344813 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.323221  [   64/10000]\n",
      "loss: 0.287273  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.344027 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.322454  [   64/10000]\n",
      "loss: 0.286735  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.343250 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.321695  [   64/10000]\n",
      "loss: 0.286209  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.342481 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.320949  [   64/10000]\n",
      "loss: 0.285699  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.341720 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.320210  [   64/10000]\n",
      "loss: 0.285192  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.340968 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.319482  [   64/10000]\n",
      "loss: 0.284692  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.340222 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.318763  [   64/10000]\n",
      "loss: 0.284211  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.339485 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.318053  [   64/10000]\n",
      "loss: 0.283736  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.338755 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.317354  [   64/10000]\n",
      "loss: 0.283271  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.338032 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.316663  [   64/10000]\n",
      "loss: 0.282803  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.337316 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.315978  [   64/10000]\n",
      "loss: 0.282342  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.336609 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.315302  [   64/10000]\n",
      "loss: 0.281874  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.335909 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.314633  [   64/10000]\n",
      "loss: 0.281414  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.335216 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.313972  [   64/10000]\n",
      "loss: 0.280969  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.334529 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.313319  [   64/10000]\n",
      "loss: 0.280533  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.333850 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.312671  [   64/10000]\n",
      "loss: 0.280102  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.333176 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.312029  [   64/10000]\n",
      "loss: 0.279693  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.332508 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.311394  [   64/10000]\n",
      "loss: 0.279300  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.331845 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.310773  [   64/10000]\n",
      "loss: 0.278902  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.331188 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.310160  [   64/10000]\n",
      "loss: 0.278491  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.330538 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.309551  [   64/10000]\n",
      "loss: 0.278082  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.329895 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.308950  [   64/10000]\n",
      "loss: 0.277666  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.329257 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.308356  [   64/10000]\n",
      "loss: 0.277258  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.328626 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.307771  [   64/10000]\n",
      "loss: 0.276855  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.327999 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.307192  [   64/10000]\n",
      "loss: 0.276460  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.327379 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.306620  [   64/10000]\n",
      "loss: 0.276068  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.326763 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.306054  [   64/10000]\n",
      "loss: 0.275698  [ 6464/10000]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.326153 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d169efe-dc50-4644-8d91-f6d68065c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1db01a-88c8-4a36-88bd-c0606f6d0787",
   "metadata": {},
   "source": [
    "### Load saved model test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f10b629-72cc-4aec-a206-b2f176b4d55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c8c5c-c43b-4514-8e8f-cbfaebad5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0639d546-ef63-47bc-8a50-60013c36bc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
       "           0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
       "           0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
       "           0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
       "           0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
       "           0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
       "           0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
       "           0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
       "           0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
       "           0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
       "           0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
       "           0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 7)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0], test_data[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
