{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918003c9-e2bd-469f-bc71-e92d03b87420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2bf673-a598-4ce2-8039-6597514f2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "bs = 1\n",
    "epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc0000dd-416d-442c-a631-de9a5538ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "def get_model():\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "def get_data(training_data, test_data, bs):\n",
    "    return (\n",
    "        DataLoader(training_data, batch_size=bs), #, shuffle=True),\n",
    "        DataLoader(test_data, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    correct = 0\n",
    "    size = len(train_dl.dataset)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (xb, yb) in enumerate(train_dl):\n",
    "            # xb = xb.to(device), yb.to(device)\n",
    "            loss, _ = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            correct += bool(model(xb).argmax() == yb)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                current = (batch + 1) * len(xb)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        \n",
    "\n",
    "        correct /= len(train_dl.dataset)\n",
    "\n",
    "        print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a7c2f7c-0a4a-41b4-ae71-9e11e930393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef6081fd-a23b-4728-8358-802a03c27d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(model(next(iter(train_dl))[0]).argmax() == next(iter(train_dl))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f305e497-c077-4f2b-9c8c-ae4df9dfcd5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.457382  [    1/60000]\n",
      "loss: 2.469971  [  101/60000]\n",
      "loss: 2.292928  [  201/60000]\n",
      "loss: 1.991651  [  301/60000]\n",
      "loss: 2.224533  [  401/60000]\n",
      "loss: 2.491241  [  501/60000]\n",
      "loss: 2.182206  [  601/60000]\n",
      "loss: 2.204501  [  701/60000]\n",
      "loss: 2.122061  [  801/60000]\n",
      "loss: 2.184120  [  901/60000]\n",
      "loss: 1.960044  [ 1001/60000]\n",
      "loss: 2.345141  [ 1101/60000]\n",
      "loss: 1.907452  [ 1201/60000]\n",
      "loss: 1.807153  [ 1301/60000]\n",
      "loss: 1.795830  [ 1401/60000]\n",
      "loss: 2.075871  [ 1501/60000]\n",
      "loss: 2.231993  [ 1601/60000]\n",
      "loss: 1.818370  [ 1701/60000]\n",
      "loss: 2.145934  [ 1801/60000]\n",
      "loss: 2.020865  [ 1901/60000]\n",
      "loss: 2.630142  [ 2001/60000]\n",
      "loss: 0.805722  [ 2101/60000]\n",
      "loss: 1.297330  [ 2201/60000]\n",
      "loss: 1.768152  [ 2301/60000]\n",
      "loss: 1.540125  [ 2401/60000]\n",
      "loss: 0.791902  [ 2501/60000]\n",
      "loss: 2.116289  [ 2601/60000]\n",
      "loss: 1.788357  [ 2701/60000]\n",
      "loss: 1.698769  [ 2801/60000]\n",
      "loss: 2.317448  [ 2901/60000]\n",
      "loss: 2.271123  [ 3001/60000]\n",
      "loss: 1.052840  [ 3101/60000]\n",
      "loss: 1.676888  [ 3201/60000]\n",
      "loss: 2.480352  [ 3301/60000]\n",
      "loss: 0.830005  [ 3401/60000]\n",
      "loss: 2.346186  [ 3501/60000]\n",
      "loss: 1.645255  [ 3601/60000]\n",
      "loss: 1.409584  [ 3701/60000]\n",
      "loss: 1.047595  [ 3801/60000]\n",
      "loss: 1.005613  [ 3901/60000]\n",
      "loss: 1.146248  [ 4001/60000]\n",
      "loss: 1.238464  [ 4101/60000]\n",
      "loss: 1.307854  [ 4201/60000]\n",
      "loss: 0.691030  [ 4301/60000]\n",
      "loss: 0.494261  [ 4401/60000]\n",
      "loss: 0.883365  [ 4501/60000]\n",
      "loss: 2.101048  [ 4601/60000]\n",
      "loss: 1.261743  [ 4701/60000]\n",
      "loss: 1.404801  [ 4801/60000]\n",
      "loss: 1.214261  [ 4901/60000]\n",
      "loss: 0.806522  [ 5001/60000]\n",
      "loss: 0.538568  [ 5101/60000]\n",
      "loss: 1.116953  [ 5201/60000]\n",
      "loss: 0.230791  [ 5301/60000]\n",
      "loss: 1.480805  [ 5401/60000]\n",
      "loss: 0.532331  [ 5501/60000]\n",
      "loss: 1.294934  [ 5601/60000]\n",
      "loss: 2.097468  [ 5701/60000]\n",
      "loss: 1.891521  [ 5801/60000]\n",
      "loss: 0.510180  [ 5901/60000]\n",
      "loss: 0.209871  [ 6001/60000]\n",
      "loss: 1.276977  [ 6101/60000]\n",
      "loss: 1.690305  [ 6201/60000]\n",
      "loss: 1.775306  [ 6301/60000]\n",
      "loss: 0.206924  [ 6401/60000]\n",
      "loss: 0.754294  [ 6501/60000]\n",
      "loss: 1.243602  [ 6601/60000]\n",
      "loss: 0.122923  [ 6701/60000]\n",
      "loss: 0.487441  [ 6801/60000]\n",
      "loss: 0.310920  [ 6901/60000]\n",
      "loss: 1.270935  [ 7001/60000]\n",
      "loss: 0.835888  [ 7101/60000]\n",
      "loss: 1.335366  [ 7201/60000]\n",
      "loss: 2.648135  [ 7301/60000]\n",
      "loss: 1.075899  [ 7401/60000]\n",
      "loss: 0.321354  [ 7501/60000]\n",
      "loss: 1.055610  [ 7601/60000]\n",
      "loss: 0.624778  [ 7701/60000]\n",
      "loss: 0.941027  [ 7801/60000]\n",
      "loss: 1.261657  [ 7901/60000]\n",
      "loss: 0.033364  [ 8001/60000]\n",
      "loss: 0.415489  [ 8101/60000]\n",
      "loss: 3.936905  [ 8201/60000]\n",
      "loss: 1.482984  [ 8301/60000]\n",
      "loss: 0.981400  [ 8401/60000]\n",
      "loss: 0.067732  [ 8501/60000]\n",
      "loss: 1.912108  [ 8601/60000]\n",
      "loss: 1.130462  [ 8701/60000]\n",
      "loss: 0.375548  [ 8801/60000]\n",
      "loss: 1.317495  [ 8901/60000]\n",
      "loss: 0.608287  [ 9001/60000]\n",
      "loss: 0.260167  [ 9101/60000]\n",
      "loss: 0.146545  [ 9201/60000]\n",
      "loss: 0.288596  [ 9301/60000]\n",
      "loss: 0.178621  [ 9401/60000]\n",
      "loss: 0.120184  [ 9501/60000]\n",
      "loss: 0.055478  [ 9601/60000]\n",
      "loss: 0.044645  [ 9701/60000]\n",
      "loss: 1.791669  [ 9801/60000]\n",
      "loss: 0.303006  [ 9901/60000]\n",
      "loss: 0.041392  [10001/60000]\n",
      "loss: 0.454979  [10101/60000]\n",
      "loss: 0.436967  [10201/60000]\n",
      "loss: 0.297043  [10301/60000]\n",
      "loss: 0.966008  [10401/60000]\n",
      "loss: 0.105118  [10501/60000]\n",
      "loss: 0.201102  [10601/60000]\n",
      "loss: 0.316867  [10701/60000]\n",
      "loss: 1.729945  [10801/60000]\n",
      "loss: 0.118862  [10901/60000]\n",
      "loss: 0.023401  [11001/60000]\n",
      "loss: 0.692057  [11101/60000]\n",
      "loss: 0.177608  [11201/60000]\n",
      "loss: 0.911204  [11301/60000]\n",
      "loss: 0.263976  [11401/60000]\n",
      "loss: 1.280058  [11501/60000]\n",
      "loss: 4.295838  [11601/60000]\n",
      "loss: 0.446181  [11701/60000]\n",
      "loss: 0.044605  [11801/60000]\n",
      "loss: 0.625691  [11901/60000]\n",
      "loss: 0.137604  [12001/60000]\n",
      "loss: 0.031722  [12101/60000]\n",
      "loss: 0.529119  [12201/60000]\n",
      "loss: 0.223223  [12301/60000]\n",
      "loss: 0.044159  [12401/60000]\n",
      "loss: 0.097859  [12501/60000]\n",
      "loss: 1.017704  [12601/60000]\n",
      "loss: 0.387653  [12701/60000]\n",
      "loss: 0.867346  [12801/60000]\n",
      "loss: 1.150134  [12901/60000]\n",
      "loss: 0.542095  [13001/60000]\n",
      "loss: 0.903811  [13101/60000]\n",
      "loss: 0.740646  [13201/60000]\n",
      "loss: 0.613746  [13301/60000]\n",
      "loss: 0.698899  [13401/60000]\n",
      "loss: 0.429878  [13501/60000]\n",
      "loss: 0.013916  [13601/60000]\n",
      "loss: 0.248881  [13701/60000]\n",
      "loss: 0.734487  [13801/60000]\n",
      "loss: 0.679669  [13901/60000]\n",
      "loss: 0.073277  [14001/60000]\n",
      "loss: 2.766280  [14101/60000]\n",
      "loss: 0.145937  [14201/60000]\n",
      "loss: 0.622273  [14301/60000]\n",
      "loss: 0.032410  [14401/60000]\n",
      "loss: 2.495465  [14501/60000]\n",
      "loss: 0.123082  [14601/60000]\n",
      "loss: 0.717033  [14701/60000]\n",
      "loss: 0.257228  [14801/60000]\n",
      "loss: 0.121197  [14901/60000]\n",
      "loss: 0.820833  [15001/60000]\n",
      "loss: 0.157773  [15101/60000]\n",
      "loss: 0.030456  [15201/60000]\n",
      "loss: 0.173205  [15301/60000]\n",
      "loss: 0.423922  [15401/60000]\n",
      "loss: 0.379696  [15501/60000]\n",
      "loss: 0.011478  [15601/60000]\n",
      "loss: 0.045406  [15701/60000]\n",
      "loss: 0.280160  [15801/60000]\n",
      "loss: 0.011453  [15901/60000]\n",
      "loss: 0.016573  [16001/60000]\n",
      "loss: 2.437783  [16101/60000]\n",
      "loss: 1.564160  [16201/60000]\n",
      "loss: 0.058611  [16301/60000]\n",
      "loss: 0.935476  [16401/60000]\n",
      "loss: 0.020372  [16501/60000]\n",
      "loss: 1.372684  [16601/60000]\n",
      "loss: 0.640132  [16701/60000]\n",
      "loss: 0.035507  [16801/60000]\n",
      "loss: 0.219138  [16901/60000]\n",
      "loss: 0.385744  [17001/60000]\n",
      "loss: 0.856524  [17101/60000]\n",
      "loss: 0.282739  [17201/60000]\n",
      "loss: 0.406964  [17301/60000]\n",
      "loss: 1.810165  [17401/60000]\n",
      "loss: 0.283852  [17501/60000]\n",
      "loss: 0.065813  [17601/60000]\n",
      "loss: 1.881645  [17701/60000]\n",
      "loss: 0.315063  [17801/60000]\n",
      "loss: 0.346124  [17901/60000]\n",
      "loss: 0.370059  [18001/60000]\n",
      "loss: 0.187072  [18101/60000]\n",
      "loss: 0.324048  [18201/60000]\n",
      "loss: 0.026590  [18301/60000]\n",
      "loss: 0.179815  [18401/60000]\n",
      "loss: 0.695858  [18501/60000]\n",
      "loss: 0.418495  [18601/60000]\n",
      "loss: 0.885796  [18701/60000]\n",
      "loss: 0.048430  [18801/60000]\n",
      "loss: 1.638515  [18901/60000]\n",
      "loss: 0.183118  [19001/60000]\n",
      "loss: 0.030681  [19101/60000]\n",
      "loss: 0.040618  [19201/60000]\n",
      "loss: 0.133311  [19301/60000]\n",
      "loss: 0.101784  [19401/60000]\n",
      "loss: 0.003636  [19501/60000]\n",
      "loss: 0.633050  [19601/60000]\n",
      "loss: 0.288397  [19701/60000]\n",
      "loss: 0.039850  [19801/60000]\n",
      "loss: 0.001759  [19901/60000]\n",
      "loss: 0.019106  [20001/60000]\n",
      "loss: 4.334841  [20101/60000]\n",
      "loss: 0.067911  [20201/60000]\n",
      "loss: 0.301863  [20301/60000]\n",
      "loss: 0.036717  [20401/60000]\n",
      "loss: 0.125302  [20501/60000]\n",
      "loss: 1.115970  [20601/60000]\n",
      "loss: 0.175401  [20701/60000]\n",
      "loss: 0.115811  [20801/60000]\n",
      "loss: 0.360513  [20901/60000]\n",
      "loss: 0.725107  [21001/60000]\n",
      "loss: 0.372169  [21101/60000]\n",
      "loss: 0.054674  [21201/60000]\n",
      "loss: 0.073597  [21301/60000]\n",
      "loss: 0.027558  [21401/60000]\n",
      "loss: 0.101612  [21501/60000]\n",
      "loss: 0.041867  [21601/60000]\n",
      "loss: 2.263171  [21701/60000]\n",
      "loss: 0.017845  [21801/60000]\n",
      "loss: 0.069922  [21901/60000]\n",
      "loss: 0.323058  [22001/60000]\n",
      "loss: 0.214728  [22101/60000]\n",
      "loss: 2.537234  [22201/60000]\n",
      "loss: 0.307699  [22301/60000]\n",
      "loss: 0.939013  [22401/60000]\n",
      "loss: 0.039027  [22501/60000]\n",
      "loss: 0.008430  [22601/60000]\n",
      "loss: 0.033617  [22701/60000]\n",
      "loss: 1.390679  [22801/60000]\n",
      "loss: 0.006419  [22901/60000]\n",
      "loss: 0.008975  [23001/60000]\n",
      "loss: 2.470105  [23101/60000]\n",
      "loss: 0.446338  [23201/60000]\n",
      "loss: 0.145641  [23301/60000]\n",
      "loss: 0.789641  [23401/60000]\n",
      "loss: 0.036743  [23501/60000]\n",
      "loss: 0.068540  [23601/60000]\n",
      "loss: 0.551414  [23701/60000]\n",
      "loss: 0.041745  [23801/60000]\n",
      "loss: 0.168936  [23901/60000]\n",
      "loss: 0.144979  [24001/60000]\n",
      "loss: 0.044109  [24101/60000]\n",
      "loss: 0.025826  [24201/60000]\n",
      "loss: 0.962590  [24301/60000]\n",
      "loss: 0.006517  [24401/60000]\n",
      "loss: 0.022202  [24501/60000]\n",
      "loss: 0.137093  [24601/60000]\n",
      "loss: 0.003830  [24701/60000]\n",
      "loss: 0.045806  [24801/60000]\n",
      "loss: 0.078315  [24901/60000]\n",
      "loss: 1.217725  [25001/60000]\n",
      "loss: 0.312235  [25101/60000]\n",
      "loss: 0.715946  [25201/60000]\n",
      "loss: 0.383919  [25301/60000]\n",
      "loss: 0.351764  [25401/60000]\n",
      "loss: 0.009546  [25501/60000]\n",
      "loss: 0.037214  [25601/60000]\n",
      "loss: 0.062877  [25701/60000]\n",
      "loss: 5.706127  [25801/60000]\n",
      "loss: 1.167052  [25901/60000]\n",
      "loss: 0.165231  [26001/60000]\n",
      "loss: 0.448918  [26101/60000]\n",
      "loss: 0.061067  [26201/60000]\n",
      "loss: 3.551347  [26301/60000]\n",
      "loss: 0.131383  [26401/60000]\n",
      "loss: 0.114605  [26501/60000]\n",
      "loss: 2.160501  [26601/60000]\n",
      "loss: 0.215235  [26701/60000]\n",
      "loss: 0.002481  [26801/60000]\n",
      "loss: 3.203165  [26901/60000]\n",
      "loss: 0.818569  [27001/60000]\n",
      "loss: 1.370994  [27101/60000]\n",
      "loss: 0.164534  [27201/60000]\n",
      "loss: 0.041328  [27301/60000]\n",
      "loss: 0.051845  [27401/60000]\n",
      "loss: 0.101599  [27501/60000]\n",
      "loss: 0.005967  [27601/60000]\n",
      "loss: 0.610532  [27701/60000]\n",
      "loss: 0.130772  [27801/60000]\n",
      "loss: 0.101309  [27901/60000]\n",
      "loss: 0.037347  [28001/60000]\n",
      "loss: 0.063258  [28101/60000]\n",
      "loss: 0.902528  [28201/60000]\n",
      "loss: 0.058207  [28301/60000]\n",
      "loss: 0.023057  [28401/60000]\n",
      "loss: 0.032610  [28501/60000]\n",
      "loss: 0.320117  [28601/60000]\n",
      "loss: 0.314335  [28701/60000]\n",
      "loss: 0.152387  [28801/60000]\n",
      "loss: 0.015153  [28901/60000]\n",
      "loss: 0.062757  [29001/60000]\n",
      "loss: 0.467227  [29101/60000]\n",
      "loss: 0.015978  [29201/60000]\n",
      "loss: 0.697589  [29301/60000]\n",
      "loss: 1.128170  [29401/60000]\n",
      "loss: 0.048403  [29501/60000]\n",
      "loss: 0.068778  [29601/60000]\n",
      "loss: 0.245054  [29701/60000]\n",
      "loss: 1.033142  [29801/60000]\n",
      "loss: 1.522182  [29901/60000]\n",
      "loss: 0.011713  [30001/60000]\n",
      "loss: 0.157321  [30101/60000]\n",
      "loss: 0.110296  [30201/60000]\n",
      "loss: 0.008777  [30301/60000]\n",
      "loss: 0.313596  [30401/60000]\n",
      "loss: 0.236161  [30501/60000]\n",
      "loss: 2.575183  [30601/60000]\n",
      "loss: 0.065728  [30701/60000]\n",
      "loss: 0.195528  [30801/60000]\n",
      "loss: 4.674928  [30901/60000]\n",
      "loss: 1.864911  [31001/60000]\n",
      "loss: 0.421660  [31101/60000]\n",
      "loss: 3.563569  [31201/60000]\n",
      "loss: 0.172818  [31301/60000]\n",
      "loss: 0.226082  [31401/60000]\n",
      "loss: 1.072698  [31501/60000]\n",
      "loss: 3.783630  [31601/60000]\n",
      "loss: 0.279930  [31701/60000]\n",
      "loss: 0.672922  [31801/60000]\n",
      "loss: 0.718057  [31901/60000]\n",
      "loss: 0.034437  [32001/60000]\n",
      "loss: 0.681753  [32101/60000]\n",
      "loss: 0.110710  [32201/60000]\n",
      "loss: 0.034475  [32301/60000]\n",
      "loss: 0.614066  [32401/60000]\n",
      "loss: 0.614639  [32501/60000]\n",
      "loss: 0.092604  [32601/60000]\n",
      "loss: 0.013105  [32701/60000]\n",
      "loss: 0.016372  [32801/60000]\n",
      "loss: 0.034712  [32901/60000]\n",
      "loss: 0.905521  [33001/60000]\n",
      "loss: 0.001929  [33101/60000]\n",
      "loss: 0.608544  [33201/60000]\n",
      "loss: 0.171651  [33301/60000]\n",
      "loss: 0.049378  [33401/60000]\n",
      "loss: 0.180876  [33501/60000]\n",
      "loss: 0.032428  [33601/60000]\n",
      "loss: 0.152885  [33701/60000]\n",
      "loss: 0.023463  [33801/60000]\n",
      "loss: 0.761424  [33901/60000]\n",
      "loss: 0.194662  [34001/60000]\n",
      "loss: 0.324450  [34101/60000]\n",
      "loss: 0.020994  [34201/60000]\n",
      "loss: 0.523472  [34301/60000]\n",
      "loss: 0.147169  [34401/60000]\n",
      "loss: 5.083581  [34501/60000]\n",
      "loss: 0.233335  [34601/60000]\n",
      "loss: 0.246374  [34701/60000]\n",
      "loss: 3.114969  [34801/60000]\n",
      "loss: 0.156589  [34901/60000]\n",
      "loss: 0.010983  [35001/60000]\n",
      "loss: 0.696261  [35101/60000]\n",
      "loss: 0.036567  [35201/60000]\n",
      "loss: 0.254823  [35301/60000]\n",
      "loss: 0.000768  [35401/60000]\n",
      "loss: 1.056696  [35501/60000]\n",
      "loss: 0.024424  [35601/60000]\n",
      "loss: 0.000264  [35701/60000]\n",
      "loss: 0.001485  [35801/60000]\n",
      "loss: 0.006348  [35901/60000]\n",
      "loss: 1.030469  [36001/60000]\n",
      "loss: 0.186535  [36101/60000]\n",
      "loss: 0.156222  [36201/60000]\n",
      "loss: 0.142515  [36301/60000]\n",
      "loss: 0.044311  [36401/60000]\n",
      "loss: 0.527040  [36501/60000]\n",
      "loss: 0.257057  [36601/60000]\n",
      "loss: 0.013829  [36701/60000]\n",
      "loss: 0.283923  [36801/60000]\n",
      "loss: 0.962478  [36901/60000]\n",
      "loss: 0.023980  [37001/60000]\n",
      "loss: 1.856211  [37101/60000]\n",
      "loss: 0.004474  [37201/60000]\n",
      "loss: 0.146510  [37301/60000]\n",
      "loss: 0.519574  [37401/60000]\n",
      "loss: 0.017412  [37501/60000]\n",
      "loss: 0.172482  [37601/60000]\n",
      "loss: 0.032593  [37701/60000]\n",
      "loss: 0.040835  [37801/60000]\n",
      "loss: 0.121474  [37901/60000]\n",
      "loss: 0.213859  [38001/60000]\n",
      "loss: 0.006189  [38101/60000]\n",
      "loss: 0.000087  [38201/60000]\n",
      "loss: 0.556273  [38301/60000]\n",
      "loss: 0.014477  [38401/60000]\n",
      "loss: 0.042494  [38501/60000]\n",
      "loss: 0.129894  [38601/60000]\n",
      "loss: 0.773919  [38701/60000]\n",
      "loss: 0.763013  [38801/60000]\n",
      "loss: 0.038650  [38901/60000]\n",
      "loss: 0.889616  [39001/60000]\n",
      "loss: 0.177947  [39101/60000]\n",
      "loss: 0.002869  [39201/60000]\n",
      "loss: 0.022155  [39301/60000]\n",
      "loss: 0.155056  [39401/60000]\n",
      "loss: 0.260561  [39501/60000]\n",
      "loss: 2.524298  [39601/60000]\n",
      "loss: 1.081022  [39701/60000]\n",
      "loss: 0.006562  [39801/60000]\n",
      "loss: 0.018109  [39901/60000]\n",
      "loss: 0.002181  [40001/60000]\n",
      "loss: 0.284623  [40101/60000]\n",
      "loss: 0.054010  [40201/60000]\n",
      "loss: 0.740011  [40301/60000]\n",
      "loss: 0.003544  [40401/60000]\n",
      "loss: 0.028713  [40501/60000]\n",
      "loss: 0.145408  [40601/60000]\n",
      "loss: 0.446545  [40701/60000]\n",
      "loss: 0.231097  [40801/60000]\n",
      "loss: 0.136533  [40901/60000]\n",
      "loss: 0.149712  [41001/60000]\n",
      "loss: 0.058302  [41101/60000]\n",
      "loss: 1.340831  [41201/60000]\n",
      "loss: 0.056587  [41301/60000]\n",
      "loss: 1.038752  [41401/60000]\n",
      "loss: 0.313334  [41501/60000]\n",
      "loss: 0.037241  [41601/60000]\n",
      "loss: 0.979085  [41701/60000]\n",
      "loss: 0.014415  [41801/60000]\n",
      "loss: 1.066867  [41901/60000]\n",
      "loss: 0.012149  [42001/60000]\n",
      "loss: 0.061750  [42101/60000]\n",
      "loss: 0.102290  [42201/60000]\n",
      "loss: 0.002434  [42301/60000]\n",
      "loss: 0.471195  [42401/60000]\n",
      "loss: 0.073698  [42501/60000]\n",
      "loss: 0.053434  [42601/60000]\n",
      "loss: 0.010964  [42701/60000]\n",
      "loss: 0.079249  [42801/60000]\n",
      "loss: 0.021224  [42901/60000]\n",
      "loss: 0.905389  [43001/60000]\n",
      "loss: 0.167333  [43101/60000]\n",
      "loss: 0.888645  [43201/60000]\n",
      "loss: 0.040439  [43301/60000]\n",
      "loss: 0.274947  [43401/60000]\n",
      "loss: 0.062644  [43501/60000]\n",
      "loss: 0.219577  [43601/60000]\n",
      "loss: 0.156410  [43701/60000]\n",
      "loss: 0.163195  [43801/60000]\n",
      "loss: 0.415386  [43901/60000]\n",
      "loss: 0.013884  [44001/60000]\n",
      "loss: 1.828244  [44101/60000]\n",
      "loss: 0.258537  [44201/60000]\n",
      "loss: 1.514784  [44301/60000]\n",
      "loss: 0.037951  [44401/60000]\n",
      "loss: 0.212244  [44501/60000]\n",
      "loss: 0.127261  [44601/60000]\n",
      "loss: 0.001773  [44701/60000]\n",
      "loss: 0.356036  [44801/60000]\n",
      "loss: 0.003806  [44901/60000]\n",
      "loss: 0.090617  [45001/60000]\n",
      "loss: 0.044084  [45101/60000]\n",
      "loss: 0.003264  [45201/60000]\n",
      "loss: 0.382951  [45301/60000]\n",
      "loss: 0.011074  [45401/60000]\n",
      "loss: 0.349783  [45501/60000]\n",
      "loss: 0.142436  [45601/60000]\n",
      "loss: 0.313726  [45701/60000]\n",
      "loss: 4.900605  [45801/60000]\n",
      "loss: 0.047610  [45901/60000]\n",
      "loss: 0.228034  [46001/60000]\n",
      "loss: 0.291555  [46101/60000]\n",
      "loss: 0.867074  [46201/60000]\n",
      "loss: 4.417083  [46301/60000]\n",
      "loss: 0.068164  [46401/60000]\n",
      "loss: 0.003897  [46501/60000]\n",
      "loss: 0.182193  [46601/60000]\n",
      "loss: 0.261476  [46701/60000]\n",
      "loss: 0.077634  [46801/60000]\n",
      "loss: 0.006890  [46901/60000]\n",
      "loss: 0.015972  [47001/60000]\n",
      "loss: 1.113764  [47101/60000]\n",
      "loss: 0.039238  [47201/60000]\n",
      "loss: 0.409534  [47301/60000]\n",
      "loss: 0.033890  [47401/60000]\n",
      "loss: 0.036642  [47501/60000]\n",
      "loss: 3.018127  [47601/60000]\n",
      "loss: 0.002754  [47701/60000]\n",
      "loss: 0.001320  [47801/60000]\n",
      "loss: 0.000488  [47901/60000]\n",
      "loss: 0.083831  [48001/60000]\n",
      "loss: 0.001417  [48101/60000]\n",
      "loss: 0.220662  [48201/60000]\n",
      "loss: 0.078545  [48301/60000]\n",
      "loss: 0.072157  [48401/60000]\n",
      "loss: 0.160239  [48501/60000]\n",
      "loss: 0.012563  [48601/60000]\n",
      "loss: 0.310531  [48701/60000]\n",
      "loss: 1.378173  [48801/60000]\n",
      "loss: 0.249646  [48901/60000]\n",
      "loss: 0.032217  [49001/60000]\n",
      "loss: 0.130147  [49101/60000]\n",
      "loss: 0.729573  [49201/60000]\n",
      "loss: 3.434250  [49301/60000]\n",
      "loss: 0.327526  [49401/60000]\n",
      "loss: 1.703244  [49501/60000]\n",
      "loss: 0.022281  [49601/60000]\n",
      "loss: 0.368536  [49701/60000]\n",
      "loss: 0.203643  [49801/60000]\n",
      "loss: 0.024516  [49901/60000]\n",
      "loss: 0.146853  [50001/60000]\n",
      "loss: 0.003509  [50101/60000]\n",
      "loss: 0.004195  [50201/60000]\n",
      "loss: 0.040783  [50301/60000]\n",
      "loss: 0.034032  [50401/60000]\n",
      "loss: 0.120464  [50501/60000]\n",
      "loss: 0.005061  [50601/60000]\n",
      "loss: 0.786560  [50701/60000]\n",
      "loss: 0.043425  [50801/60000]\n",
      "loss: 0.032592  [50901/60000]\n",
      "loss: 0.016153  [51001/60000]\n",
      "loss: 0.259272  [51101/60000]\n",
      "loss: 2.013326  [51201/60000]\n",
      "loss: 3.777466  [51301/60000]\n",
      "loss: 0.039802  [51401/60000]\n",
      "loss: 0.013599  [51501/60000]\n",
      "loss: 3.553118  [51601/60000]\n",
      "loss: 0.008629  [51701/60000]\n",
      "loss: 0.067409  [51801/60000]\n",
      "loss: 0.028240  [51901/60000]\n",
      "loss: 0.060117  [52001/60000]\n",
      "loss: 0.834336  [52101/60000]\n",
      "loss: 0.170378  [52201/60000]\n",
      "loss: 0.002435  [52301/60000]\n",
      "loss: 0.042267  [52401/60000]\n",
      "loss: 0.064326  [52501/60000]\n",
      "loss: 0.311400  [52601/60000]\n",
      "loss: 0.028213  [52701/60000]\n",
      "loss: 3.177749  [52801/60000]\n",
      "loss: 0.160467  [52901/60000]\n",
      "loss: 0.007515  [53001/60000]\n",
      "loss: 0.014387  [53101/60000]\n",
      "loss: 0.055984  [53201/60000]\n",
      "loss: 0.270577  [53301/60000]\n",
      "loss: 0.003108  [53401/60000]\n",
      "loss: 0.227279  [53501/60000]\n",
      "loss: 0.234134  [53601/60000]\n",
      "loss: 0.191237  [53701/60000]\n",
      "loss: 0.040321  [53801/60000]\n",
      "loss: 0.738759  [53901/60000]\n",
      "loss: 0.571201  [54001/60000]\n",
      "loss: 0.270574  [54101/60000]\n",
      "loss: 0.155555  [54201/60000]\n",
      "loss: 0.109462  [54301/60000]\n",
      "loss: 0.212353  [54401/60000]\n",
      "loss: 0.010579  [54501/60000]\n",
      "loss: 0.330317  [54601/60000]\n",
      "loss: 0.005884  [54701/60000]\n",
      "loss: 0.005551  [54801/60000]\n",
      "loss: 0.012555  [54901/60000]\n",
      "loss: 0.039619  [55001/60000]\n",
      "loss: 0.010886  [55101/60000]\n",
      "loss: 0.023106  [55201/60000]\n",
      "loss: 0.001524  [55301/60000]\n",
      "loss: 0.683596  [55401/60000]\n",
      "loss: 0.062635  [55501/60000]\n",
      "loss: 0.081623  [55601/60000]\n",
      "loss: 0.002254  [55701/60000]\n",
      "loss: 0.082359  [55801/60000]\n",
      "loss: 0.012516  [55901/60000]\n",
      "loss: 0.026264  [56001/60000]\n",
      "loss: 0.001300  [56101/60000]\n",
      "loss: 0.017900  [56201/60000]\n",
      "loss: 2.955956  [56301/60000]\n",
      "loss: 0.058197  [56401/60000]\n",
      "loss: 0.021216  [56501/60000]\n",
      "loss: 1.934702  [56601/60000]\n",
      "loss: 0.021746  [56701/60000]\n",
      "loss: 3.552366  [56801/60000]\n",
      "loss: 0.021935  [56901/60000]\n",
      "loss: 0.000145  [57001/60000]\n",
      "loss: 0.092697  [57101/60000]\n",
      "loss: 0.186725  [57201/60000]\n",
      "loss: 0.029901  [57301/60000]\n",
      "loss: 0.198943  [57401/60000]\n",
      "loss: 0.002445  [57501/60000]\n",
      "loss: 0.003126  [57601/60000]\n",
      "loss: 0.035042  [57701/60000]\n",
      "loss: 0.065259  [57801/60000]\n",
      "loss: 0.143508  [57901/60000]\n",
      "loss: 0.263480  [58001/60000]\n",
      "loss: 0.667503  [58101/60000]\n",
      "loss: 0.001545  [58201/60000]\n",
      "loss: 0.008847  [58301/60000]\n",
      "loss: 0.024178  [58401/60000]\n",
      "loss: 0.012381  [58501/60000]\n",
      "loss: 0.020868  [58601/60000]\n",
      "loss: 0.046676  [58701/60000]\n",
      "loss: 0.038996  [58801/60000]\n",
      "loss: 0.225659  [58901/60000]\n",
      "loss: 0.041535  [59001/60000]\n",
      "loss: 0.005275  [59101/60000]\n",
      "loss: 0.025149  [59201/60000]\n",
      "loss: 0.009797  [59301/60000]\n",
      "loss: 2.185189  [59401/60000]\n",
      "loss: 0.018729  [59501/60000]\n",
      "loss: 0.083233  [59601/60000]\n",
      "loss: 0.001804  [59701/60000]\n",
      "loss: 0.024464  [59801/60000]\n",
      "loss: 0.033861  [59901/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dl, valid_dl = get_data(training_data, test_data, bs)\n",
    "model, loss_func, opt = get_model()\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b580c5-fa00-4219-a539-d30ffd373e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
