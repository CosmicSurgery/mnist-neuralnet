{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88919c8-c47c-4740-bd3a-af1a9555529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e27198a-4301-473d-9ee2-1e12dbc6ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "def get_data(training_data, test_data):\n",
    "    return (\n",
    "        DataLoader(training_data), #, shuffle=True),\n",
    "        DataLoader(test_data),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(training_data, test_data)\n",
    "train_dataloader = DataLoader(training_data, batch_size=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fb4e58-19d8-4937-b1de-9622c44deab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter(train_dl))\n",
    "x1, y1 =x1.flatten(), y1.squeeze()\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dce317fd-1713-49e9-92d1-32271c058402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias', 'linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c021a9f-68c3-4559-8673-9b807b047c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13902374, -0.14935616, -0.2146723 ,  0.21417932,  0.11655144,\n",
       "        -0.15464526, -0.16536987,  0.06310065,  0.15438937,  0.20637192,\n",
       "         0.02966635,  0.13796093,  0.09203579,  0.08334605, -0.11801162,\n",
       "        -0.17072244,  0.22889192,  0.04892804],\n",
       "       [-0.21189949, -0.08715634,  0.02339686,  0.03954171, -0.17625031,\n",
       "         0.20591186, -0.2173645 ,  0.11548699, -0.10416864, -0.16685198,\n",
       "         0.10424839, -0.05649   , -0.09371138, -0.02472298, -0.08418827,\n",
       "         0.21943991, -0.22199707, -0.08080144],\n",
       "       [-0.02861121, -0.06552699, -0.14460835,  0.0312344 ,  0.00475523,\n",
       "         0.14975406,  0.07740985,  0.16138051, -0.16647691,  0.19663243,\n",
       "        -0.09861559, -0.00300536,  0.0867797 , -0.04870926, -0.06252556,\n",
       "         0.10711958,  0.16982137, -0.15328938],\n",
       "       [ 0.13577889,  0.15609859,  0.09796895,  0.07576694,  0.12653767,\n",
       "         0.22326736,  0.16011076,  0.00751489, -0.1609039 , -0.0057642 ,\n",
       "        -0.13544682,  0.20785193, -0.06999013,  0.01699112,  0.09970738,\n",
       "         0.08198054, -0.19329548, -0.15214717],\n",
       "       [ 0.04028247, -0.17312486,  0.09176044,  0.06247665, -0.09176102,\n",
       "        -0.18643656,  0.03612225, -0.13159677,  0.22620137,  0.15221961,\n",
       "         0.15084632,  0.11025812,  0.0919001 , -0.17692761,  0.22441934,\n",
       "        -0.04816645,  0.13656075,  0.21641903],\n",
       "       [ 0.04532664, -0.17079858, -0.01118715, -0.13725299, -0.19192301,\n",
       "         0.16840635, -0.08901252,  0.18935736, -0.13702554,  0.09765171,\n",
       "        -0.01586711, -0.02623987,  0.13613869, -0.21341895,  0.19698323,\n",
       "        -0.04832752,  0.10644837, -0.13988309],\n",
       "       [-0.12294292, -0.00546537, -0.05641153, -0.09580858,  0.12754436,\n",
       "        -0.02573295, -0.09673542, -0.0269293 ,  0.21113311, -0.1932456 ,\n",
       "        -0.03922641, -0.03686962,  0.11476056,  0.19430141, -0.14800194,\n",
       "        -0.04366612, -0.21936505, -0.17431091],\n",
       "       [ 0.08837013, -0.02566712,  0.17451127, -0.02242279, -0.10761787,\n",
       "        -0.07775189,  0.02496217,  0.15099065,  0.01930688,  0.05375896,\n",
       "        -0.08277169,  0.08954869, -0.15908   ,  0.14642517, -0.01015197,\n",
       "         0.19800247, -0.11998205,  0.23106299],\n",
       "       [ 0.14974822, -0.19036274, -0.06995304,  0.20584749,  0.08005227,\n",
       "         0.19603716, -0.04943454, -0.10065904,  0.1563213 , -0.0707235 ,\n",
       "         0.21375553,  0.13763849,  0.23465045,  0.17776157, -0.0699899 ,\n",
       "         0.19879355, -0.1828627 ,  0.23063721],\n",
       "       [-0.00061756, -0.1074204 ,  0.06882687,  0.12650315,  0.01106091,\n",
       "         0.16752504, -0.01675183, -0.23162517,  0.15200402,  0.0021285 ,\n",
       "         0.13227372, -0.11641704,  0.1539803 ,  0.08852307, -0.13694966,\n",
       "        -0.21911015, -0.00785126,  0.04393448]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['linear_relu_stack.2.weight'].numpy().astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09be1b13-b164-4854-bf50-d8bf69b99e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2428a70a-96ed-46ae-8d83-54c93838bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    X[X<0] = 0\n",
    "    return X\n",
    "\n",
    "def d_relu(x):\n",
    "    d = x.clone()\n",
    "    d[x < 0] = 0\n",
    "    d[x > 0] = 1\n",
    "    return d\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum()\n",
    "\n",
    "def delta_l(y_pred, y, l):\n",
    "    if l == len(weights)-1: # This means it's the last layer... must be a vector\n",
    "        y_ = torch.zeros(len(y_pred))\n",
    "        y_[y] = 1\n",
    "        return (y_pred - y_)[None,:]\n",
    "    else:\n",
    "        return (weights[l+1].T@delta_l(y_pred,y,l+1).T)@d_relu(l)\n",
    "        # return weights[l].T@(delta_l(y_pred,y,l+1)*d_relu(l+1)).T\n",
    "    \n",
    "activations = []\n",
    "activations.append(relu)\n",
    "activations.append(linear)\n",
    "    \n",
    "def forward(X):\n",
    "    a = [X]\n",
    "    \n",
    "    for w,b,func in zip(weights, bias, activations):\n",
    "        a.append(func(a[-1]@w.T+b))\n",
    "    return a\n",
    "\n",
    "def cross_entropy(y_pred,y):\n",
    "    return -y_pred[y].log()\n",
    "\n",
    "def backward(a,y, lr=1e-3): # hard-wired backprop...\n",
    "    loss_L = delta_l(a[-1],y,1)\n",
    "    loss_l = ((weights[1].T@loss_L.T).T*d_relu(a[1]))\n",
    "    \n",
    "    weights[1] -= lr * (a[1][None,:].T@loss_L).T\n",
    "    bias[1] -= lr * loss_L.squeeze()\n",
    "    weights[0] -= lr * (a[0][None,:].T@loss_l).T\n",
    "    bias[0] -= lr * loss_l.squeeze()\n",
    "        \n",
    "    loss = cross_entropy(softmax(a[-1]),y)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067b9968-3440-4d10-abb7-3d12f4441a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "w,b,func = _,_,_\n",
    "y_pred = None\n",
    "X=None\n",
    "d=Nonel=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef36076-7bc3-4c78-a37a-92be4bdaf1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn_image as isns\n",
    "isns.imgplot(x1.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "086acb0f-4aed-41a0-b47b-490d6e275a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4ea6638-cdfc-4f39-b252-44a5b5a41790",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b12e5f0-0212-4601-91fc-d925624e1571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter_)\n",
    "x1, y1 =x1.flatten(), y1.squeeze()\n",
    "x1.shape, y1.shape\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5417e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc6966ba-5f07-4074-b5ee-d5476aba9fc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m DataLoader(training_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m x\u001b[38;5;241m.\u001b[39mdata()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "x = DataLoader(training_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a98a8fcf-4473-4697-96b7-121182922503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2666407e0f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x266644fc8f0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "936b5616-cf5c-43cd-846c-0dd18e3520b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4574)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = forward(x1)\n",
    "cross_entropy(softmax(a[-1]),y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a7f1c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0473e-09, 9.7139e-12, 2.3697e-12, 1.4955e-05, 8.6390e-03, 5.6414e-12,\n",
       "        2.3267e-09, 5.1686e-04, 9.9083e-01, 1.5852e-15])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240df45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -1.2346,  -5.9150,  -7.3258,   8.3320,  14.6910,  -7.4584,  -0.4364,\n",
       "           11.8747,  19.4332, -14.6356]]),\n",
       " tensor([[ 1.0473e-09,  9.7139e-12,  2.3697e-12,  1.4955e-05,  8.6390e-03,\n",
       "          -1.0000e+00,  2.3267e-09,  5.1686e-04,  9.9083e-01,  1.5852e-15]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_l(a[-1],y1,1), delta_l(softmax(a[-1]),y1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a791cb2e-c426-4067-88f3-8cb87f29ff1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0473e-09,  9.7139e-12,  2.3697e-12,  1.4955e-05,  8.6390e-03,\n",
       "          -1.0000e+00,  2.3267e-09,  5.1686e-04,  9.9083e-01,  1.5852e-15]]),\n",
       " tensor([[ 0.1034, -0.0193, -0.0000,  0.0000,  0.2704,  0.0000,  0.0000, -0.2902,\n",
       "           0.2939, -0.0000,  0.2289,  0.1636,  0.0000,  0.0000, -0.2644,  0.2450,\n",
       "          -0.2865,  0.0000]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_L = delta_l(softmax(a[-1]),y1,1) \n",
    "loss_L, ((weights[1].T@loss_L.T).T*d_relu(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1210f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.9009)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(softmax(a[-1]),y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8915bb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  27.6099,   32.1128,  -50.6857,  -10.6418,   22.0753,  -34.9349,\n",
       "        -113.4836,   20.4312,   54.3101,  -60.3901,   24.8315,    3.8750,\n",
       "         -66.7918,  -25.1650,   43.1287,   54.5816,    3.3011,  -17.7552])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1@weights[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa754dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.2346,  -5.9150,  -7.3258,   8.3320,  14.6910,  -6.4584,  -0.4364,\n",
       "         11.8747,  19.4332, -14.6356])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bbba118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0112,  0.0016,  0.0017, -0.0255, -0.0138, -0.0105, -0.0166, -0.0136,\n",
       "         0.0042,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320, -0.0351,  0.0314,\n",
       "         0.0258,  0.0109])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a45e5f08-4e7b-4fa5-8ef9-509a57485627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27.6211, 32.1144,  0.0000,  0.0000, 22.0615,  0.0000,  0.0000, 20.4177,\n",
      "        54.3143,  0.0000, 24.8167,  3.8441,  0.0000,  0.0000, 43.0936, 54.6130,\n",
      "         3.3269,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "fullprint(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39c2e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.2346,  -5.9150,  -7.3258,   8.3320,  14.6910,  -6.4584,  -0.4364,\n",
      "         11.8747,  19.4332, -14.6356])\n"
     ]
    }
   ],
   "source": [
    "fullprint(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "id": "0976e3e5-3b18-4b74-bcba-bc3394a3a377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3730)"
      ]
     },
     "execution_count": 1341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward(a,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "d80995d4-97b5-4ac0-81ce-f6e1d7e59a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0112,  0.0013,  0.0017, -0.0254, -0.0139, -0.0106, -0.0166, -0.0132,\n",
       "         0.0040,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320, -0.0349,  0.0310,\n",
       "         0.0263,  0.0108])"
      ]
     },
     "execution_count": 1342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "1f46bf38-657d-4869-b2c9-cd74c1ad903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1410, -0.0624, -0.0507, -0.2053, -0.0986, -0.1225,  0.0111,  0.2333,\n",
       "         0.1426,  0.1192])"
      ]
     },
     "execution_count": 1343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "id": "30447233-f74a-4c21-a1e5-73d66b451d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
       "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
       "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
       "        ...,\n",
       "        [-0.0215,  0.0106,  0.0308,  ..., -0.0199,  0.0161, -0.0342],\n",
       "        [ 0.0350, -0.0297, -0.0037,  ...,  0.0171,  0.0238, -0.0001],\n",
       "        [ 0.0085,  0.0223, -0.0324,  ..., -0.0296,  0.0182, -0.0296]])"
      ]
     },
     "execution_count": 1336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "id": "4204f907-bd09-4c59-838a-5e65e4c2d256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1390, -0.1493, -0.2147,  0.2143,  0.1171, -0.1545, -0.1654,  0.0632,\n",
       "         0.1547,  0.2064,  0.0297,  0.1380,  0.0920,  0.0833, -0.1179, -0.1704,\n",
       "         0.2290,  0.0490])"
      ]
     },
     "execution_count": 1344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1][y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "c6d79033-4ef6-4e14-a1c7-ccb824f32ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28]), torch.Size([1]))"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter(train_dl))\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "2a83a76c-bfe5-4b2b-84f3-aefc023adad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0865, 0.0919, 0.0930, 0.0835, 0.0954, 0.0857, 0.1006, 0.1327, 0.1245,\n",
       "         0.1064]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "model(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f358c9-332b-41f5-be8d-20b9306ce7a1",
   "metadata": {},
   "source": [
    "### Test the full training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40d4b6b0-4b80-4797-8399-ad796d57a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dataloader))\n",
    "x,y = x.flatten(), y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "84a71331-792a-4e83-bdf1-45d5b9b7eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for i, (x,y) in enumerate(dataloader):\n",
    "        x,y = x.flatten(), y.squeeze()\n",
    "        a = forward(x)\n",
    "        a[-1] = softmax(a[-1])\n",
    "        loss = cross_entropy(a[-1],y)\n",
    "\n",
    "        backward(a,y)\n",
    "\n",
    "        correct += (a[-1].argmax() == y).type(torch.float).sum()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            loss, current = loss.item(), (i+1)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        # if i == 100:\n",
    "        #     break;\n",
    "        \n",
    "    correct /= size\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*correct):>0.5f}%\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e780d092-ec49-4bae-a2fc-ef7fb7a17741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for x,y in dataloader:\n",
    "        x,y = x.flatten(), y.squeeze()\n",
    "        a = forward(x)\n",
    "        test_loss += cross_entropy(softmax(a[-1]),y)\n",
    "        correct += (a[-1].argmax() == y).type(torch.float).sum()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e54a84ca-7781-4143-a266-7ee08b3dfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "65eae6c9-1147-4859-ab02-9fb58a668c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.457382 [    1/60000]\n",
      "loss: 2.469971 [  101/60000]\n",
      "loss: 2.292928 [  201/60000]\n",
      "loss: 1.991651 [  301/60000]\n",
      "loss: 2.224533 [  401/60000]\n",
      "loss: 2.491240 [  501/60000]\n",
      "loss: 2.182206 [  601/60000]\n",
      "loss: 2.204501 [  701/60000]\n",
      "loss: 2.122061 [  801/60000]\n",
      "loss: 2.184120 [  901/60000]\n",
      "loss: 1.960044 [ 1001/60000]\n",
      "loss: 2.345141 [ 1101/60000]\n",
      "loss: 1.907452 [ 1201/60000]\n",
      "loss: 1.807153 [ 1301/60000]\n",
      "loss: 1.795830 [ 1401/60000]\n",
      "loss: 2.075871 [ 1501/60000]\n",
      "loss: 2.231993 [ 1601/60000]\n",
      "loss: 1.818371 [ 1701/60000]\n",
      "loss: 2.145934 [ 1801/60000]\n",
      "loss: 2.020865 [ 1901/60000]\n",
      "loss: 2.630142 [ 2001/60000]\n",
      "loss: 0.805722 [ 2101/60000]\n",
      "loss: 1.297330 [ 2201/60000]\n",
      "loss: 1.768152 [ 2301/60000]\n",
      "loss: 1.540125 [ 2401/60000]\n",
      "loss: 0.791902 [ 2501/60000]\n",
      "loss: 2.116289 [ 2601/60000]\n",
      "loss: 1.788357 [ 2701/60000]\n",
      "loss: 1.698770 [ 2801/60000]\n",
      "loss: 2.317448 [ 2901/60000]\n",
      "loss: 2.271123 [ 3001/60000]\n",
      "loss: 1.052840 [ 3101/60000]\n",
      "loss: 1.676888 [ 3201/60000]\n",
      "loss: 2.480352 [ 3301/60000]\n",
      "loss: 0.830005 [ 3401/60000]\n",
      "loss: 2.346186 [ 3501/60000]\n",
      "loss: 1.645255 [ 3601/60000]\n",
      "loss: 1.409585 [ 3701/60000]\n",
      "loss: 1.047595 [ 3801/60000]\n",
      "loss: 1.005613 [ 3901/60000]\n",
      "loss: 1.146248 [ 4001/60000]\n",
      "loss: 1.238464 [ 4101/60000]\n",
      "loss: 1.307854 [ 4201/60000]\n",
      "loss: 0.691030 [ 4301/60000]\n",
      "loss: 0.494261 [ 4401/60000]\n",
      "loss: 0.883365 [ 4501/60000]\n",
      "loss: 2.101048 [ 4601/60000]\n",
      "loss: 1.261743 [ 4701/60000]\n",
      "loss: 1.404801 [ 4801/60000]\n",
      "loss: 1.214261 [ 4901/60000]\n",
      "loss: 0.806522 [ 5001/60000]\n",
      "loss: 0.538568 [ 5101/60000]\n",
      "loss: 1.116953 [ 5201/60000]\n",
      "loss: 0.230791 [ 5301/60000]\n",
      "loss: 1.480805 [ 5401/60000]\n",
      "loss: 0.532331 [ 5501/60000]\n",
      "loss: 1.294934 [ 5601/60000]\n",
      "loss: 2.097469 [ 5701/60000]\n",
      "loss: 1.891521 [ 5801/60000]\n",
      "loss: 0.510180 [ 5901/60000]\n",
      "loss: 0.209871 [ 6001/60000]\n",
      "loss: 1.276977 [ 6101/60000]\n",
      "loss: 1.690305 [ 6201/60000]\n",
      "loss: 1.775306 [ 6301/60000]\n",
      "loss: 0.206924 [ 6401/60000]\n",
      "loss: 0.754294 [ 6501/60000]\n",
      "loss: 1.243602 [ 6601/60000]\n",
      "loss: 0.122923 [ 6701/60000]\n",
      "loss: 0.487441 [ 6801/60000]\n",
      "loss: 0.310920 [ 6901/60000]\n",
      "loss: 1.270936 [ 7001/60000]\n",
      "loss: 0.835887 [ 7101/60000]\n",
      "loss: 1.335366 [ 7201/60000]\n",
      "loss: 2.648134 [ 7301/60000]\n",
      "loss: 1.075899 [ 7401/60000]\n",
      "loss: 0.321354 [ 7501/60000]\n",
      "loss: 1.055610 [ 7601/60000]\n",
      "loss: 0.624778 [ 7701/60000]\n",
      "loss: 0.941028 [ 7801/60000]\n",
      "loss: 1.261657 [ 7901/60000]\n",
      "loss: 0.033364 [ 8001/60000]\n",
      "loss: 0.415489 [ 8101/60000]\n",
      "loss: 3.936906 [ 8201/60000]\n",
      "loss: 1.482984 [ 8301/60000]\n",
      "loss: 0.981400 [ 8401/60000]\n",
      "loss: 0.067732 [ 8501/60000]\n",
      "loss: 1.912108 [ 8601/60000]\n",
      "loss: 1.130462 [ 8701/60000]\n",
      "loss: 0.375548 [ 8801/60000]\n",
      "loss: 1.317495 [ 8901/60000]\n",
      "loss: 0.608286 [ 9001/60000]\n",
      "loss: 0.260167 [ 9101/60000]\n",
      "loss: 0.146545 [ 9201/60000]\n",
      "loss: 0.288596 [ 9301/60000]\n",
      "loss: 0.178621 [ 9401/60000]\n",
      "loss: 0.120184 [ 9501/60000]\n",
      "loss: 0.055478 [ 9601/60000]\n",
      "loss: 0.044645 [ 9701/60000]\n",
      "loss: 1.791669 [ 9801/60000]\n",
      "loss: 0.303007 [ 9901/60000]\n",
      "loss: 0.041391 [10001/60000]\n",
      "loss: 0.454979 [10101/60000]\n",
      "loss: 0.436967 [10201/60000]\n",
      "loss: 0.297043 [10301/60000]\n",
      "loss: 0.966008 [10401/60000]\n",
      "loss: 0.105118 [10501/60000]\n",
      "loss: 0.201102 [10601/60000]\n",
      "loss: 0.316867 [10701/60000]\n",
      "loss: 1.729945 [10801/60000]\n",
      "loss: 0.118862 [10901/60000]\n",
      "loss: 0.023401 [11001/60000]\n",
      "loss: 0.692057 [11101/60000]\n",
      "loss: 0.177608 [11201/60000]\n",
      "loss: 0.911204 [11301/60000]\n",
      "loss: 0.263976 [11401/60000]\n",
      "loss: 1.280058 [11501/60000]\n",
      "loss: 4.295839 [11601/60000]\n",
      "loss: 0.446181 [11701/60000]\n",
      "loss: 0.044605 [11801/60000]\n",
      "loss: 0.625691 [11901/60000]\n",
      "loss: 0.137604 [12001/60000]\n",
      "loss: 0.031721 [12101/60000]\n",
      "loss: 0.529120 [12201/60000]\n",
      "loss: 0.223223 [12301/60000]\n",
      "loss: 0.044159 [12401/60000]\n",
      "loss: 0.097859 [12501/60000]\n",
      "loss: 1.017704 [12601/60000]\n",
      "loss: 0.387653 [12701/60000]\n",
      "loss: 0.867346 [12801/60000]\n",
      "loss: 1.150134 [12901/60000]\n",
      "loss: 0.542095 [13001/60000]\n",
      "loss: 0.903811 [13101/60000]\n",
      "loss: 0.740646 [13201/60000]\n",
      "loss: 0.613746 [13301/60000]\n",
      "loss: 0.698899 [13401/60000]\n",
      "loss: 0.429878 [13501/60000]\n",
      "loss: 0.013916 [13601/60000]\n",
      "loss: 0.248882 [13701/60000]\n",
      "loss: 0.734487 [13801/60000]\n",
      "loss: 0.679669 [13901/60000]\n",
      "loss: 0.073277 [14001/60000]\n",
      "loss: 2.766279 [14101/60000]\n",
      "loss: 0.145937 [14201/60000]\n",
      "loss: 0.622272 [14301/60000]\n",
      "loss: 0.032410 [14401/60000]\n",
      "loss: 2.495465 [14501/60000]\n",
      "loss: 0.123082 [14601/60000]\n",
      "loss: 0.717034 [14701/60000]\n",
      "loss: 0.257228 [14801/60000]\n",
      "loss: 0.121197 [14901/60000]\n",
      "loss: 0.820833 [15001/60000]\n",
      "loss: 0.157773 [15101/60000]\n",
      "loss: 0.030456 [15201/60000]\n",
      "loss: 0.173205 [15301/60000]\n",
      "loss: 0.423922 [15401/60000]\n",
      "loss: 0.379697 [15501/60000]\n",
      "loss: 0.011478 [15601/60000]\n",
      "loss: 0.045406 [15701/60000]\n",
      "loss: 0.280160 [15801/60000]\n",
      "loss: 0.011453 [15901/60000]\n",
      "loss: 0.016574 [16001/60000]\n",
      "loss: 2.437783 [16101/60000]\n",
      "loss: 1.564159 [16201/60000]\n",
      "loss: 0.058611 [16301/60000]\n",
      "loss: 0.935476 [16401/60000]\n",
      "loss: 0.020372 [16501/60000]\n",
      "loss: 1.372684 [16601/60000]\n",
      "loss: 0.640132 [16701/60000]\n",
      "loss: 0.035507 [16801/60000]\n",
      "loss: 0.219137 [16901/60000]\n",
      "loss: 0.385744 [17001/60000]\n",
      "loss: 0.856523 [17101/60000]\n",
      "loss: 0.282740 [17201/60000]\n",
      "loss: 0.406964 [17301/60000]\n",
      "loss: 1.810165 [17401/60000]\n",
      "loss: 0.283852 [17501/60000]\n",
      "loss: 0.065813 [17601/60000]\n",
      "loss: 1.881645 [17701/60000]\n",
      "loss: 0.315063 [17801/60000]\n",
      "loss: 0.346124 [17901/60000]\n",
      "loss: 0.370059 [18001/60000]\n",
      "loss: 0.187071 [18101/60000]\n",
      "loss: 0.324048 [18201/60000]\n",
      "loss: 0.026590 [18301/60000]\n",
      "loss: 0.179814 [18401/60000]\n",
      "loss: 0.695858 [18501/60000]\n",
      "loss: 0.418496 [18601/60000]\n",
      "loss: 0.885797 [18701/60000]\n",
      "loss: 0.048430 [18801/60000]\n",
      "loss: 1.638515 [18901/60000]\n",
      "loss: 0.183118 [19001/60000]\n",
      "loss: 0.030681 [19101/60000]\n",
      "loss: 0.040618 [19201/60000]\n",
      "loss: 0.133311 [19301/60000]\n",
      "loss: 0.101784 [19401/60000]\n",
      "loss: 0.003636 [19501/60000]\n",
      "loss: 0.633050 [19601/60000]\n",
      "loss: 0.288397 [19701/60000]\n",
      "loss: 0.039850 [19801/60000]\n",
      "loss: 0.001759 [19901/60000]\n",
      "loss: 0.019106 [20001/60000]\n",
      "loss: 4.334841 [20101/60000]\n",
      "loss: 0.067911 [20201/60000]\n",
      "loss: 0.301863 [20301/60000]\n",
      "loss: 0.036717 [20401/60000]\n",
      "loss: 0.125302 [20501/60000]\n",
      "loss: 1.115971 [20601/60000]\n",
      "loss: 0.175401 [20701/60000]\n",
      "loss: 0.115810 [20801/60000]\n",
      "loss: 0.360513 [20901/60000]\n",
      "loss: 0.725106 [21001/60000]\n",
      "loss: 0.372169 [21101/60000]\n",
      "loss: 0.054674 [21201/60000]\n",
      "loss: 0.073597 [21301/60000]\n",
      "loss: 0.027558 [21401/60000]\n",
      "loss: 0.101612 [21501/60000]\n",
      "loss: 0.041867 [21601/60000]\n",
      "loss: 2.263171 [21701/60000]\n",
      "loss: 0.017845 [21801/60000]\n",
      "loss: 0.069923 [21901/60000]\n",
      "loss: 0.323058 [22001/60000]\n",
      "loss: 0.214728 [22101/60000]\n",
      "loss: 2.537234 [22201/60000]\n",
      "loss: 0.307699 [22301/60000]\n",
      "loss: 0.939013 [22401/60000]\n",
      "loss: 0.039027 [22501/60000]\n",
      "loss: 0.008430 [22601/60000]\n",
      "loss: 0.033617 [22701/60000]\n",
      "loss: 1.390681 [22801/60000]\n",
      "loss: 0.006419 [22901/60000]\n",
      "loss: 0.008975 [23001/60000]\n",
      "loss: 2.470106 [23101/60000]\n",
      "loss: 0.446338 [23201/60000]\n",
      "loss: 0.145641 [23301/60000]\n",
      "loss: 0.789641 [23401/60000]\n",
      "loss: 0.036743 [23501/60000]\n",
      "loss: 0.068539 [23601/60000]\n",
      "loss: 0.551414 [23701/60000]\n",
      "loss: 0.041745 [23801/60000]\n",
      "loss: 0.168936 [23901/60000]\n",
      "loss: 0.144979 [24001/60000]\n",
      "loss: 0.044109 [24101/60000]\n",
      "loss: 0.025826 [24201/60000]\n",
      "loss: 0.962590 [24301/60000]\n",
      "loss: 0.006517 [24401/60000]\n",
      "loss: 0.022202 [24501/60000]\n",
      "loss: 0.137093 [24601/60000]\n",
      "loss: 0.003830 [24701/60000]\n",
      "loss: 0.045806 [24801/60000]\n",
      "loss: 0.078315 [24901/60000]\n",
      "loss: 1.217725 [25001/60000]\n",
      "loss: 0.312235 [25101/60000]\n",
      "loss: 0.715946 [25201/60000]\n",
      "loss: 0.383919 [25301/60000]\n",
      "loss: 0.351763 [25401/60000]\n",
      "loss: 0.009546 [25501/60000]\n",
      "loss: 0.037214 [25601/60000]\n",
      "loss: 0.062877 [25701/60000]\n",
      "loss: 5.706126 [25801/60000]\n",
      "loss: 1.167052 [25901/60000]\n",
      "loss: 0.165231 [26001/60000]\n",
      "loss: 0.448918 [26101/60000]\n",
      "loss: 0.061067 [26201/60000]\n",
      "loss: 3.551347 [26301/60000]\n",
      "loss: 0.131383 [26401/60000]\n",
      "loss: 0.114605 [26501/60000]\n",
      "loss: 2.160500 [26601/60000]\n",
      "loss: 0.215235 [26701/60000]\n",
      "loss: 0.002481 [26801/60000]\n",
      "loss: 3.203165 [26901/60000]\n",
      "loss: 0.818569 [27001/60000]\n",
      "loss: 1.370995 [27101/60000]\n",
      "loss: 0.164534 [27201/60000]\n",
      "loss: 0.041328 [27301/60000]\n",
      "loss: 0.051845 [27401/60000]\n",
      "loss: 0.101599 [27501/60000]\n",
      "loss: 0.005967 [27601/60000]\n",
      "loss: 0.610533 [27701/60000]\n",
      "loss: 0.130772 [27801/60000]\n",
      "loss: 0.101309 [27901/60000]\n",
      "loss: 0.037347 [28001/60000]\n",
      "loss: 0.063258 [28101/60000]\n",
      "loss: 0.902528 [28201/60000]\n",
      "loss: 0.058207 [28301/60000]\n",
      "loss: 0.023057 [28401/60000]\n",
      "loss: 0.032611 [28501/60000]\n",
      "loss: 0.320117 [28601/60000]\n",
      "loss: 0.314335 [28701/60000]\n",
      "loss: 0.152387 [28801/60000]\n",
      "loss: 0.015153 [28901/60000]\n",
      "loss: 0.062756 [29001/60000]\n",
      "loss: 0.467227 [29101/60000]\n",
      "loss: 0.015978 [29201/60000]\n",
      "loss: 0.697589 [29301/60000]\n",
      "loss: 1.128170 [29401/60000]\n",
      "loss: 0.048403 [29501/60000]\n",
      "loss: 0.068778 [29601/60000]\n",
      "loss: 0.245054 [29701/60000]\n",
      "loss: 1.033142 [29801/60000]\n",
      "loss: 1.522182 [29901/60000]\n",
      "loss: 0.011713 [30001/60000]\n",
      "loss: 0.157321 [30101/60000]\n",
      "loss: 0.110296 [30201/60000]\n",
      "loss: 0.008777 [30301/60000]\n",
      "loss: 0.313596 [30401/60000]\n",
      "loss: 0.236162 [30501/60000]\n",
      "loss: 2.575184 [30601/60000]\n",
      "loss: 0.065728 [30701/60000]\n",
      "loss: 0.195528 [30801/60000]\n",
      "loss: 4.674927 [30901/60000]\n",
      "loss: 1.864910 [31001/60000]\n",
      "loss: 0.421660 [31101/60000]\n",
      "loss: 3.563569 [31201/60000]\n",
      "loss: 0.172818 [31301/60000]\n",
      "loss: 0.226082 [31401/60000]\n",
      "loss: 1.072699 [31501/60000]\n",
      "loss: 3.783630 [31601/60000]\n",
      "loss: 0.279930 [31701/60000]\n",
      "loss: 0.672922 [31801/60000]\n",
      "loss: 0.718056 [31901/60000]\n",
      "loss: 0.034438 [32001/60000]\n",
      "loss: 0.681753 [32101/60000]\n",
      "loss: 0.110710 [32201/60000]\n",
      "loss: 0.034475 [32301/60000]\n",
      "loss: 0.614066 [32401/60000]\n",
      "loss: 0.614639 [32501/60000]\n",
      "loss: 0.092604 [32601/60000]\n",
      "loss: 0.013105 [32701/60000]\n",
      "loss: 0.016372 [32801/60000]\n",
      "loss: 0.034712 [32901/60000]\n",
      "loss: 0.905521 [33001/60000]\n",
      "loss: 0.001929 [33101/60000]\n",
      "loss: 0.608544 [33201/60000]\n",
      "loss: 0.171651 [33301/60000]\n",
      "loss: 0.049378 [33401/60000]\n",
      "loss: 0.180876 [33501/60000]\n",
      "loss: 0.032427 [33601/60000]\n",
      "loss: 0.152885 [33701/60000]\n",
      "loss: 0.023463 [33801/60000]\n",
      "loss: 0.761424 [33901/60000]\n",
      "loss: 0.194662 [34001/60000]\n",
      "loss: 0.324450 [34101/60000]\n",
      "loss: 0.020994 [34201/60000]\n",
      "loss: 0.523472 [34301/60000]\n",
      "loss: 0.147169 [34401/60000]\n",
      "loss: 5.083580 [34501/60000]\n",
      "loss: 0.233334 [34601/60000]\n",
      "loss: 0.246374 [34701/60000]\n",
      "loss: 3.114969 [34801/60000]\n",
      "loss: 0.156589 [34901/60000]\n",
      "loss: 0.010984 [35001/60000]\n",
      "loss: 0.696261 [35101/60000]\n",
      "loss: 0.036567 [35201/60000]\n",
      "loss: 0.254823 [35301/60000]\n",
      "loss: 0.000768 [35401/60000]\n",
      "loss: 1.056696 [35501/60000]\n",
      "loss: 0.024424 [35601/60000]\n",
      "loss: 0.000264 [35701/60000]\n",
      "loss: 0.001485 [35801/60000]\n",
      "loss: 0.006348 [35901/60000]\n",
      "loss: 1.030469 [36001/60000]\n",
      "loss: 0.186535 [36101/60000]\n",
      "loss: 0.156222 [36201/60000]\n",
      "loss: 0.142515 [36301/60000]\n",
      "loss: 0.044311 [36401/60000]\n",
      "loss: 0.527041 [36501/60000]\n",
      "loss: 0.257057 [36601/60000]\n",
      "loss: 0.013829 [36701/60000]\n",
      "loss: 0.283923 [36801/60000]\n",
      "loss: 0.962478 [36901/60000]\n",
      "loss: 0.023980 [37001/60000]\n",
      "loss: 1.856211 [37101/60000]\n",
      "loss: 0.004474 [37201/60000]\n",
      "loss: 0.146510 [37301/60000]\n",
      "loss: 0.519574 [37401/60000]\n",
      "loss: 0.017412 [37501/60000]\n",
      "loss: 0.172483 [37601/60000]\n",
      "loss: 0.032593 [37701/60000]\n",
      "loss: 0.040835 [37801/60000]\n",
      "loss: 0.121474 [37901/60000]\n",
      "loss: 0.213859 [38001/60000]\n",
      "loss: 0.006189 [38101/60000]\n",
      "loss: 0.000087 [38201/60000]\n",
      "loss: 0.556273 [38301/60000]\n",
      "loss: 0.014477 [38401/60000]\n",
      "loss: 0.042494 [38501/60000]\n",
      "loss: 0.129894 [38601/60000]\n",
      "loss: 0.773918 [38701/60000]\n",
      "loss: 0.763013 [38801/60000]\n",
      "loss: 0.038650 [38901/60000]\n",
      "loss: 0.889616 [39001/60000]\n",
      "loss: 0.177947 [39101/60000]\n",
      "loss: 0.002869 [39201/60000]\n",
      "loss: 0.022155 [39301/60000]\n",
      "loss: 0.155056 [39401/60000]\n",
      "loss: 0.260561 [39501/60000]\n",
      "loss: 2.524297 [39601/60000]\n",
      "loss: 1.081021 [39701/60000]\n",
      "loss: 0.006562 [39801/60000]\n",
      "loss: 0.018109 [39901/60000]\n",
      "loss: 0.002181 [40001/60000]\n",
      "loss: 0.284623 [40101/60000]\n",
      "loss: 0.054010 [40201/60000]\n",
      "loss: 0.740011 [40301/60000]\n",
      "loss: 0.003543 [40401/60000]\n",
      "loss: 0.028713 [40501/60000]\n",
      "loss: 0.145408 [40601/60000]\n",
      "loss: 0.446545 [40701/60000]\n",
      "loss: 0.231097 [40801/60000]\n",
      "loss: 0.136533 [40901/60000]\n",
      "loss: 0.149712 [41001/60000]\n",
      "loss: 0.058302 [41101/60000]\n",
      "loss: 1.340832 [41201/60000]\n",
      "loss: 0.056587 [41301/60000]\n",
      "loss: 1.038752 [41401/60000]\n",
      "loss: 0.313333 [41501/60000]\n",
      "loss: 0.037241 [41601/60000]\n",
      "loss: 0.979084 [41701/60000]\n",
      "loss: 0.014415 [41801/60000]\n",
      "loss: 1.066867 [41901/60000]\n",
      "loss: 0.012149 [42001/60000]\n",
      "loss: 0.061750 [42101/60000]\n",
      "loss: 0.102290 [42201/60000]\n",
      "loss: 0.002434 [42301/60000]\n",
      "loss: 0.471195 [42401/60000]\n",
      "loss: 0.073699 [42501/60000]\n",
      "loss: 0.053435 [42601/60000]\n",
      "loss: 0.010964 [42701/60000]\n",
      "loss: 0.079249 [42801/60000]\n",
      "loss: 0.021224 [42901/60000]\n",
      "loss: 0.905389 [43001/60000]\n",
      "loss: 0.167333 [43101/60000]\n",
      "loss: 0.888645 [43201/60000]\n",
      "loss: 0.040439 [43301/60000]\n",
      "loss: 0.274947 [43401/60000]\n",
      "loss: 0.062644 [43501/60000]\n",
      "loss: 0.219577 [43601/60000]\n",
      "loss: 0.156410 [43701/60000]\n",
      "loss: 0.163195 [43801/60000]\n",
      "loss: 0.415386 [43901/60000]\n",
      "loss: 0.013884 [44001/60000]\n",
      "loss: 1.828244 [44101/60000]\n",
      "loss: 0.258537 [44201/60000]\n",
      "loss: 1.514784 [44301/60000]\n",
      "loss: 0.037950 [44401/60000]\n",
      "loss: 0.212244 [44501/60000]\n",
      "loss: 0.127261 [44601/60000]\n",
      "loss: 0.001773 [44701/60000]\n",
      "loss: 0.356037 [44801/60000]\n",
      "loss: 0.003806 [44901/60000]\n",
      "loss: 0.090617 [45001/60000]\n",
      "loss: 0.044084 [45101/60000]\n",
      "loss: 0.003264 [45201/60000]\n",
      "loss: 0.382950 [45301/60000]\n",
      "loss: 0.011074 [45401/60000]\n",
      "loss: 0.349783 [45501/60000]\n",
      "loss: 0.142436 [45601/60000]\n",
      "loss: 0.313727 [45701/60000]\n",
      "loss: 4.900605 [45801/60000]\n",
      "loss: 0.047610 [45901/60000]\n",
      "loss: 0.228034 [46001/60000]\n",
      "loss: 0.291555 [46101/60000]\n",
      "loss: 0.867074 [46201/60000]\n",
      "loss: 4.417084 [46301/60000]\n",
      "loss: 0.068164 [46401/60000]\n",
      "loss: 0.003897 [46501/60000]\n",
      "loss: 0.182193 [46601/60000]\n",
      "loss: 0.261476 [46701/60000]\n",
      "loss: 0.077634 [46801/60000]\n",
      "loss: 0.006891 [46901/60000]\n",
      "loss: 0.015972 [47001/60000]\n",
      "loss: 1.113765 [47101/60000]\n",
      "loss: 0.039238 [47201/60000]\n",
      "loss: 0.409534 [47301/60000]\n",
      "loss: 0.033890 [47401/60000]\n",
      "loss: 0.036642 [47501/60000]\n",
      "loss: 3.018127 [47601/60000]\n",
      "loss: 0.002754 [47701/60000]\n",
      "loss: 0.001320 [47801/60000]\n",
      "loss: 0.000488 [47901/60000]\n",
      "loss: 0.083832 [48001/60000]\n",
      "loss: 0.001417 [48101/60000]\n",
      "loss: 0.220662 [48201/60000]\n",
      "loss: 0.078545 [48301/60000]\n",
      "loss: 0.072157 [48401/60000]\n",
      "loss: 0.160239 [48501/60000]\n",
      "loss: 0.012563 [48601/60000]\n",
      "loss: 0.310532 [48701/60000]\n",
      "loss: 1.378174 [48801/60000]\n",
      "loss: 0.249646 [48901/60000]\n",
      "loss: 0.032217 [49001/60000]\n",
      "loss: 0.130147 [49101/60000]\n",
      "loss: 0.729573 [49201/60000]\n",
      "loss: 3.434250 [49301/60000]\n",
      "loss: 0.327527 [49401/60000]\n",
      "loss: 1.703243 [49501/60000]\n",
      "loss: 0.022281 [49601/60000]\n",
      "loss: 0.368536 [49701/60000]\n",
      "loss: 0.203643 [49801/60000]\n",
      "loss: 0.024516 [49901/60000]\n",
      "loss: 0.146853 [50001/60000]\n",
      "loss: 0.003509 [50101/60000]\n",
      "loss: 0.004194 [50201/60000]\n",
      "loss: 0.040783 [50301/60000]\n",
      "loss: 0.034032 [50401/60000]\n",
      "loss: 0.120464 [50501/60000]\n",
      "loss: 0.005061 [50601/60000]\n",
      "loss: 0.786561 [50701/60000]\n",
      "loss: 0.043425 [50801/60000]\n",
      "loss: 0.032592 [50901/60000]\n",
      "loss: 0.016153 [51001/60000]\n",
      "loss: 0.259271 [51101/60000]\n",
      "loss: 2.013327 [51201/60000]\n",
      "loss: 3.777465 [51301/60000]\n",
      "loss: 0.039802 [51401/60000]\n",
      "loss: 0.013599 [51501/60000]\n",
      "loss: 3.553119 [51601/60000]\n",
      "loss: 0.008629 [51701/60000]\n",
      "loss: 0.067409 [51801/60000]\n",
      "loss: 0.028239 [51901/60000]\n",
      "loss: 0.060117 [52001/60000]\n",
      "loss: 0.834336 [52101/60000]\n",
      "loss: 0.170378 [52201/60000]\n",
      "loss: 0.002435 [52301/60000]\n",
      "loss: 0.042267 [52401/60000]\n",
      "loss: 0.064326 [52501/60000]\n",
      "loss: 0.311400 [52601/60000]\n",
      "loss: 0.028213 [52701/60000]\n",
      "loss: 3.177749 [52801/60000]\n",
      "loss: 0.160467 [52901/60000]\n",
      "loss: 0.007515 [53001/60000]\n",
      "loss: 0.014387 [53101/60000]\n",
      "loss: 0.055984 [53201/60000]\n",
      "loss: 0.270577 [53301/60000]\n",
      "loss: 0.003108 [53401/60000]\n",
      "loss: 0.227280 [53501/60000]\n",
      "loss: 0.234135 [53601/60000]\n",
      "loss: 0.191237 [53701/60000]\n",
      "loss: 0.040321 [53801/60000]\n",
      "loss: 0.738760 [53901/60000]\n",
      "loss: 0.571201 [54001/60000]\n",
      "loss: 0.270574 [54101/60000]\n",
      "loss: 0.155554 [54201/60000]\n",
      "loss: 0.109462 [54301/60000]\n",
      "loss: 0.212353 [54401/60000]\n",
      "loss: 0.010580 [54501/60000]\n",
      "loss: 0.330317 [54601/60000]\n",
      "loss: 0.005884 [54701/60000]\n",
      "loss: 0.005551 [54801/60000]\n",
      "loss: 0.012555 [54901/60000]\n",
      "loss: 0.039619 [55001/60000]\n",
      "loss: 0.010886 [55101/60000]\n",
      "loss: 0.023106 [55201/60000]\n",
      "loss: 0.001524 [55301/60000]\n",
      "loss: 0.683596 [55401/60000]\n",
      "loss: 0.062635 [55501/60000]\n",
      "loss: 0.081623 [55601/60000]\n",
      "loss: 0.002254 [55701/60000]\n",
      "loss: 0.082359 [55801/60000]\n",
      "loss: 0.012516 [55901/60000]\n",
      "loss: 0.026264 [56001/60000]\n",
      "loss: 0.001300 [56101/60000]\n",
      "loss: 0.017900 [56201/60000]\n",
      "loss: 2.955956 [56301/60000]\n",
      "loss: 0.058197 [56401/60000]\n",
      "loss: 0.021216 [56501/60000]\n",
      "loss: 1.934702 [56601/60000]\n",
      "loss: 0.021746 [56701/60000]\n",
      "loss: 3.552365 [56801/60000]\n",
      "loss: 0.021935 [56901/60000]\n",
      "loss: 0.000145 [57001/60000]\n",
      "loss: 0.092697 [57101/60000]\n",
      "loss: 0.186725 [57201/60000]\n",
      "loss: 0.029901 [57301/60000]\n",
      "loss: 0.198943 [57401/60000]\n",
      "loss: 0.002445 [57501/60000]\n",
      "loss: 0.003126 [57601/60000]\n",
      "loss: 0.035042 [57701/60000]\n",
      "loss: 0.065259 [57801/60000]\n",
      "loss: 0.143508 [57901/60000]\n",
      "loss: 0.263480 [58001/60000]\n",
      "loss: 0.667503 [58101/60000]\n",
      "loss: 0.001545 [58201/60000]\n",
      "loss: 0.008847 [58301/60000]\n",
      "loss: 0.024178 [58401/60000]\n",
      "loss: 0.012381 [58501/60000]\n",
      "loss: 0.020868 [58601/60000]\n",
      "loss: 0.046676 [58701/60000]\n",
      "loss: 0.038996 [58801/60000]\n",
      "loss: 0.225659 [58901/60000]\n",
      "loss: 0.041535 [59001/60000]\n",
      "loss: 0.005276 [59101/60000]\n",
      "loss: 0.025149 [59201/60000]\n",
      "loss: 0.009797 [59301/60000]\n",
      "loss: 2.185189 [59401/60000]\n",
      "loss: 0.018729 [59501/60000]\n",
      "loss: 0.083233 [59601/60000]\n",
      "loss: 0.001804 [59701/60000]\n",
      "loss: 0.024464 [59801/60000]\n",
      "loss: 0.033861 [59901/60000]\n",
      "Training Error: \n",
      " Accuracy: 85.66500%\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b710ecbd-6d8d-4836-85dc-9cb0059090bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1448, -0.0842, -0.0713, -0.1799, -0.0463, -0.1540,  0.0068,  0.2833,\n",
       "         0.2197,  0.0625])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5638e652-3c29-4b64-8ebb-8d13ab398c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2784)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data[50000].reshape(784)[653]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daab6bd-e549-4321-b39e-b33e934c34d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "65aa6140-7727-47d8-82bf-86c7dbb78afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 2.107958 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6720)"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "07eeddf3-477d-45e6-8350-741c2dfbd219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0086,  0.0015,  0.0017, -0.0255, -0.0140, -0.0105, -0.0166,  0.0336,\n",
       "         0.0040,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320,  0.0282,  0.0313,\n",
       "         0.0468,  0.0109])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "b1c77e31-daa3-4108-8307-56ff38fce7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x,y) = train_dataloader.dataset[500]\n",
    "x,y = x.flatten(), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "3d0908c7-e944-4760-889d-d4bb27890283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3171)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = forward(x)\n",
    "cross_entropy(a[-1],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb53441-be1d-484e-9861-3161779e2d14",
   "metadata": {},
   "source": [
    "## Claude code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "id": "146a1c94-766d-4283-94dc-cd1e25ccb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradients(f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical gradients of a function f at a point x.\n",
    "    \"\"\"\n",
    "    x = x.detach().requires_grad_(True)\n",
    "    grads = []\n",
    "    for i in range(x.nelement()):\n",
    "        ori_x = x.clone()\n",
    "        ori_x.flatten()[i] += eps\n",
    "        y_pos = f(ori_x)\n",
    "        ori_x.flatten()[i] -= 2 * eps\n",
    "        y_neg = f(ori_x)\n",
    "        grads.append((y_pos - y_neg) / (2 * eps))\n",
    "    return torch.stack(grads).view_as(x)\n",
    "\n",
    "def compare_gradients(f, x, eps=1e-3):\n",
    "    \"\"\"\n",
    "    Compare analytical and numerical gradients of a function f at a point x.\n",
    "    \"\"\"\n",
    "    x = x.detach().requires_grad_(True)\n",
    "    analytical_grad = torch.autograd.grad(f(x), x, create_graph=True)[0]\n",
    "    numerical_grad = numerical_gradients(f, x, eps)\n",
    "    diff = torch.abs(analytical_grad - numerical_grad)\n",
    "    # return analytical_grad,numerical_grad\n",
    "    return torch.allclose(analytical_grad, numerical_grad, atol=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "id": "53a4e29a-4da7-42fb-bbf1-c3150428df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y):\n",
    "    y_pred = softmax(forward(x)[-1])\n",
    "    return cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "a1177b34-1b31-4772-bc23-ba5d78359b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check before backward pass: True\n",
      "Gradient check after backward pass: True\n"
     ]
    }
   ],
   "source": [
    "iter_ = iter(train_dl)\n",
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())\n",
    "\n",
    "x1, y1 = next(iter_)\n",
    "x1, y1 = x1.flatten(), y1.squeeze()\n",
    "\n",
    "# Check gradients before the backward pass\n",
    "# a1,n1 = compare_gradients(lambda x: compute_loss(x, y1), x1)\n",
    "print(\"Gradient check before backward pass:\", compare_gradients(lambda x: compute_loss(x, y1), x1))\n",
    "\n",
    "a = forward(x1)\n",
    "loss = cross_entropy(softmax(a[-1]), y1)\n",
    "backward(a, y1)\n",
    "\n",
    "# Check gradients after the backward pass\n",
    "# a2,n2 = compare_gradients(lambda x: compute_loss(x, y1), x1)\n",
    "print(\"Gradient check after backward pass:\", compare_gradients(lambda x: compute_loss(x, y1), x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "9c8466e5-176e-4cf5-86d8-9c052f8cfeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1448, -0.0842, -0.0713, -0.1799, -0.0463, -0.1540,  0.0068,  0.2833,\n",
       "         0.2197,  0.0625])"
      ]
     },
     "execution_count": 1324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "8d66e5fd-d49d-4210-82ea-4146ff55c923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4099e-03, -5.8973e-03, -7.6170e-03,  9.3721e-03, -1.3587e-02,\n",
       "         1.8532e-02,  1.4251e-02,  3.1801e-04,  3.0352e-03, -3.2623e-03,\n",
       "        -1.5701e-04,  3.3171e-03,  1.7362e-03, -1.6741e-02, -1.0981e-02,\n",
       "         8.7031e-03, -1.7773e-03, -1.4019e-02,  4.1619e-03, -1.4817e-02,\n",
       "         1.1686e-02,  2.3578e-02,  5.8366e-03, -2.1297e-02,  1.2433e-02,\n",
       "         6.0295e-03, -3.1840e-03,  1.0132e-02,  4.2972e-03,  1.6210e-03,\n",
       "         1.0653e-02,  7.5712e-03, -5.2470e-03,  5.2288e-03, -1.8128e-03,\n",
       "         1.1832e-02,  3.6505e-04, -7.4102e-03,  4.9205e-03, -2.8104e-03,\n",
       "         2.3235e-03,  5.7258e-03,  5.6098e-03, -2.7119e-02,  3.5684e-03,\n",
       "        -4.9104e-03, -2.0233e-02, -1.5040e-02,  1.9762e-03,  1.2869e-02,\n",
       "        -1.3593e-02,  2.6663e-03,  8.3546e-04,  1.1420e-04, -2.2234e-03,\n",
       "        -7.7329e-03,  1.3043e-02,  1.0374e-03, -3.6343e-03,  1.6006e-02,\n",
       "        -4.2107e-03,  6.9430e-03,  3.9188e-03, -1.1052e-02, -1.0793e-02,\n",
       "        -2.7822e-03,  6.6480e-03, -1.3865e-02, -6.4596e-03, -4.3614e-03,\n",
       "         7.9239e-03, -1.7550e-02,  8.4070e-03,  1.7274e-03,  2.3647e-04,\n",
       "         9.5655e-03,  7.8308e-03,  3.3702e-03, -1.5272e-02,  1.1356e-02,\n",
       "         9.2493e-04, -1.6476e-02, -3.4568e-03,  4.0190e-03,  6.6195e-03,\n",
       "         1.8404e-03,  2.3134e-03, -2.6442e-03, -8.5248e-03, -8.0906e-03,\n",
       "        -1.2538e-02, -5.4577e-03, -1.3421e-03,  6.5981e-03,  7.6074e-03,\n",
       "        -4.5457e-03, -1.8668e-03, -1.7189e-02, -8.5475e-03,  2.5048e-03,\n",
       "        -1.0515e-03, -3.4921e-03,  5.5269e-03,  8.9754e-03, -2.3449e-03,\n",
       "        -6.3000e-03,  1.8980e-03,  1.6653e-02, -6.6904e-03, -5.8078e-03,\n",
       "        -3.4411e-04, -1.0169e-03, -1.2014e-02,  9.5344e-03,  1.4412e-02,\n",
       "        -3.7068e-03, -2.3374e-03, -1.2199e-03, -1.1700e-04, -5.9668e-03,\n",
       "         1.1483e-04, -7.1202e-03, -2.7524e-03,  4.5145e-03,  4.5223e-03,\n",
       "         2.9892e-03, -6.6641e-03,  1.5674e-03,  2.4297e-04,  2.2726e-03,\n",
       "        -1.5749e-03, -2.9594e-04, -4.5969e-03, -1.0677e-03, -2.4302e-03,\n",
       "         1.1766e-02,  2.9497e-03, -5.6257e-03, -1.7224e-02, -2.3138e-02,\n",
       "         1.1275e-02, -3.0647e-03, -1.0379e-02, -1.1667e-02, -7.0770e-03,\n",
       "         9.9773e-03, -4.6209e-03, -4.5869e-04, -3.5971e-03,  3.9822e-03,\n",
       "        -5.7489e-03,  1.0972e-02,  2.1936e-04, -3.8459e-04,  6.6689e-03,\n",
       "         1.5479e-02,  1.0327e-02, -2.6382e-03,  3.2197e-03, -6.8177e-03,\n",
       "         3.0987e-03,  1.3005e-02, -2.7875e-02,  3.3079e-03,  8.7369e-04,\n",
       "        -7.1141e-03, -1.3656e-02, -2.9257e-03,  2.4826e-04, -4.9001e-03,\n",
       "        -4.8084e-03,  2.8515e-03,  1.1481e-02,  9.2251e-03, -8.1557e-04,\n",
       "        -1.3310e-02, -4.6017e-03,  1.9773e-02,  6.0751e-03, -1.1226e-02,\n",
       "         3.2344e-04,  8.0715e-03,  4.6232e-03, -9.9197e-03, -3.3058e-03,\n",
       "        -6.0332e-03,  1.3313e-02,  1.4637e-02,  1.2196e-02,  4.7105e-03,\n",
       "         1.6894e-02, -1.4839e-02,  6.4173e-03, -8.8699e-03, -1.2259e-03,\n",
       "         1.0395e-02, -6.1168e-03, -1.3281e-02, -4.9896e-03, -1.7774e-03,\n",
       "         2.8195e-03, -8.3501e-03, -3.9648e-03, -7.2498e-03,  1.0444e-02,\n",
       "         5.2402e-03, -7.5102e-04, -1.3946e-02, -1.9806e-02, -2.3433e-03,\n",
       "         1.0891e-02,  1.7984e-04,  1.9758e-03,  3.3495e-03,  1.8413e-04,\n",
       "         5.0109e-03,  5.0441e-03, -1.6611e-03,  5.2713e-03,  1.8397e-02,\n",
       "        -6.5242e-03,  9.3545e-03,  5.1630e-03, -7.2108e-03, -1.8343e-02,\n",
       "        -1.7217e-02, -8.1912e-03,  1.2153e-03, -3.4784e-04, -1.0236e-02,\n",
       "        -1.8553e-03, -1.5326e-02,  4.8288e-03,  1.1620e-02,  6.3111e-03,\n",
       "         4.1687e-03,  7.4551e-04, -2.3931e-03, -1.1667e-02,  8.7616e-03,\n",
       "        -1.5229e-02, -4.5863e-03,  7.6415e-03,  5.5382e-03,  4.8949e-03,\n",
       "         1.2130e-02,  6.0513e-03, -6.9740e-04, -1.3843e-02, -2.0388e-03,\n",
       "        -8.8167e-05,  1.6524e-03, -7.2370e-03,  8.0734e-03,  1.3498e-03,\n",
       "         4.7087e-03, -4.5088e-03,  9.4298e-04,  2.4713e-03, -1.4918e-03,\n",
       "        -7.9033e-03, -3.0955e-03,  2.7307e-04,  6.1059e-03, -4.6420e-03,\n",
       "        -1.4155e-02, -1.9085e-02, -3.2429e-04,  1.2520e-02,  5.4182e-04,\n",
       "        -1.8567e-03,  4.5324e-03,  1.4740e-03,  4.3761e-05,  9.8130e-03,\n",
       "        -8.0744e-04, -4.0639e-03,  1.7967e-02, -1.6276e-02,  2.1985e-03,\n",
       "        -3.6617e-03, -2.4022e-02, -8.2151e-03, -1.3133e-02, -5.6604e-03,\n",
       "        -9.6414e-04,  1.6027e-02, -7.3653e-03,  1.0229e-02,  6.1541e-03,\n",
       "         2.0389e-03, -6.2851e-03, -5.1559e-03,  1.1564e-02, -7.8008e-03,\n",
       "         1.0226e-03, -7.3790e-03,  2.0597e-02, -1.2173e-02,  1.4342e-02,\n",
       "         8.9762e-03, -8.0176e-03, -8.1114e-03, -6.0139e-03,  8.9137e-03,\n",
       "        -8.7085e-03,  4.6991e-04,  9.0568e-03, -1.0402e-03,  9.5042e-03,\n",
       "         1.0163e-03, -1.8754e-02, -1.8111e-03, -9.1153e-03,  5.2916e-03,\n",
       "        -3.6385e-03,  2.5126e-03,  7.7073e-03,  2.1627e-02,  5.2083e-03,\n",
       "        -1.2255e-02, -3.4397e-03,  1.5714e-03, -1.2761e-02,  3.0959e-03,\n",
       "        -1.0912e-02, -4.1552e-03, -1.8927e-03,  2.7701e-04, -1.1190e-02,\n",
       "        -1.8567e-02, -1.8549e-03, -5.4450e-03, -1.3004e-02, -1.4727e-02,\n",
       "        -5.9650e-03,  2.0184e-03, -1.1849e-03, -7.6520e-03,  8.7759e-04,\n",
       "        -2.1067e-03, -7.4472e-03, -7.2434e-04,  2.6528e-03, -9.0449e-03,\n",
       "        -1.3383e-02,  2.0514e-03, -1.2844e-02,  8.7124e-05, -1.5537e-02,\n",
       "        -1.1127e-02, -5.3911e-03, -4.3707e-04,  6.9538e-03,  1.5854e-03,\n",
       "        -1.2558e-02, -5.1060e-03, -1.4002e-02,  3.8105e-04, -5.0221e-03,\n",
       "        -1.2166e-03,  2.7381e-03, -1.7397e-03,  7.3918e-04,  5.1693e-03,\n",
       "        -1.8123e-02, -2.9255e-04,  3.2706e-03, -1.3848e-02, -1.1469e-03,\n",
       "        -5.1215e-03,  8.4054e-03,  1.5089e-02,  2.3299e-04, -4.6183e-03,\n",
       "        -1.0062e-02, -6.5577e-03, -1.8872e-02,  5.3024e-03,  6.6901e-03,\n",
       "        -3.6001e-03,  5.5449e-03,  7.6214e-03,  1.9560e-02,  3.5908e-04,\n",
       "        -1.2380e-02,  3.5623e-03,  2.0561e-03,  5.0804e-03,  7.0595e-03,\n",
       "        -8.1178e-04, -3.5618e-03, -1.0151e-02, -4.2405e-03,  4.1027e-03,\n",
       "        -1.3565e-02, -6.0112e-03,  5.8766e-03,  2.3024e-03, -5.5064e-03,\n",
       "        -1.3278e-02, -3.5608e-03, -1.3636e-02,  1.9163e-04,  1.0642e-02,\n",
       "         8.4645e-03,  1.8459e-03, -6.2956e-03, -1.5858e-02,  1.0532e-02,\n",
       "         1.3651e-03,  5.8069e-03, -4.3518e-03, -1.1449e-03,  6.0804e-03,\n",
       "         7.4888e-03,  3.7276e-04, -3.7614e-03,  8.1770e-03,  2.8113e-04,\n",
       "         7.2803e-03, -1.3594e-02,  1.4523e-03,  8.8777e-03,  5.5844e-03,\n",
       "        -7.5109e-03,  8.2332e-04,  1.1009e-02,  1.7314e-03, -6.5563e-03,\n",
       "         1.1770e-03, -2.5648e-02, -1.4050e-02, -4.9626e-03, -6.6830e-03,\n",
       "         8.5499e-03,  1.1658e-02,  1.5588e-03, -2.9875e-03,  2.7139e-03,\n",
       "        -2.3051e-03,  3.1574e-03, -5.0225e-03,  4.9964e-03,  3.2355e-03,\n",
       "        -1.6583e-03, -1.7263e-03,  2.7383e-03, -5.1868e-03, -6.9864e-03,\n",
       "         9.7265e-03, -3.7918e-04,  1.1752e-02, -1.4173e-03,  6.8879e-03,\n",
       "         8.4187e-03,  6.7893e-03,  8.3180e-04,  7.9386e-04,  1.6547e-02,\n",
       "        -8.7560e-03,  1.6030e-03, -1.6827e-02,  3.2125e-03, -6.4778e-03,\n",
       "        -4.4987e-03, -1.5815e-02,  9.1193e-03, -6.2894e-03,  4.1059e-03,\n",
       "         1.5121e-03,  5.9670e-03, -8.6802e-03, -7.8893e-03,  1.3727e-03,\n",
       "        -4.5477e-04,  9.9684e-03,  5.2702e-03,  4.9441e-03, -1.0778e-02,\n",
       "        -2.2038e-03, -7.5814e-03,  1.3740e-02,  1.3796e-02,  5.2578e-03,\n",
       "        -1.1510e-02, -4.6208e-03,  5.9539e-04,  1.6319e-03, -5.1588e-03,\n",
       "        -5.4758e-03, -7.0699e-03,  6.6627e-03,  1.6765e-04,  4.7632e-03,\n",
       "        -1.7272e-03,  6.3051e-03, -1.1557e-02,  3.2794e-03, -6.7268e-04,\n",
       "        -5.9919e-03, -7.3828e-03,  1.5227e-02, -4.6141e-03, -1.1370e-02,\n",
       "        -1.3739e-02,  2.0686e-03,  1.6610e-02,  7.0146e-03,  1.7413e-03,\n",
       "        -5.2371e-03,  1.5631e-03,  4.8202e-03,  1.0403e-02,  2.9125e-03,\n",
       "        -9.0912e-03,  3.2042e-04,  3.7332e-03, -1.0017e-02, -4.4182e-03,\n",
       "         2.7649e-02,  5.1258e-03, -2.3627e-04,  4.3986e-03,  2.5446e-04,\n",
       "        -1.8107e-04,  1.8142e-02,  1.3015e-02,  1.0839e-02,  9.5545e-03,\n",
       "         2.5547e-03,  4.3382e-03, -5.6168e-03,  3.7425e-03,  8.9823e-03,\n",
       "         1.4866e-02, -1.0268e-02, -1.5245e-02, -1.7416e-02, -1.7034e-03,\n",
       "         8.4295e-03,  9.5766e-03, -1.5505e-02,  6.2981e-03,  7.6794e-03,\n",
       "        -8.6596e-03, -1.6815e-02, -2.1939e-03,  4.8445e-03,  1.8321e-04,\n",
       "        -3.2825e-03,  6.2843e-03,  1.6510e-03,  1.5077e-02, -1.8540e-04,\n",
       "        -7.4759e-03,  5.3255e-03, -1.4583e-02, -6.8708e-03,  8.8637e-03,\n",
       "        -9.9750e-03, -1.1274e-03,  8.8297e-03, -1.1857e-02, -5.3389e-04,\n",
       "         3.0451e-03, -5.5876e-03,  1.4719e-03,  4.0652e-03, -1.1362e-02,\n",
       "        -7.7890e-06, -4.2905e-03, -6.5442e-03,  1.4229e-02, -6.5763e-03,\n",
       "        -3.5385e-04,  1.5765e-03,  5.3690e-03, -8.2157e-03, -6.6164e-03,\n",
       "         2.6804e-03, -7.1458e-04, -1.6770e-02, -1.7920e-02,  7.7999e-03,\n",
       "         1.8601e-02, -2.9108e-03, -6.7457e-03, -3.7183e-03, -1.9293e-02,\n",
       "        -9.8036e-03,  7.5849e-03, -7.0472e-03,  2.4486e-03,  1.3608e-03,\n",
       "         1.0098e-02, -3.1317e-03, -1.6971e-03,  8.9465e-04, -2.1870e-03,\n",
       "         9.5605e-03,  1.7231e-02,  3.3754e-03, -5.6542e-03,  7.2483e-03,\n",
       "         1.1144e-02, -6.1963e-03, -4.6786e-04,  4.3460e-03, -2.2002e-03,\n",
       "         3.4251e-03, -6.7473e-03,  1.4087e-02, -1.8993e-02,  1.7786e-02,\n",
       "        -9.8379e-03, -5.4634e-03,  1.2873e-03, -2.0041e-02, -1.1377e-02,\n",
       "         2.6995e-02,  7.2226e-05, -7.9888e-03,  2.3203e-03, -1.8296e-02,\n",
       "         1.5127e-03,  2.0797e-02, -3.5582e-03,  9.1086e-03, -7.3239e-04,\n",
       "         5.7407e-03,  4.2886e-03,  9.8906e-03, -8.6811e-03, -6.2283e-03,\n",
       "        -7.7144e-03,  1.8123e-03, -6.2366e-03,  1.7517e-02, -5.0139e-03,\n",
       "        -1.9107e-02, -9.8813e-03, -8.3628e-03,  3.8114e-03, -8.3182e-03,\n",
       "         1.0935e-02, -1.2879e-02,  1.3855e-02, -2.2307e-03, -2.0496e-03,\n",
       "        -2.9741e-04, -2.0361e-02, -1.2161e-02,  3.3543e-03, -8.0695e-03,\n",
       "         9.3348e-03,  2.1811e-02, -6.3629e-03, -1.9528e-02, -9.8233e-03,\n",
       "         3.7842e-03,  2.1074e-02, -6.8755e-03,  1.2912e-02,  1.0363e-02,\n",
       "         7.3825e-03, -1.1114e-02, -6.4901e-03, -1.2932e-02,  1.6797e-02,\n",
       "         6.2250e-03, -1.0451e-03,  1.0846e-02,  6.7275e-03,  4.1699e-03,\n",
       "         1.2190e-02,  6.5010e-05, -7.4951e-04,  7.9796e-03,  1.2950e-02,\n",
       "        -1.0756e-02,  5.9763e-03,  5.2459e-03, -1.2635e-02, -1.5376e-03,\n",
       "         5.4463e-03,  2.0340e-03, -5.0741e-04,  7.7490e-04,  1.5005e-02,\n",
       "        -2.2254e-03, -6.9149e-04, -4.5582e-05, -2.0616e-02, -5.0587e-03,\n",
       "         1.2123e-02, -3.6046e-03,  1.1589e-02,  2.3413e-03, -1.3134e-02,\n",
       "         1.7374e-02,  3.9336e-03, -8.8667e-03, -6.5015e-03, -3.7057e-03,\n",
       "        -1.3497e-02,  1.2065e-02,  4.5793e-03, -8.3119e-03,  6.9646e-03,\n",
       "         1.5163e-03,  3.8914e-04, -1.5402e-02,  8.7180e-03,  1.4165e-02,\n",
       "         7.2567e-03,  5.0829e-03,  9.2566e-03,  8.6523e-03,  1.8037e-02,\n",
       "         2.3719e-03, -8.1502e-04,  7.0155e-03,  6.5872e-03,  1.0450e-02,\n",
       "        -4.2342e-03,  1.0670e-03,  6.1607e-04,  1.0470e-03,  2.7049e-03,\n",
       "        -4.8361e-03, -1.2859e-02,  9.5885e-03, -8.0827e-03,  1.4245e-03,\n",
       "        -2.9118e-03,  8.2315e-03,  9.5470e-03, -2.8359e-03,  1.5556e-02,\n",
       "         1.0611e-02, -7.4114e-03,  2.7416e-03, -1.1110e-02, -1.4414e-02,\n",
       "         5.9046e-03,  7.5263e-03, -1.0933e-02,  4.5891e-03,  8.0689e-03,\n",
       "         1.9201e-02,  7.4032e-03,  6.1166e-03,  2.5036e-04, -1.9781e-02,\n",
       "        -5.2692e-03,  2.3589e-03, -2.0159e-02, -9.3808e-03, -1.1069e-03,\n",
       "        -6.5599e-03, -5.3730e-03, -1.9683e-02,  1.1728e-04,  4.2114e-03,\n",
       "         1.6481e-02,  6.3046e-03,  1.0214e-02,  1.4000e-02,  9.2786e-03,\n",
       "         7.8380e-03, -1.5826e-02, -1.5823e-03,  1.0538e-02,  9.7204e-03,\n",
       "         5.3063e-03,  1.6528e-02,  6.3525e-03, -4.7120e-03, -1.9497e-03,\n",
       "         1.2910e-02, -2.0331e-02, -2.2953e-03, -2.3942e-03],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 1325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "id": "7152b67d-015d-44e9-8164-379b47dbcd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0035, -0.0058, -0.0076,  0.0093, -0.0137,  0.0186,  0.0142,  0.0004,\n",
       "         0.0030, -0.0032, -0.0001,  0.0032,  0.0018, -0.0168, -0.0111,  0.0088,\n",
       "        -0.0017, -0.0141,  0.0042, -0.0148,  0.0117,  0.0236,  0.0058, -0.0213,\n",
       "         0.0125,  0.0061, -0.0031,  0.0103,  0.0042,  0.0015,  0.0107,  0.0077,\n",
       "        -0.0052,  0.0051, -0.0018,  0.0119,  0.0004, -0.0074,  0.0050, -0.0029,\n",
       "         0.0023,  0.0057,  0.0056, -0.0273,  0.0036, -0.0050, -0.0204, -0.0151,\n",
       "         0.0020,  0.0130, -0.0136,  0.0026,  0.0007,  0.0001, -0.0023, -0.0077,\n",
       "         0.0132,  0.0011, -0.0038,  0.0161, -0.0042,  0.0070,  0.0039, -0.0112,\n",
       "        -0.0108, -0.0029,  0.0067, -0.0141, -0.0064, -0.0043,  0.0080, -0.0175,\n",
       "         0.0083,  0.0017,  0.0002,  0.0097,  0.0079,  0.0033, -0.0154,  0.0113,\n",
       "         0.0008, -0.0165, -0.0035,  0.0041,  0.0066,  0.0018,  0.0023, -0.0026,\n",
       "        -0.0085, -0.0080, -0.0125, -0.0055, -0.0013,  0.0067,  0.0077, -0.0045,\n",
       "        -0.0018, -0.0172, -0.0086,  0.0025, -0.0011, -0.0035,  0.0055,  0.0089,\n",
       "        -0.0024, -0.0062,  0.0019,  0.0167, -0.0068, -0.0058, -0.0004, -0.0011,\n",
       "        -0.0120,  0.0097,  0.0144, -0.0038, -0.0024, -0.0012, -0.0001, -0.0061,\n",
       "         0.0001, -0.0072, -0.0027,  0.0045,  0.0045,  0.0030, -0.0067,  0.0017,\n",
       "         0.0002,  0.0023, -0.0015, -0.0002, -0.0045, -0.0012, -0.0025,  0.0118,\n",
       "         0.0031, -0.0057, -0.0173, -0.0231,  0.0112, -0.0031, -0.0104, -0.0117,\n",
       "        -0.0070,  0.0099, -0.0045, -0.0006, -0.0036,  0.0039, -0.0058,  0.0111,\n",
       "         0.0002, -0.0004,  0.0068,  0.0155,  0.0104, -0.0026,  0.0035, -0.0069,\n",
       "         0.0032,  0.0132, -0.0278,  0.0035,  0.0008, -0.0070, -0.0137, -0.0030,\n",
       "         0.0004, -0.0049, -0.0049,  0.0029,  0.0114,  0.0093, -0.0008, -0.0134,\n",
       "        -0.0048,  0.0199,  0.0061, -0.0112,  0.0004,  0.0082,  0.0048, -0.0098,\n",
       "        -0.0031, -0.0058,  0.0135,  0.0148,  0.0125,  0.0049,  0.0172, -0.0149,\n",
       "         0.0064, -0.0089, -0.0013,  0.0104, -0.0061, -0.0134, -0.0051, -0.0017,\n",
       "         0.0029, -0.0083, -0.0041, -0.0073,  0.0106,  0.0055, -0.0006, -0.0138,\n",
       "        -0.0198, -0.0021,  0.0111,  0.0002,  0.0023,  0.0036,  0.0002,  0.0051,\n",
       "         0.0050, -0.0015,  0.0052,  0.0185, -0.0066,  0.0093,  0.0051, -0.0070,\n",
       "        -0.0184, -0.0172, -0.0082,  0.0012, -0.0004, -0.0103, -0.0017, -0.0154,\n",
       "         0.0050,  0.0118,  0.0064,  0.0044,  0.0008, -0.0023, -0.0116,  0.0089,\n",
       "        -0.0151, -0.0043,  0.0077,  0.0055,  0.0049,  0.0123,  0.0061, -0.0008,\n",
       "        -0.0139, -0.0021, -0.0001,  0.0017, -0.0074,  0.0081,  0.0014,  0.0048,\n",
       "        -0.0045,  0.0010,  0.0025, -0.0014, -0.0079, -0.0030,  0.0004,  0.0064,\n",
       "        -0.0045, -0.0141, -0.0192, -0.0004,  0.0125,  0.0006, -0.0018,  0.0044,\n",
       "         0.0015, -0.0001,  0.0098, -0.0008, -0.0041,  0.0180, -0.0165,  0.0023,\n",
       "        -0.0037, -0.0241, -0.0083, -0.0132, -0.0057, -0.0008,  0.0161, -0.0074,\n",
       "         0.0103,  0.0063,  0.0020, -0.0063, -0.0050,  0.0116, -0.0079,  0.0010,\n",
       "        -0.0074,  0.0206, -0.0123,  0.0143,  0.0089, -0.0081, -0.0081, -0.0061,\n",
       "         0.0089, -0.0087,  0.0006,  0.0089, -0.0012,  0.0095,  0.0011, -0.0188,\n",
       "        -0.0019, -0.0093,  0.0052, -0.0036,  0.0026,  0.0079,  0.0217,  0.0054,\n",
       "        -0.0122, -0.0032,  0.0015, -0.0128,  0.0031, -0.0110, -0.0041, -0.0019,\n",
       "         0.0002, -0.0112, -0.0187, -0.0018, -0.0055, -0.0130, -0.0147, -0.0060,\n",
       "         0.0020, -0.0012, -0.0076,  0.0008, -0.0021, -0.0074, -0.0006,  0.0027,\n",
       "        -0.0091, -0.0135,  0.0020, -0.0129,  0.0002, -0.0154, -0.0112, -0.0054,\n",
       "        -0.0004,  0.0069,  0.0015, -0.0126, -0.0051, -0.0141,  0.0004, -0.0050,\n",
       "        -0.0013,  0.0027, -0.0017,  0.0007,  0.0051, -0.0181, -0.0004,  0.0033,\n",
       "        -0.0138, -0.0012, -0.0050,  0.0083,  0.0151,  0.0002, -0.0045, -0.0103,\n",
       "        -0.0066, -0.0188,  0.0055,  0.0069, -0.0036,  0.0056,  0.0077,  0.0197,\n",
       "         0.0004, -0.0125,  0.0036,  0.0021,  0.0051,  0.0072, -0.0007, -0.0036,\n",
       "        -0.0103, -0.0043,  0.0042, -0.0136, -0.0060,  0.0060,  0.0023, -0.0055,\n",
       "        -0.0132, -0.0036, -0.0136,  0.0002,  0.0106,  0.0086,  0.0021, -0.0061,\n",
       "        -0.0159,  0.0106,  0.0013,  0.0058, -0.0043, -0.0011,  0.0061,  0.0075,\n",
       "         0.0004, -0.0039,  0.0082,  0.0002,  0.0072, -0.0136,  0.0015,  0.0089,\n",
       "         0.0055, -0.0075,  0.0008,  0.0112,  0.0017, -0.0066,  0.0013, -0.0257,\n",
       "        -0.0142, -0.0050, -0.0067,  0.0087,  0.0119,  0.0018, -0.0030,  0.0026,\n",
       "        -0.0024,  0.0030, -0.0050,  0.0050,  0.0032, -0.0015, -0.0017,  0.0026,\n",
       "        -0.0052, -0.0070,  0.0098, -0.0004,  0.0118, -0.0013,  0.0069,  0.0085,\n",
       "         0.0068,  0.0008,  0.0007,  0.0167, -0.0087,  0.0015, -0.0169,  0.0033,\n",
       "        -0.0064, -0.0044, -0.0156,  0.0093, -0.0063,  0.0042,  0.0015,  0.0060,\n",
       "        -0.0088, -0.0079,  0.0013, -0.0005,  0.0100,  0.0055,  0.0050, -0.0108,\n",
       "        -0.0023, -0.0076,  0.0138,  0.0138,  0.0052, -0.0116, -0.0046,  0.0006,\n",
       "         0.0015, -0.0052, -0.0055, -0.0070,  0.0068,  0.0004,  0.0049, -0.0015,\n",
       "         0.0064, -0.0116,  0.0033, -0.0007, -0.0060, -0.0074,  0.0153, -0.0046,\n",
       "        -0.0113, -0.0137,  0.0021,  0.0166,  0.0070,  0.0018, -0.0054,  0.0015,\n",
       "         0.0049,  0.0105,  0.0030, -0.0091,  0.0002,  0.0038, -0.0100, -0.0044,\n",
       "         0.0279,  0.0054, -0.0001,  0.0045,  0.0002, -0.0002,  0.0184,  0.0131,\n",
       "         0.0108,  0.0097,  0.0026,  0.0043, -0.0056,  0.0038,  0.0089,  0.0149,\n",
       "        -0.0103, -0.0154, -0.0175, -0.0017,  0.0083,  0.0097, -0.0156,  0.0063,\n",
       "         0.0077, -0.0087, -0.0167, -0.0020,  0.0050,  0.0004, -0.0031,  0.0064,\n",
       "         0.0015,  0.0151, -0.0001, -0.0075,  0.0055, -0.0147, -0.0069,  0.0089,\n",
       "        -0.0101, -0.0012,  0.0088, -0.0118, -0.0006,  0.0030, -0.0056,  0.0015,\n",
       "         0.0041, -0.0114,  0.0000, -0.0042, -0.0064,  0.0145, -0.0064, -0.0001,\n",
       "         0.0018,  0.0055, -0.0082, -0.0068,  0.0026, -0.0006, -0.0168, -0.0180,\n",
       "         0.0079,  0.0187, -0.0030, -0.0068, -0.0038, -0.0194, -0.0098,  0.0076,\n",
       "        -0.0072,  0.0025,  0.0013,  0.0101, -0.0031, -0.0017,  0.0010, -0.0020,\n",
       "         0.0098,  0.0174,  0.0036, -0.0055,  0.0074,  0.0111, -0.0062, -0.0005,\n",
       "         0.0043, -0.0023,  0.0035, -0.0068,  0.0142, -0.0191,  0.0179, -0.0099,\n",
       "        -0.0055,  0.0013, -0.0200, -0.0114,  0.0272,  0.0001, -0.0080,  0.0024,\n",
       "        -0.0181,  0.0018,  0.0210, -0.0032,  0.0093, -0.0006,  0.0058,  0.0043,\n",
       "         0.0099, -0.0087, -0.0062, -0.0077,  0.0018, -0.0063,  0.0175, -0.0050,\n",
       "        -0.0192, -0.0100, -0.0083,  0.0038, -0.0083,  0.0110, -0.0130,  0.0139,\n",
       "        -0.0023, -0.0018, -0.0001, -0.0203, -0.0120,  0.0035, -0.0079,  0.0094,\n",
       "         0.0221, -0.0063, -0.0197, -0.0098,  0.0038,  0.0212, -0.0068,  0.0130,\n",
       "         0.0104,  0.0074, -0.0112, -0.0064, -0.0130,  0.0169,  0.0062, -0.0011,\n",
       "         0.0108,  0.0067,  0.0042,  0.0123,  0.0001, -0.0006,  0.0081,  0.0132,\n",
       "        -0.0106,  0.0061,  0.0054, -0.0128, -0.0015,  0.0055,  0.0020, -0.0005,\n",
       "         0.0008,  0.0150, -0.0023, -0.0007,  0.0000, -0.0206, -0.0051,  0.0123,\n",
       "        -0.0036,  0.0117,  0.0024, -0.0132,  0.0174,  0.0039, -0.0089, -0.0064,\n",
       "        -0.0038, -0.0135,  0.0120,  0.0046, -0.0083,  0.0070,  0.0015,  0.0004,\n",
       "        -0.0154,  0.0088,  0.0142,  0.0073,  0.0051,  0.0093,  0.0087,  0.0181,\n",
       "         0.0023, -0.0008,  0.0070,  0.0066,  0.0105, -0.0043,  0.0011,  0.0006,\n",
       "         0.0011,  0.0026, -0.0049, -0.0129,  0.0095, -0.0081,  0.0014, -0.0030,\n",
       "         0.0083,  0.0097, -0.0029,  0.0156,  0.0106, -0.0074,  0.0026, -0.0112,\n",
       "        -0.0144,  0.0058,  0.0075, -0.0111,  0.0046,  0.0081,  0.0192,  0.0074,\n",
       "         0.0061,  0.0002, -0.0198, -0.0054,  0.0024, -0.0203, -0.0093, -0.0011,\n",
       "        -0.0067, -0.0055, -0.0197,  0.0001,  0.0043,  0.0165,  0.0063,  0.0103,\n",
       "         0.0141,  0.0093,  0.0079, -0.0159, -0.0015,  0.0106,  0.0098,  0.0054,\n",
       "         0.0167,  0.0063, -0.0048, -0.0020,  0.0131, -0.0205, -0.0023, -0.0024],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b45a0-68af-4c88-9381-3dcd2a9725a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
