{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d88919c8-c47c-4740-bd3a-af1a9555529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e27198a-4301-473d-9ee2-1e12dbc6ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "def get_data(training_data, test_data):\n",
    "    return (\n",
    "        DataLoader(training_data), #, shuffle=True),\n",
    "        DataLoader(test_data),\n",
    "    )\n",
    "\n",
    "train_dl, valid_dl = get_data(training_data, test_data)\n",
    "train_dataloader = DataLoader(training_data, batch_size=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2fb4e58-19d8-4937-b1de-9622c44deab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter(train_dl))\n",
    "x1, y1 =x1.flatten(), y1.squeeze()\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dce317fd-1713-49e9-92d1-32271c058402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias', 'linear_relu_stack.2.weight', 'linear_relu_stack.2.bias'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c021a9f-68c3-4559-8673-9b807b047c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13902374, -0.14935616, -0.2146723 ,  0.21417932,  0.11655144,\n",
       "        -0.15464526, -0.16536987,  0.06310065,  0.15438937,  0.20637192,\n",
       "         0.02966635,  0.13796093,  0.09203579,  0.08334605, -0.11801162,\n",
       "        -0.17072244,  0.22889192,  0.04892804],\n",
       "       [-0.21189949, -0.08715634,  0.02339686,  0.03954171, -0.17625031,\n",
       "         0.20591186, -0.2173645 ,  0.11548699, -0.10416864, -0.16685198,\n",
       "         0.10424839, -0.05649   , -0.09371138, -0.02472298, -0.08418827,\n",
       "         0.21943991, -0.22199707, -0.08080144],\n",
       "       [-0.02861121, -0.06552699, -0.14460835,  0.0312344 ,  0.00475523,\n",
       "         0.14975406,  0.07740985,  0.16138051, -0.16647691,  0.19663243,\n",
       "        -0.09861559, -0.00300536,  0.0867797 , -0.04870926, -0.06252556,\n",
       "         0.10711958,  0.16982137, -0.15328938],\n",
       "       [ 0.13577889,  0.15609859,  0.09796895,  0.07576694,  0.12653767,\n",
       "         0.22326736,  0.16011076,  0.00751489, -0.1609039 , -0.0057642 ,\n",
       "        -0.13544682,  0.20785193, -0.06999013,  0.01699112,  0.09970738,\n",
       "         0.08198054, -0.19329548, -0.15214717],\n",
       "       [ 0.04028247, -0.17312486,  0.09176044,  0.06247665, -0.09176102,\n",
       "        -0.18643656,  0.03612225, -0.13159677,  0.22620137,  0.15221961,\n",
       "         0.15084632,  0.11025812,  0.0919001 , -0.17692761,  0.22441934,\n",
       "        -0.04816645,  0.13656075,  0.21641903],\n",
       "       [ 0.04532664, -0.17079858, -0.01118715, -0.13725299, -0.19192301,\n",
       "         0.16840635, -0.08901252,  0.18935736, -0.13702554,  0.09765171,\n",
       "        -0.01586711, -0.02623987,  0.13613869, -0.21341895,  0.19698323,\n",
       "        -0.04832752,  0.10644837, -0.13988309],\n",
       "       [-0.12294292, -0.00546537, -0.05641153, -0.09580858,  0.12754436,\n",
       "        -0.02573295, -0.09673542, -0.0269293 ,  0.21113311, -0.1932456 ,\n",
       "        -0.03922641, -0.03686962,  0.11476056,  0.19430141, -0.14800194,\n",
       "        -0.04366612, -0.21936505, -0.17431091],\n",
       "       [ 0.08837013, -0.02566712,  0.17451127, -0.02242279, -0.10761787,\n",
       "        -0.07775189,  0.02496217,  0.15099065,  0.01930688,  0.05375896,\n",
       "        -0.08277169,  0.08954869, -0.15908   ,  0.14642517, -0.01015197,\n",
       "         0.19800247, -0.11998205,  0.23106299],\n",
       "       [ 0.14974822, -0.19036274, -0.06995304,  0.20584749,  0.08005227,\n",
       "         0.19603716, -0.04943454, -0.10065904,  0.1563213 , -0.0707235 ,\n",
       "         0.21375553,  0.13763849,  0.23465045,  0.17776157, -0.0699899 ,\n",
       "         0.19879355, -0.1828627 ,  0.23063721],\n",
       "       [-0.00061756, -0.1074204 ,  0.06882687,  0.12650315,  0.01106091,\n",
       "         0.16752504, -0.01675183, -0.23162517,  0.15200402,  0.0021285 ,\n",
       "         0.13227372, -0.11641704,  0.1539803 ,  0.08852307, -0.13694966,\n",
       "        -0.21911015, -0.00785126,  0.04393448]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['linear_relu_stack.2.weight'].numpy().astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09be1b13-b164-4854-bf50-d8bf69b99e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2428a70a-96ed-46ae-8d83-54c93838bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    X[X<0] = 0\n",
    "    return X\n",
    "\n",
    "def d_relu(x):\n",
    "    d = x.clone()\n",
    "    d[x < 0] = 0\n",
    "    d[x > 0] = 1\n",
    "    return d\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum()\n",
    "\n",
    "def delta_l(y_pred, y, l):\n",
    "    if l == len(weights)-1: # This means it's the last layer... must be a vector\n",
    "        y_ = torch.zeros(len(y_pred))\n",
    "        y_[y] = 1\n",
    "        return (y_pred - y_)[None,:]\n",
    "    else:\n",
    "        return (weights[l+1].T@delta_l(y_pred,y,l+1).T)@d_relu(l)\n",
    "        # return weights[l].T@(delta_l(y_pred,y,l+1)*d_relu(l+1)).T\n",
    "    \n",
    "activations = []\n",
    "activations.append(relu)\n",
    "activations.append(linear)\n",
    "    \n",
    "def forward(X):\n",
    "    a = [X]\n",
    "    \n",
    "    for w,b,func in zip(weights, bias, activations):\n",
    "        a.append(func(a[-1]@w.T+b))\n",
    "    return a\n",
    "\n",
    "def cross_entropy(y_pred,y):\n",
    "    return -y_pred[y].log()\n",
    "\n",
    "def backward(a,y, lr=1e-3): # hard-wired backprop...\n",
    "    loss_L = delta_l(a[-1],y,1)\n",
    "    loss_l = ((weights[1].T@loss_L.T).T*d_relu(a[1]))\n",
    "    \n",
    "    weights[1] -= lr * (a[1][None,:].T@loss_L).T\n",
    "    bias[1] -= lr * loss_L.squeeze()\n",
    "    weights[0] -= lr * (a[0][None,:].T@loss_l).T\n",
    "    bias[0] -= lr * loss_l.squeeze()\n",
    "        \n",
    "    loss = cross_entropy(softmax(a[-1]),y)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067b9968-3440-4d10-abb7-3d12f4441a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "w,b,func = _,_,_\n",
    "y_pred = None\n",
    "X=None\n",
    "d=Nonel=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eef36076-7bc3-4c78-a37a-92be4bdaf1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn_image as isns\n",
    "isns.imgplot(x1.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "086acb0f-4aed-41a0-b47b-490d6e275a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ = iter(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4ea6638-cdfc-4f39-b252-44a5b5a41790",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b12e5f0-0212-4601-91fc-d925624e1571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter_)\n",
    "x1, y1 =x1.flatten(), y1.squeeze()\n",
    "x1.shape, y1.shape\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5417e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "936b5616-cf5c-43cd-846c-0dd18e3520b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.9009)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = forward(x1)\n",
    "cross_entropy(softmax(a[-1]),y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6f794dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0003)\n",
      "tensor(0.0192)\n",
      "tensor(-0.0294)\n",
      "tensor(-0.0263)\n",
      "tensor(-0.0138)\n",
      "tensor(0.0096)\n",
      "tensor(-0.0007)\n",
      "tensor(0.0283)\n",
      "tensor(-0.0032)\n",
      "tensor(0.0095)\n",
      "tensor(-0.0108)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0341)\n",
      "tensor(-0.0237)\n",
      "tensor(-0.0147)\n",
      "tensor(0.0013)\n",
      "tensor(0.0141)\n",
      "tensor(0.0214)\n",
      "tensor(-0.0242)\n",
      "tensor(-0.0156)\n",
      "tensor(0.0130)\n",
      "tensor(0.0297)\n",
      "tensor(-0.0074)\n",
      "tensor(0.0267)\n",
      "tensor(-0.0058)\n",
      "tensor(0.0038)\n",
      "tensor(0.0323)\n",
      "tensor(-0.0331)\n",
      "tensor(-0.0225)\n",
      "tensor(-0.0090)\n",
      "tensor(-0.0139)\n",
      "tensor(0.0309)\n",
      "tensor(-0.0231)\n",
      "tensor(-0.0164)\n",
      "tensor(-0.0250)\n",
      "tensor(-0.0334)\n",
      "tensor(-0.0208)\n",
      "tensor(0.0307)\n",
      "tensor(0.0159)\n",
      "tensor(0.0173)\n",
      "tensor(0.0019)\n",
      "tensor(-0.0183)\n",
      "tensor(0.0060)\n",
      "tensor(-0.0333)\n",
      "tensor(-0.0258)\n",
      "tensor(-0.0184)\n",
      "tensor(0.0225)\n",
      "tensor(0.0209)\n",
      "tensor(-0.0158)\n",
      "tensor(-0.0013)\n",
      "tensor(0.0228)\n",
      "tensor(0.0355)\n",
      "tensor(0.0142)\n",
      "tensor(0.0048)\n",
      "tensor(0.0239)\n",
      "tensor(-0.0210)\n",
      "tensor(0.0067)\n",
      "tensor(-0.0277)\n",
      "tensor(-0.0248)\n",
      "tensor(-0.0184)\n",
      "tensor(0.0162)\n",
      "tensor(0.0144)\n",
      "tensor(-0.0212)\n",
      "tensor(0.0108)\n",
      "tensor(0.0196)\n",
      "tensor(-0.0045)\n",
      "tensor(0.0014)\n",
      "tensor(0.0083)\n",
      "tensor(0.0222)\n",
      "tensor(0.0343)\n",
      "tensor(-0.0275)\n",
      "tensor(-0.0131)\n",
      "tensor(0.0140)\n",
      "tensor(0.0296)\n",
      "tensor(0.0311)\n",
      "tensor(0.0315)\n",
      "tensor(0.0071)\n",
      "tensor(-0.0311)\n",
      "tensor(0.0033)\n",
      "tensor(-0.0223)\n",
      "tensor(-0.0333)\n",
      "tensor(0.0317)\n",
      "tensor(0.0272)\n",
      "tensor(-0.0356)\n",
      "tensor(0.0067)\n",
      "tensor(-0.0060)\n",
      "tensor(-0.0059)\n",
      "tensor(-0.0163)\n",
      "tensor(0.0137)\n",
      "tensor(-0.0212)\n",
      "tensor(0.0131)\n",
      "tensor(0.0181)\n",
      "tensor(0.0256)\n",
      "tensor(0.0134)\n",
      "tensor(-0.0353)\n",
      "tensor(-0.0232)\n",
      "tensor(0.0178)\n",
      "tensor(0.0075)\n",
      "tensor(-0.0279)\n",
      "tensor(-0.0206)\n",
      "tensor(0.0336)\n",
      "tensor(0.0241)\n",
      "tensor(-0.0156)\n",
      "tensor(-0.0090)\n",
      "tensor(-0.0340)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0269)\n",
      "tensor(-0.0275)\n",
      "tensor(-0.0020)\n",
      "tensor(0.0054)\n",
      "tensor(-0.0146)\n",
      "tensor(0.0212)\n",
      "tensor(-0.0217)\n",
      "tensor(0.0324)\n",
      "tensor(0.0245)\n",
      "tensor(-0.0301)\n",
      "tensor(-0.0089)\n",
      "tensor(0.0016)\n",
      "tensor(0.0052)\n",
      "tensor(0.0085)\n",
      "tensor(0.0140)\n",
      "tensor(0.0021)\n",
      "tensor(-0.0174)\n",
      "tensor(0.0169)\n",
      "tensor(-0.0343)\n",
      "tensor(-0.0212)\n",
      "tensor(-0.0089)\n",
      "tensor(-0.0174)\n",
      "tensor(-0.0125)\n",
      "tensor(-0.0293)\n",
      "tensor(-0.0076)\n",
      "tensor(0.0076)\n",
      "tensor(-0.0233)\n",
      "tensor(-0.0018)\n",
      "tensor(0.0256)\n",
      "tensor(-0.0037)\n",
      "tensor(0.0010)\n",
      "tensor(-0.0031)\n",
      "tensor(0.0072)\n",
      "tensor(0.0227)\n",
      "tensor(0.0338)\n",
      "tensor(0.0227)\n",
      "tensor(0.0339)\n",
      "tensor(-0.0026)\n",
      "tensor(-0.0321)\n",
      "tensor(-0.0169)\n",
      "tensor(0.0243)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0178)\n",
      "tensor(-0.0274)\n",
      "tensor(-0.0334)\n",
      "tensor(-0.0301)\n",
      "tensor(-0.0072)\n",
      "tensor(0.0196)\n",
      "tensor(0.0193)\n",
      "tensor(-0.0344)\n",
      "tensor(0.0223)\n",
      "tensor(-0.0279)\n",
      "tensor(-0.0076)\n",
      "tensor(-0.0145)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0320)\n",
      "tensor(-0.0308)\n",
      "tensor(-0.0056)\n",
      "tensor(0.0005)\n",
      "tensor(-0.0162)\n",
      "tensor(0.0135)\n",
      "tensor(-0.0321)\n",
      "tensor(-0.0024)\n",
      "tensor(0.0314)\n",
      "tensor(-0.0146)\n",
      "tensor(0.0323)\n",
      "tensor(0.0129)\n",
      "tensor(-0.0322)\n",
      "tensor(0.0226)\n",
      "tensor(-0.0041)\n",
      "tensor(-0.0159)\n",
      "tensor(0.0286)\n",
      "tensor(-0.0289)\n",
      "tensor(0.0038)\n",
      "tensor(-0.0075)\n",
      "tensor(0.0255)\n",
      "tensor(0.0100)\n",
      "tensor(0.0172)\n",
      "tensor(0.0126)\n",
      "tensor(-0.0086)\n",
      "tensor(-0.0075)\n",
      "tensor(-0.0294)\n",
      "tensor(0.0194)\n",
      "tensor(0.0284)\n",
      "tensor(0.0244)\n",
      "tensor(-0.0252)\n",
      "tensor(0.0016)\n",
      "tensor(-0.0252)\n",
      "tensor(-0.0197)\n",
      "tensor(-0.0208)\n",
      "tensor(0.0122)\n",
      "tensor(-0.0213)\n",
      "tensor(-0.0008)\n",
      "tensor(0.0015)\n",
      "tensor(0.0230)\n",
      "tensor(-0.0270)\n",
      "tensor(-0.0245)\n",
      "tensor(-0.0207)\n",
      "tensor(0.0250)\n",
      "tensor(-0.0128)\n",
      "tensor(0.0301)\n",
      "tensor(0.0129)\n",
      "tensor(0.0045)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0071)\n",
      "tensor(0.0045)\n",
      "tensor(-0.0082)\n",
      "tensor(-0.0003)\n",
      "tensor(0.0046)\n",
      "tensor(-0.0279)\n",
      "tensor(-0.0187)\n",
      "tensor(0.0288)\n",
      "tensor(-0.0290)\n",
      "tensor(-0.0026)\n",
      "tensor(0.0353)\n",
      "tensor(0.0129)\n",
      "tensor(0.0010)\n",
      "tensor(-0.0310)\n",
      "tensor(0.0177)\n",
      "tensor(-0.0254)\n",
      "tensor(-0.0101)\n",
      "tensor(-0.0120)\n",
      "tensor(-0.0053)\n",
      "tensor(0.0004)\n",
      "tensor(0.0295)\n",
      "tensor(0.0045)\n",
      "tensor(0.0320)\n",
      "tensor(0.0218)\n",
      "tensor(-0.0226)\n",
      "tensor(0.0160)\n",
      "tensor(-0.0252)\n",
      "tensor(-0.0151)\n",
      "tensor(0.0105)\n",
      "tensor(0.0118)\n",
      "tensor(0.0268)\n",
      "tensor(-0.0115)\n",
      "tensor(5.7172e-05)\n",
      "tensor(0.0184)\n",
      "tensor(-0.0345)\n",
      "tensor(0.0258)\n",
      "tensor(-0.0295)\n",
      "tensor(0.0005)\n",
      "tensor(-0.0061)\n",
      "tensor(-0.0188)\n",
      "tensor(0.0047)\n",
      "tensor(0.0295)\n",
      "tensor(-0.0104)\n",
      "tensor(-0.0212)\n",
      "tensor(-0.0132)\n",
      "tensor(-0.0354)\n",
      "tensor(0.0161)\n",
      "tensor(-0.0172)\n",
      "tensor(-0.0238)\n",
      "tensor(-0.0206)\n",
      "tensor(0.0205)\n",
      "tensor(0.0189)\n",
      "tensor(0.0274)\n",
      "tensor(0.0130)\n",
      "tensor(-0.0119)\n",
      "tensor(-0.0100)\n",
      "tensor(0.0106)\n",
      "tensor(0.0294)\n",
      "tensor(0.0097)\n",
      "tensor(-0.0169)\n",
      "tensor(-0.0168)\n",
      "tensor(-0.0338)\n",
      "tensor(0.0077)\n",
      "tensor(-0.0200)\n",
      "tensor(-0.0318)\n",
      "tensor(0.0313)\n",
      "tensor(-0.0232)\n",
      "tensor(-0.0041)\n",
      "tensor(0.0102)\n",
      "tensor(0.0011)\n",
      "tensor(-0.0240)\n",
      "tensor(-0.0289)\n",
      "tensor(0.0285)\n",
      "tensor(0.0058)\n",
      "tensor(0.0296)\n",
      "tensor(-0.0120)\n",
      "tensor(0.0105)\n",
      "tensor(-0.0082)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0218)\n",
      "tensor(0.0121)\n",
      "tensor(0.0113)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0080)\n",
      "tensor(-0.0220)\n",
      "tensor(0.0247)\n",
      "tensor(-0.0266)\n",
      "tensor(0.0146)\n",
      "tensor(-0.0120)\n",
      "tensor(-0.0172)\n",
      "tensor(0.0064)\n",
      "tensor(-0.0186)\n",
      "tensor(0.0082)\n",
      "tensor(0.0070)\n",
      "tensor(-0.0265)\n",
      "tensor(0.0059)\n",
      "tensor(0.0152)\n",
      "tensor(0.0141)\n",
      "tensor(-0.0045)\n",
      "tensor(-0.0293)\n",
      "tensor(-0.0055)\n",
      "tensor(0.0124)\n",
      "tensor(-0.0130)\n",
      "tensor(0.0136)\n",
      "tensor(0.0238)\n",
      "tensor(-0.0186)\n",
      "tensor(0.0004)\n",
      "tensor(0.0148)\n",
      "tensor(0.0028)\n",
      "tensor(0.0030)\n",
      "tensor(0.0045)\n",
      "tensor(-0.0281)\n",
      "tensor(0.0028)\n",
      "tensor(0.0247)\n",
      "tensor(0.0322)\n",
      "tensor(0.0210)\n",
      "tensor(0.0048)\n",
      "tensor(0.0167)\n",
      "tensor(-0.0174)\n",
      "tensor(-0.0296)\n",
      "tensor(-0.0307)\n",
      "tensor(0.0356)\n",
      "tensor(0.0227)\n",
      "tensor(-0.0247)\n",
      "tensor(0.0140)\n",
      "tensor(0.0270)\n",
      "tensor(0.0357)\n",
      "tensor(0.0312)\n",
      "tensor(0.0277)\n",
      "tensor(-0.0082)\n",
      "tensor(-0.0125)\n",
      "tensor(0.0293)\n",
      "tensor(0.0200)\n",
      "tensor(-0.0215)\n",
      "tensor(0.0321)\n",
      "tensor(0.0173)\n",
      "tensor(0.0195)\n",
      "tensor(-0.0224)\n",
      "tensor(0.0102)\n",
      "tensor(-0.0125)\n",
      "tensor(0.0279)\n",
      "tensor(-0.0064)\n",
      "tensor(0.0139)\n",
      "tensor(0.0063)\n",
      "tensor(0.0152)\n",
      "tensor(-0.0121)\n",
      "tensor(0.0174)\n",
      "tensor(-0.0249)\n",
      "tensor(0.0081)\n",
      "tensor(-0.0242)\n",
      "tensor(-0.0352)\n",
      "tensor(-0.0287)\n",
      "tensor(0.0282)\n",
      "tensor(0.0193)\n",
      "tensor(0.0335)\n",
      "tensor(0.0286)\n",
      "tensor(-0.0319)\n",
      "tensor(-0.0244)\n",
      "tensor(-0.0058)\n",
      "tensor(-0.0232)\n",
      "tensor(0.0248)\n",
      "tensor(-0.0270)\n",
      "tensor(-0.0174)\n",
      "tensor(-0.0345)\n",
      "tensor(-0.0203)\n",
      "tensor(0.0294)\n",
      "tensor(0.0292)\n",
      "tensor(0.0256)\n",
      "tensor(0.0276)\n",
      "tensor(0.0318)\n",
      "tensor(-0.0091)\n",
      "tensor(0.0157)\n",
      "tensor(0.0318)\n",
      "tensor(0.0118)\n",
      "tensor(0.0357)\n",
      "tensor(0.0185)\n",
      "tensor(0.0222)\n",
      "tensor(-0.0125)\n",
      "tensor(0.0171)\n",
      "tensor(0.0041)\n",
      "tensor(-0.0085)\n",
      "tensor(-0.0201)\n",
      "tensor(-0.0200)\n",
      "tensor(-0.0275)\n",
      "tensor(0.0240)\n",
      "tensor(0.0254)\n",
      "tensor(-0.0041)\n",
      "tensor(-0.0207)\n",
      "tensor(0.0276)\n",
      "tensor(0.0228)\n",
      "tensor(0.0027)\n",
      "tensor(-0.0169)\n",
      "tensor(0.0328)\n",
      "tensor(0.0146)\n",
      "tensor(-0.0271)\n",
      "tensor(0.0342)\n",
      "tensor(0.0271)\n",
      "tensor(-0.0130)\n",
      "tensor(0.0201)\n",
      "tensor(-0.0203)\n",
      "tensor(-0.0056)\n",
      "tensor(0.0303)\n",
      "tensor(0.0015)\n",
      "tensor(-0.0253)\n",
      "tensor(-0.0119)\n",
      "tensor(-0.0097)\n",
      "tensor(-0.0069)\n",
      "tensor(0.0034)\n",
      "tensor(0.0330)\n",
      "tensor(0.0019)\n",
      "tensor(-0.0221)\n",
      "tensor(0.0018)\n",
      "tensor(0.0171)\n",
      "tensor(0.0177)\n",
      "tensor(-0.0326)\n",
      "tensor(-0.0064)\n",
      "tensor(-0.0265)\n",
      "tensor(-0.0152)\n",
      "tensor(0.0129)\n",
      "tensor(-0.0254)\n",
      "tensor(0.0133)\n",
      "tensor(0.0303)\n",
      "tensor(0.0023)\n",
      "tensor(-0.0238)\n",
      "tensor(-0.0128)\n",
      "tensor(0.0078)\n",
      "tensor(-0.0272)\n",
      "tensor(0.0177)\n",
      "tensor(-0.0324)\n",
      "tensor(-0.0343)\n",
      "tensor(-0.0347)\n",
      "tensor(-0.0072)\n",
      "tensor(0.0240)\n",
      "tensor(-0.0338)\n",
      "tensor(0.0297)\n",
      "tensor(-0.0143)\n",
      "tensor(0.0105)\n",
      "tensor(0.0016)\n",
      "tensor(-0.0322)\n",
      "tensor(0.0296)\n",
      "tensor(0.0192)\n",
      "tensor(0.0355)\n",
      "tensor(0.0180)\n",
      "tensor(-0.0236)\n",
      "tensor(0.0298)\n",
      "tensor(0.0019)\n",
      "tensor(0.0169)\n",
      "tensor(-0.0286)\n",
      "tensor(-0.0103)\n",
      "tensor(-0.0351)\n",
      "tensor(-0.0139)\n",
      "tensor(0.0077)\n",
      "tensor(-0.0280)\n",
      "tensor(0.0114)\n",
      "tensor(0.0192)\n",
      "tensor(0.0050)\n",
      "tensor(-0.0239)\n",
      "tensor(-0.0277)\n",
      "tensor(-0.0110)\n",
      "tensor(0.0157)\n",
      "tensor(0.0352)\n",
      "tensor(0.0205)\n",
      "tensor(-0.0040)\n",
      "tensor(0.0125)\n",
      "tensor(-0.0350)\n",
      "tensor(-0.0305)\n",
      "tensor(0.0167)\n",
      "tensor(-0.0202)\n",
      "tensor(0.0172)\n",
      "tensor(-0.0252)\n",
      "tensor(-0.0177)\n",
      "tensor(-0.0294)\n",
      "tensor(0.0186)\n",
      "tensor(-0.0036)\n",
      "tensor(0.0275)\n",
      "tensor(0.0221)\n",
      "tensor(0.0198)\n",
      "tensor(0.0011)\n",
      "tensor(-0.0110)\n",
      "tensor(-0.0078)\n",
      "tensor(0.0047)\n",
      "tensor(0.0177)\n",
      "tensor(-0.0250)\n",
      "tensor(0.0300)\n",
      "tensor(-0.0039)\n",
      "tensor(-0.0299)\n",
      "tensor(-0.0193)\n",
      "tensor(0.0316)\n",
      "tensor(0.0327)\n",
      "tensor(-0.0331)\n",
      "tensor(0.0252)\n",
      "tensor(0.0179)\n",
      "tensor(0.0211)\n",
      "tensor(0.0302)\n",
      "tensor(-0.0192)\n",
      "tensor(0.0113)\n",
      "tensor(0.0146)\n",
      "tensor(-0.0106)\n",
      "tensor(0.0120)\n",
      "tensor(-0.0103)\n",
      "tensor(0.0221)\n",
      "tensor(-0.0099)\n",
      "tensor(-0.0133)\n",
      "tensor(0.0090)\n",
      "tensor(0.0127)\n",
      "tensor(-0.0174)\n",
      "tensor(0.0032)\n",
      "tensor(0.0207)\n",
      "tensor(-0.0036)\n",
      "tensor(0.0109)\n",
      "tensor(-0.0086)\n",
      "tensor(0.0125)\n",
      "tensor(-0.0259)\n",
      "tensor(-0.0210)\n",
      "tensor(-0.0181)\n",
      "tensor(0.0328)\n",
      "tensor(-0.0096)\n",
      "tensor(-9.7513e-05)\n",
      "tensor(-0.0173)\n",
      "tensor(0.0357)\n",
      "tensor(0.0349)\n",
      "tensor(-0.0269)\n",
      "tensor(-0.0290)\n",
      "tensor(-0.0271)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0091)\n",
      "tensor(-0.0234)\n",
      "tensor(-0.0128)\n",
      "tensor(0.0067)\n",
      "tensor(-0.0187)\n",
      "tensor(0.0079)\n",
      "tensor(-0.0082)\n",
      "tensor(-0.0173)\n",
      "tensor(0.0049)\n",
      "tensor(0.0294)\n",
      "tensor(-0.0241)\n",
      "tensor(0.0017)\n",
      "tensor(-0.0132)\n",
      "tensor(0.0350)\n",
      "tensor(-0.0339)\n",
      "tensor(-0.0342)\n",
      "tensor(0.0352)\n",
      "tensor(-0.0226)\n",
      "tensor(0.0068)\n",
      "tensor(-0.0031)\n",
      "tensor(-0.0075)\n",
      "tensor(-0.0080)\n",
      "tensor(0.0227)\n",
      "tensor(0.0017)\n",
      "tensor(-0.0348)\n",
      "tensor(-0.0211)\n",
      "tensor(-0.0122)\n",
      "tensor(0.0180)\n",
      "tensor(-0.0231)\n",
      "tensor(0.0337)\n",
      "tensor(-0.0080)\n",
      "tensor(-0.0064)\n",
      "tensor(0.0280)\n",
      "tensor(0.0180)\n",
      "tensor(0.0303)\n",
      "tensor(0.0207)\n",
      "tensor(-0.0108)\n",
      "tensor(-0.0237)\n",
      "tensor(-0.0027)\n",
      "tensor(0.0296)\n",
      "tensor(-0.0120)\n",
      "tensor(-0.0331)\n",
      "tensor(0.0146)\n",
      "tensor(0.0348)\n",
      "tensor(-0.0102)\n",
      "tensor(-0.0296)\n",
      "tensor(-0.0324)\n",
      "tensor(0.0089)\n",
      "tensor(-0.0027)\n",
      "tensor(-0.0180)\n",
      "tensor(0.0072)\n",
      "tensor(0.0136)\n",
      "tensor(0.0284)\n",
      "tensor(0.0277)\n",
      "tensor(-0.0053)\n",
      "tensor(-0.0315)\n",
      "tensor(-0.0323)\n",
      "tensor(0.0333)\n",
      "tensor(0.0158)\n",
      "tensor(0.0156)\n",
      "tensor(-0.0309)\n",
      "tensor(0.0331)\n",
      "tensor(0.0338)\n",
      "tensor(0.0322)\n",
      "tensor(-0.0301)\n",
      "tensor(-0.0135)\n",
      "tensor(-0.0246)\n",
      "tensor(0.0338)\n",
      "tensor(-0.0153)\n",
      "tensor(-0.0163)\n",
      "tensor(0.0187)\n",
      "tensor(-0.0165)\n",
      "tensor(-0.0176)\n",
      "tensor(-0.0031)\n",
      "tensor(-0.0034)\n",
      "tensor(-0.0278)\n",
      "tensor(0.0298)\n",
      "tensor(-0.0158)\n",
      "tensor(0.0127)\n",
      "tensor(0.0311)\n",
      "tensor(0.0180)\n",
      "tensor(0.0051)\n",
      "tensor(0.0304)\n",
      "tensor(0.0048)\n",
      "tensor(-0.0165)\n",
      "tensor(0.0338)\n",
      "tensor(0.0085)\n",
      "tensor(-0.0348)\n",
      "tensor(-0.0102)\n",
      "tensor(-0.0243)\n",
      "tensor(0.0313)\n",
      "tensor(-0.0059)\n",
      "tensor(-0.0326)\n",
      "tensor(-0.0022)\n",
      "tensor(0.0224)\n",
      "tensor(0.0093)\n",
      "tensor(0.0113)\n",
      "tensor(0.0033)\n",
      "tensor(0.0133)\n",
      "tensor(-0.0087)\n",
      "tensor(-0.0142)\n",
      "tensor(-0.0334)\n",
      "tensor(-0.0269)\n",
      "tensor(0.0155)\n",
      "tensor(-0.0211)\n",
      "tensor(0.0051)\n",
      "tensor(0.0114)\n",
      "tensor(0.0025)\n",
      "tensor(-0.0232)\n",
      "tensor(0.0342)\n",
      "tensor(-0.0208)\n",
      "tensor(0.0294)\n",
      "tensor(-0.0284)\n",
      "tensor(-0.0086)\n",
      "tensor(0.0194)\n",
      "tensor(-0.0146)\n",
      "tensor(0.0300)\n",
      "tensor(-0.0246)\n",
      "tensor(-0.0300)\n",
      "tensor(-0.0161)\n",
      "tensor(0.0058)\n",
      "tensor(0.0329)\n",
      "tensor(-0.0171)\n",
      "tensor(0.0128)\n",
      "tensor(-0.0090)\n",
      "tensor(-0.0077)\n",
      "tensor(0.0263)\n",
      "tensor(-0.0277)\n",
      "tensor(0.0038)\n",
      "tensor(0.0336)\n",
      "tensor(-0.0049)\n",
      "tensor(0.0277)\n",
      "tensor(-0.0110)\n",
      "tensor(0.0287)\n",
      "tensor(-0.0345)\n",
      "tensor(-0.0051)\n",
      "tensor(-0.0063)\n",
      "tensor(0.0116)\n",
      "tensor(0.0140)\n",
      "tensor(0.0274)\n",
      "tensor(-0.0053)\n",
      "tensor(-0.0014)\n",
      "tensor(0.0245)\n",
      "tensor(-0.0097)\n",
      "tensor(0.0313)\n",
      "tensor(-0.0238)\n",
      "tensor(-0.0039)\n",
      "tensor(-0.0019)\n",
      "tensor(0.0159)\n",
      "tensor(0.0244)\n",
      "tensor(-0.0057)\n",
      "tensor(-0.0296)\n",
      "tensor(0.0177)\n",
      "tensor(0.0107)\n",
      "tensor(0.0143)\n",
      "tensor(-0.0220)\n",
      "tensor(0.0230)\n",
      "tensor(0.0338)\n",
      "tensor(0.0031)\n",
      "tensor(-0.0334)\n",
      "tensor(0.0251)\n",
      "tensor(-0.0265)\n",
      "tensor(0.0082)\n",
      "tensor(0.0052)\n",
      "tensor(-0.0167)\n",
      "tensor(0.0124)\n",
      "tensor(-0.0319)\n",
      "tensor(0.0081)\n",
      "tensor(-0.0226)\n",
      "tensor(-0.0039)\n",
      "tensor(0.0046)\n",
      "tensor(0.0304)\n",
      "tensor(-0.0170)\n",
      "tensor(0.0229)\n",
      "tensor(-0.0045)\n",
      "tensor(-0.0170)\n",
      "tensor(-0.0311)\n",
      "tensor(-0.0328)\n",
      "tensor(0.0349)\n",
      "tensor(-0.0089)\n",
      "tensor(0.0018)\n",
      "tensor(0.0097)\n",
      "tensor(0.0243)\n",
      "tensor(0.0305)\n",
      "tensor(0.0290)\n",
      "tensor(-0.0265)\n",
      "tensor(-0.0057)\n",
      "tensor(-0.0211)\n",
      "tensor(-0.0204)\n",
      "tensor(0.0085)\n",
      "tensor(0.0335)\n",
      "tensor(-0.0286)\n",
      "tensor(0.0216)\n",
      "tensor(-0.0185)\n",
      "tensor(-0.0070)\n",
      "tensor(0.0283)\n",
      "tensor(-0.0081)\n",
      "tensor(0.0033)\n",
      "tensor(-0.0250)\n",
      "tensor(0.0304)\n",
      "tensor(-0.0046)\n",
      "tensor(-0.0261)\n",
      "tensor(0.0105)\n",
      "tensor(-0.0254)\n",
      "tensor(-0.0283)\n",
      "tensor(0.0022)\n",
      "tensor(0.0283)\n",
      "tensor(-0.0101)\n",
      "tensor(0.0168)\n",
      "tensor(0.0307)\n",
      "tensor(0.0237)\n",
      "tensor(-0.0187)\n",
      "tensor(-0.0039)\n",
      "tensor(-0.0112)\n",
      "tensor(-0.0287)\n",
      "tensor(1.4756e-05)\n",
      "tensor(0.0269)\n",
      "tensor(0.0301)\n",
      "tensor(0.0033)\n",
      "tensor(0.0081)\n",
      "tensor(-0.0155)\n",
      "tensor(0.0270)\n",
      "tensor(-0.0149)\n",
      "tensor(-0.0248)\n",
      "tensor(0.0055)\n",
      "tensor(0.0214)\n",
      "tensor(-0.0322)\n",
      "tensor(0.0323)\n",
      "tensor(0.0128)\n",
      "tensor(-0.0250)\n",
      "tensor(-0.0077)\n",
      "tensor(0.0310)\n",
      "tensor(-0.0274)\n",
      "tensor(-0.0104)\n",
      "tensor(0.0117)\n",
      "tensor(-0.0313)\n",
      "tensor(0.0196)\n",
      "tensor(0.0186)\n",
      "tensor(0.0222)\n",
      "tensor(-0.0228)\n",
      "tensor(0.0356)\n",
      "tensor(-0.0212)\n",
      "tensor(0.0357)\n",
      "tensor(-0.0343)\n",
      "tensor(-0.0318)\n",
      "tensor(0.0219)\n",
      "tensor(0.0037)\n",
      "tensor(0.0021)\n"
     ]
    }
   ],
   "source": [
    "for k in weights[0][1]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dc8d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.) tensor(-0.0198)\n",
      "tensor(0.) tensor(0.) tensor(-0.0150)\n",
      "tensor(0.) tensor(0.) tensor(-0.0104)\n",
      "tensor(0.) tensor(0.) tensor(-0.0348)\n",
      "tensor(0.) tensor(0.) tensor(0.0019)\n",
      "tensor(0.) tensor(0.) tensor(0.0063)\n",
      "tensor(0.) tensor(0.) tensor(-3.0238e-05)\n",
      "tensor(0.) tensor(0.) tensor(0.0115)\n",
      "tensor(0.) tensor(0.) tensor(0.0339)\n",
      "tensor(0.) tensor(0.) tensor(0.0095)\n",
      "tensor(0.) tensor(0.) tensor(-0.0131)\n",
      "tensor(0.) tensor(0.) tensor(-0.0147)\n",
      "tensor(0.) tensor(0.) tensor(-0.0229)\n",
      "tensor(0.) tensor(0.) tensor(-0.0248)\n",
      "tensor(0.) tensor(0.) tensor(-0.0058)\n",
      "tensor(0.) tensor(0.) tensor(-0.0063)\n",
      "tensor(0.) tensor(0.) tensor(0.0159)\n",
      "tensor(0.) tensor(0.) tensor(-0.0153)\n",
      "tensor(0.) tensor(0.) tensor(0.0285)\n",
      "tensor(0.) tensor(0.) tensor(-0.0251)\n",
      "tensor(0.) tensor(0.) tensor(0.0001)\n",
      "tensor(0.) tensor(0.) tensor(0.0321)\n",
      "tensor(0.) tensor(0.) tensor(0.0355)\n",
      "tensor(0.) tensor(0.) tensor(-0.0207)\n",
      "tensor(0.) tensor(0.) tensor(0.0064)\n",
      "tensor(0.) tensor(0.) tensor(0.0042)\n",
      "tensor(0.) tensor(0.) tensor(-0.0167)\n",
      "tensor(0.) tensor(0.) tensor(-0.0123)\n",
      "tensor(0.) tensor(0.) tensor(0.0097)\n",
      "tensor(0.) tensor(0.) tensor(-0.0248)\n",
      "tensor(0.) tensor(0.) tensor(0.0059)\n",
      "tensor(0.) tensor(0.) tensor(0.0155)\n",
      "tensor(0.) tensor(0.) tensor(-0.0141)\n",
      "tensor(0.) tensor(0.) tensor(0.0297)\n",
      "tensor(0.) tensor(0.) tensor(-0.0024)\n",
      "tensor(0.) tensor(0.) tensor(0.0162)\n",
      "tensor(0.) tensor(0.) tensor(0.0354)\n",
      "tensor(0.) tensor(0.) tensor(-0.0109)\n",
      "tensor(0.) tensor(0.) tensor(0.0194)\n",
      "tensor(0.) tensor(0.) tensor(-0.0102)\n",
      "tensor(0.) tensor(0.) tensor(-0.0052)\n",
      "tensor(0.) tensor(0.) tensor(-0.0061)\n",
      "tensor(0.) tensor(0.) tensor(-0.0002)\n",
      "tensor(0.) tensor(0.) tensor(-0.0135)\n",
      "tensor(0.) tensor(0.) tensor(0.0084)\n",
      "tensor(0.) tensor(0.) tensor(0.0013)\n",
      "tensor(0.) tensor(0.) tensor(0.0226)\n",
      "tensor(0.) tensor(0.) tensor(-0.0072)\n",
      "tensor(0.) tensor(0.) tensor(0.0036)\n",
      "tensor(0.) tensor(0.) tensor(-0.0133)\n",
      "tensor(0.) tensor(0.) tensor(-0.0299)\n",
      "tensor(0.) tensor(0.) tensor(0.0145)\n",
      "tensor(0.) tensor(0.) tensor(0.0046)\n",
      "tensor(0.) tensor(0.) tensor(-0.0143)\n",
      "tensor(0.) tensor(0.) tensor(-0.0121)\n",
      "tensor(0.) tensor(0.) tensor(0.0093)\n",
      "tensor(0.) tensor(0.) tensor(-0.0065)\n",
      "tensor(0.) tensor(0.) tensor(0.0233)\n",
      "tensor(0.) tensor(0.) tensor(0.0020)\n",
      "tensor(0.) tensor(0.) tensor(0.0135)\n",
      "tensor(0.) tensor(0.) tensor(0.0156)\n",
      "tensor(0.) tensor(0.) tensor(-0.0088)\n",
      "tensor(0.) tensor(0.) tensor(0.0155)\n",
      "tensor(0.) tensor(0.) tensor(0.0264)\n",
      "tensor(0.) tensor(0.) tensor(0.0016)\n",
      "tensor(0.) tensor(0.) tensor(0.0070)\n",
      "tensor(0.) tensor(0.) tensor(0.0013)\n",
      "tensor(0.) tensor(0.) tensor(0.0247)\n",
      "tensor(0.) tensor(0.) tensor(-0.0151)\n",
      "tensor(0.) tensor(0.) tensor(-0.0190)\n",
      "tensor(0.) tensor(0.) tensor(0.0156)\n",
      "tensor(0.) tensor(0.) tensor(-0.0311)\n",
      "tensor(0.) tensor(0.) tensor(0.0006)\n",
      "tensor(0.) tensor(0.) tensor(-0.0164)\n",
      "tensor(0.) tensor(0.) tensor(0.0236)\n",
      "tensor(0.) tensor(0.) tensor(-0.0333)\n",
      "tensor(0.) tensor(0.) tensor(0.0217)\n",
      "tensor(0.) tensor(0.) tensor(0.0353)\n",
      "tensor(0.) tensor(0.) tensor(0.0072)\n",
      "tensor(0.) tensor(0.) tensor(-0.0024)\n",
      "tensor(0.) tensor(0.) tensor(0.0352)\n",
      "tensor(0.) tensor(0.) tensor(-0.0153)\n",
      "tensor(0.) tensor(0.) tensor(-0.0029)\n",
      "tensor(0.) tensor(0.) tensor(-0.0156)\n",
      "tensor(0.) tensor(0.) tensor(-0.0049)\n",
      "tensor(0.) tensor(0.) tensor(0.0078)\n",
      "tensor(0.) tensor(0.) tensor(0.0048)\n",
      "tensor(0.) tensor(0.) tensor(-0.0067)\n",
      "tensor(0.) tensor(0.) tensor(-0.0357)\n",
      "tensor(0.) tensor(0.) tensor(0.0018)\n",
      "tensor(0.) tensor(0.) tensor(-0.0011)\n",
      "tensor(0.) tensor(0.) tensor(0.0054)\n",
      "tensor(0.) tensor(0.) tensor(0.0274)\n",
      "tensor(0.) tensor(0.) tensor(0.0347)\n",
      "tensor(0.) tensor(0.) tensor(-0.0212)\n",
      "tensor(0.) tensor(0.) tensor(-0.0022)\n",
      "tensor(0.) tensor(0.) tensor(-0.0143)\n",
      "tensor(0.) tensor(0.) tensor(-0.0329)\n",
      "tensor(0.) tensor(0.) tensor(-0.0260)\n",
      "tensor(0.) tensor(0.) tensor(0.0335)\n",
      "tensor(0.) tensor(0.) tensor(0.0208)\n",
      "tensor(0.) tensor(0.) tensor(0.0192)\n",
      "tensor(0.) tensor(0.) tensor(0.0198)\n",
      "tensor(0.) tensor(0.) tensor(-0.0284)\n",
      "tensor(0.) tensor(0.) tensor(0.0038)\n",
      "tensor(0.) tensor(0.) tensor(0.0332)\n",
      "tensor(0.) tensor(0.) tensor(-0.0199)\n",
      "tensor(0.) tensor(0.) tensor(0.0318)\n",
      "tensor(0.) tensor(0.) tensor(0.0094)\n",
      "tensor(0.) tensor(0.) tensor(0.0251)\n",
      "tensor(0.) tensor(0.) tensor(-0.0153)\n",
      "tensor(0.) tensor(0.) tensor(0.0165)\n",
      "tensor(0.) tensor(0.) tensor(-0.0317)\n",
      "tensor(0.) tensor(0.) tensor(-0.0023)\n",
      "tensor(0.) tensor(0.) tensor(0.0119)\n",
      "tensor(0.) tensor(0.) tensor(0.0107)\n",
      "tensor(0.) tensor(0.) tensor(0.0299)\n",
      "tensor(0.) tensor(0.) tensor(0.0351)\n",
      "tensor(0.) tensor(0.) tensor(0.0325)\n",
      "tensor(0.) tensor(0.) tensor(0.0240)\n",
      "tensor(0.) tensor(0.) tensor(0.0251)\n",
      "tensor(0.) tensor(0.) tensor(-0.0046)\n",
      "tensor(0.) tensor(0.) tensor(-0.0277)\n",
      "tensor(0.) tensor(0.) tensor(-0.0133)\n",
      "tensor(0.) tensor(0.) tensor(-0.0040)\n",
      "tensor(0.) tensor(0.) tensor(-0.0193)\n",
      "tensor(0.) tensor(0.) tensor(0.0183)\n",
      "tensor(0.) tensor(0.) tensor(0.0125)\n",
      "tensor(0.) tensor(0.) tensor(0.0058)\n",
      "tensor(0.) tensor(0.) tensor(0.0086)\n",
      "tensor(0.) tensor(0.) tensor(0.0314)\n",
      "tensor(0.) tensor(0.) tensor(0.0130)\n",
      "tensor(0.) tensor(0.) tensor(-0.0313)\n",
      "tensor(0.) tensor(0.) tensor(-0.0259)\n",
      "tensor(0.) tensor(0.) tensor(0.0157)\n",
      "tensor(0.) tensor(0.) tensor(0.0049)\n",
      "tensor(0.) tensor(0.) tensor(0.0174)\n",
      "tensor(0.) tensor(0.) tensor(-0.0357)\n",
      "tensor(0.) tensor(0.) tensor(-0.0330)\n",
      "tensor(0.) tensor(0.) tensor(0.0219)\n",
      "tensor(0.) tensor(0.) tensor(0.0228)\n",
      "tensor(0.) tensor(0.) tensor(-0.0323)\n",
      "tensor(0.) tensor(0.) tensor(0.0136)\n",
      "tensor(0.) tensor(0.) tensor(-0.0279)\n",
      "tensor(0.) tensor(0.) tensor(0.0270)\n",
      "tensor(0.) tensor(0.) tensor(0.0112)\n",
      "tensor(0.) tensor(0.) tensor(0.0353)\n",
      "tensor(0.) tensor(0.) tensor(-0.0352)\n",
      "tensor(0.) tensor(0.) tensor(-0.0344)\n",
      "tensor(0.) tensor(0.) tensor(0.0241)\n",
      "tensor(0.) tensor(0.) tensor(0.0317)\n",
      "tensor(0.) tensor(0.) tensor(0.0219)\n",
      "tensor(0.0712) tensor(3.) tensor(0.0237)\n",
      "tensor(-0.4702) tensor(18.) tensor(-0.0301)\n",
      "tensor(-0.0049) tensor(18.) tensor(0.0259)\n",
      "tensor(-0.6119) tensor(18.) tensor(-0.0337)\n",
      "tensor(0.3934) tensor(126.) tensor(0.0080)\n",
      "tensor(-2.7882) tensor(136.) tensor(-0.0234)\n",
      "tensor(-5.2706) tensor(175.) tensor(-0.0142)\n",
      "tensor(-5.1062) tensor(26.) tensor(0.0063)\n",
      "tensor(-7.4372) tensor(166.) tensor(-0.0140)\n",
      "tensor(-8.8782) tensor(255.) tensor(-0.0057)\n",
      "tensor(-17.2393) tensor(247.) tensor(-0.0339)\n",
      "tensor(-21.5159) tensor(127.) tensor(-0.0337)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0138)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0151)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0217)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0141)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0292)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0286)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0214)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0182)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0048)\n",
      "tensor(-21.5159) tensor(0.) tensor(-0.0195)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0119)\n",
      "tensor(-21.5159) tensor(0.) tensor(0.0016)\n",
      "tensor(-22.2817) tensor(30.) tensor(-0.0255)\n",
      "tensor(-22.0050) tensor(36.) tensor(0.0077)\n",
      "tensor(-18.9482) tensor(94.) tensor(0.0325)\n",
      "tensor(-24.3580) tensor(154.) tensor(-0.0351)\n",
      "tensor(-30.3954) tensor(170.) tensor(-0.0355)\n",
      "tensor(-29.2377) tensor(253.) tensor(0.0046)\n",
      "tensor(-22.4336) tensor(253.) tensor(0.0269)\n",
      "tensor(-27.9317) tensor(253.) tensor(-0.0217)\n",
      "tensor(-25.9426) tensor(253.) tensor(0.0079)\n",
      "tensor(-19.5906) tensor(253.) tensor(0.0251)\n",
      "tensor(-17.1131) tensor(225.) tensor(0.0110)\n",
      "tensor(-15.7837) tensor(172.) tensor(0.0077)\n",
      "tensor(-6.7686) tensor(253.) tensor(0.0356)\n",
      "tensor(-1.1380) tensor(242.) tensor(0.0233)\n",
      "tensor(-5.0549) tensor(195.) tensor(-0.0201)\n",
      "tensor(-4.8573) tensor(64.) tensor(0.0031)\n",
      "tensor(-4.8573) tensor(0.) tensor(0.0182)\n",
      "tensor(-4.8573) tensor(0.) tensor(0.0315)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0355)\n",
      "tensor(-4.8573) tensor(0.) tensor(0.0193)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0104)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0350)\n",
      "tensor(-4.8573) tensor(0.) tensor(0.0079)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0194)\n",
      "tensor(-4.8573) tensor(0.) tensor(0.0051)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0232)\n",
      "tensor(-4.8573) tensor(0.) tensor(-0.0269)\n",
      "tensor(-5.0058) tensor(49.) tensor(-0.0030)\n",
      "tensor(-7.8612) tensor(238.) tensor(-0.0120)\n",
      "tensor(-2.5765) tensor(253.) tensor(0.0209)\n",
      "tensor(-1.7975) tensor(253.) tensor(0.0031)\n",
      "tensor(5.2555) tensor(253.) tensor(0.0279)\n",
      "tensor(6.9488) tensor(253.) tensor(0.0067)\n",
      "tensor(4.0432) tensor(253.) tensor(-0.0115)\n",
      "tensor(10.1634) tensor(253.) tensor(0.0242)\n",
      "tensor(10.6572) tensor(253.) tensor(0.0020)\n",
      "tensor(12.3531) tensor(253.) tensor(0.0067)\n",
      "tensor(9.4700) tensor(251.) tensor(-0.0115)\n",
      "tensor(9.4819) tensor(93.) tensor(0.0001)\n",
      "tensor(8.9188) tensor(82.) tensor(-0.0069)\n",
      "tensor(8.6699) tensor(82.) tensor(-0.0030)\n",
      "tensor(7.1101) tensor(56.) tensor(-0.0279)\n",
      "tensor(7.1015) tensor(39.) tensor(-0.0002)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0296)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0077)\n",
      "tensor(7.1015) tensor(0.) tensor(0.0207)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0248)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0241)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0087)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0098)\n",
      "tensor(7.1015) tensor(0.) tensor(0.0060)\n",
      "tensor(7.1015) tensor(0.) tensor(0.0059)\n",
      "tensor(7.1015) tensor(0.) tensor(0.0181)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0246)\n",
      "tensor(7.1015) tensor(0.) tensor(-0.0025)\n",
      "tensor(7.4611) tensor(18.) tensor(0.0200)\n",
      "tensor(14.2264) tensor(219.) tensor(0.0309)\n",
      "tensor(13.5673) tensor(253.) tensor(-0.0026)\n",
      "tensor(8.3390) tensor(253.) tensor(-0.0207)\n",
      "tensor(10.9365) tensor(253.) tensor(0.0103)\n",
      "tensor(16.0821) tensor(253.) tensor(0.0203)\n",
      "tensor(10.4370) tensor(253.) tensor(-0.0223)\n",
      "tensor(6.3335) tensor(198.) tensor(-0.0207)\n",
      "tensor(9.2065) tensor(182.) tensor(0.0158)\n",
      "tensor(8.5884) tensor(247.) tensor(-0.0025)\n",
      "tensor(0.4590) tensor(241.) tensor(-0.0337)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0206)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0145)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0121)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0247)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0282)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0052)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0028)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0113)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0018)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0065)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0273)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0085)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0297)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0219)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0227)\n",
      "tensor(0.4590) tensor(0.) tensor(0.0326)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0092)\n",
      "tensor(0.4590) tensor(0.) tensor(-0.0205)\n",
      "tensor(1.8317) tensor(80.) tensor(0.0172)\n",
      "tensor(2.6623) tensor(156.) tensor(0.0053)\n",
      "tensor(5.3085) tensor(107.) tensor(0.0247)\n",
      "tensor(9.0806) tensor(253.) tensor(0.0149)\n",
      "tensor(0.3761) tensor(253.) tensor(-0.0344)\n",
      "tensor(5.0069) tensor(205.) tensor(0.0226)\n",
      "tensor(4.9329) tensor(11.) tensor(-0.0067)\n",
      "tensor(4.9329) tensor(0.) tensor(-0.0158)\n",
      "tensor(5.9082) tensor(43.) tensor(0.0227)\n",
      "tensor(9.9195) tensor(154.) tensor(0.0260)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0314)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0032)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0293)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0138)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0301)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0122)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0197)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0307)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0149)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0343)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0149)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0229)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0042)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0336)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0140)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0263)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0086)\n",
      "tensor(9.9195) tensor(0.) tensor(-0.0035)\n",
      "tensor(9.9195) tensor(0.) tensor(0.0177)\n",
      "tensor(9.6021) tensor(14.) tensor(-0.0227)\n",
      "tensor(9.6370) tensor(1.) tensor(0.0349)\n",
      "tensor(4.1681) tensor(154.) tensor(-0.0355)\n",
      "tensor(-4.4876) tensor(253.) tensor(-0.0342)\n",
      "tensor(-5.2471) tensor(90.) tensor(-0.0084)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0292)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0036)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0137)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0262)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0130)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0040)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0143)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0252)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0155)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0030)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0022)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0224)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0129)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0232)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0143)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0222)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0142)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0083)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0008)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0328)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0004)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0036)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0032)\n",
      "tensor(-5.2471) tensor(0.) tensor(-0.0071)\n",
      "tensor(-5.2471) tensor(0.) tensor(0.0282)\n",
      "tensor(-1.5835) tensor(139.) tensor(0.0264)\n",
      "tensor(-7.7076) tensor(253.) tensor(-0.0242)\n",
      "tensor(-4.5554) tensor(190.) tensor(0.0166)\n",
      "tensor(-4.6114) tensor(2.) tensor(-0.0280)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0304)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0109)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0005)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0167)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0344)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0138)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0119)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0331)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0347)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0045)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0257)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0313)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0138)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0058)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0325)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0238)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0023)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0337)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0050)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0231)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0248)\n",
      "tensor(-4.6114) tensor(0.) tensor(0.0140)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0275)\n",
      "tensor(-4.6114) tensor(0.) tensor(-0.0271)\n",
      "tensor(-4.3874) tensor(11.) tensor(0.0204)\n",
      "tensor(-3.1187) tensor(190.) tensor(0.0067)\n",
      "tensor(5.1102) tensor(253.) tensor(0.0325)\n",
      "tensor(4.4420) tensor(70.) tensor(-0.0095)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0173)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0033)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0002)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0272)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0356)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0025)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0158)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0197)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0181)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0173)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0342)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0085)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0276)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0178)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0305)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0220)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0338)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0300)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0005)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0281)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0306)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0159)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0314)\n",
      "tensor(4.4420) tensor(0.) tensor(-0.0060)\n",
      "tensor(4.4420) tensor(0.) tensor(0.0148)\n",
      "tensor(3.2395) tensor(35.) tensor(-0.0344)\n",
      "tensor(0.5327) tensor(241.) tensor(-0.0112)\n",
      "tensor(-3.5481) tensor(225.) tensor(-0.0181)\n",
      "tensor(1.2694) tensor(160.) tensor(0.0301)\n",
      "tensor(2.8703) tensor(108.) tensor(0.0148)\n",
      "tensor(2.8442) tensor(1.) tensor(-0.0261)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0058)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0004)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0147)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0204)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0119)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0282)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0256)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0254)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0110)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0006)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0097)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0011)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0086)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0217)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0064)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0144)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0289)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0348)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0228)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0291)\n",
      "tensor(2.8442) tensor(0.) tensor(-0.0065)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0278)\n",
      "tensor(2.8442) tensor(0.) tensor(0.0313)\n",
      "tensor(0.3258) tensor(81.) tensor(-0.0311)\n",
      "tensor(-0.0455) tensor(240.) tensor(-0.0015)\n",
      "tensor(5.2581) tensor(253.) tensor(0.0210)\n",
      "tensor(9.2410) tensor(253.) tensor(0.0157)\n",
      "tensor(12.7222) tensor(119.) tensor(0.0293)\n",
      "tensor(12.6975) tensor(25.) tensor(-0.0010)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0240)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0236)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0122)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0185)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0088)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0152)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0321)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0332)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0325)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0236)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0148)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0228)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0343)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0032)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0237)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0159)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0197)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0146)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0312)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0333)\n",
      "tensor(12.6975) tensor(0.) tensor(-0.0352)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0154)\n",
      "tensor(12.6975) tensor(0.) tensor(0.0177)\n",
      "tensor(11.6993) tensor(45.) tensor(-0.0222)\n",
      "tensor(13.3466) tensor(186.) tensor(0.0089)\n",
      "tensor(20.0166) tensor(253.) tensor(0.0264)\n",
      "tensor(21.3295) tensor(253.) tensor(0.0052)\n",
      "tensor(20.1960) tensor(150.) tensor(-0.0076)\n",
      "tensor(20.2673) tensor(27.) tensor(0.0026)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0038)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0180)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0243)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0181)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0201)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0216)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0292)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0351)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0141)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0100)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0178)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0280)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0141)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0083)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0225)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0310)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0297)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0160)\n",
      "tensor(20.2673) tensor(0.) tensor(0.0129)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0332)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0199)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0327)\n",
      "tensor(20.2673) tensor(0.) tensor(-0.0256)\n",
      "tensor(20.0095) tensor(16.) tensor(-0.0161)\n",
      "tensor(22.7581) tensor(93.) tensor(0.0296)\n",
      "tensor(20.9666) tensor(252.) tensor(-0.0071)\n",
      "tensor(26.1563) tensor(253.) tensor(0.0205)\n",
      "tensor(32.4988) tensor(187.) tensor(0.0339)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0163)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0320)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0180)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0136)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0277)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0352)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0220)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0292)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0332)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0037)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0305)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0260)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0117)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0158)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0094)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0355)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0292)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0306)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0045)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0300)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0333)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0193)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0076)\n",
      "tensor(32.4988) tensor(0.) tensor(-0.0106)\n",
      "tensor(32.4988) tensor(0.) tensor(0.0330)\n",
      "tensor(36.6933) tensor(249.) tensor(0.0168)\n",
      "tensor(42.5106) tensor(253.) tensor(0.0230)\n",
      "tensor(48.5596) tensor(249.) tensor(0.0243)\n",
      "tensor(46.8521) tensor(64.) tensor(-0.0267)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0041)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0052)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0093)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0184)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0142)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0108)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0299)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0096)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0248)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0242)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0242)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0112)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0071)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0264)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0298)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0134)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0153)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0284)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0286)\n",
      "tensor(46.8521) tensor(0.) tensor(-0.0049)\n",
      "tensor(46.8521) tensor(0.) tensor(0.0008)\n",
      "tensor(47.6965) tensor(46.) tensor(0.0184)\n",
      "tensor(51.4541) tensor(130.) tensor(0.0289)\n",
      "tensor(55.8801) tensor(183.) tensor(0.0242)\n",
      "tensor(59.8576) tensor(253.) tensor(0.0157)\n",
      "tensor(55.7102) tensor(253.) tensor(-0.0164)\n",
      "tensor(50.4307) tensor(207.) tensor(-0.0255)\n",
      "tensor(50.3668) tensor(2.) tensor(-0.0320)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0018)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0346)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0068)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0015)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0260)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0110)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0226)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0352)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0251)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0265)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0085)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0022)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0064)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0054)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0250)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0334)\n",
      "tensor(50.3668) tensor(0.) tensor(0.0235)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0042)\n",
      "tensor(50.3668) tensor(0.) tensor(-0.0133)\n",
      "tensor(50.0072) tensor(39.) tensor(-0.0092)\n",
      "tensor(47.1243) tensor(148.) tensor(-0.0195)\n",
      "tensor(44.7019) tensor(229.) tensor(-0.0106)\n",
      "tensor(36.9455) tensor(253.) tensor(-0.0307)\n",
      "tensor(43.7074) tensor(253.) tensor(0.0267)\n",
      "tensor(47.3970) tensor(253.) tensor(0.0146)\n",
      "tensor(42.8942) tensor(250.) tensor(-0.0180)\n",
      "tensor(40.6756) tensor(182.) tensor(-0.0122)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0160)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0132)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0094)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0098)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0225)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0089)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0022)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0312)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0129)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0092)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0308)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0292)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0026)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0344)\n",
      "tensor(40.6756) tensor(0.) tensor(-0.0160)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0342)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0317)\n",
      "tensor(40.6756) tensor(0.) tensor(0.0332)\n",
      "tensor(40.5938) tensor(24.) tensor(-0.0034)\n",
      "tensor(44.2770) tensor(114.) tensor(0.0323)\n",
      "tensor(40.8379) tensor(221.) tensor(-0.0156)\n",
      "tensor(37.7449) tensor(253.) tensor(-0.0122)\n",
      "tensor(41.1625) tensor(253.) tensor(0.0135)\n",
      "tensor(43.0713) tensor(253.) tensor(0.0075)\n",
      "tensor(48.0525) tensor(253.) tensor(0.0197)\n",
      "tensor(54.8744) tensor(201.) tensor(0.0339)\n",
      "tensor(53.1306) tensor(78.) tensor(-0.0224)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0190)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0119)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0188)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0169)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0134)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0282)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0286)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0204)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0228)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0340)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0062)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0004)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0062)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0029)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0113)\n",
      "tensor(53.1306) tensor(0.) tensor(-0.0040)\n",
      "tensor(53.1306) tensor(0.) tensor(0.0087)\n",
      "tensor(53.9435) tensor(23.) tensor(0.0353)\n",
      "tensor(52.4183) tensor(66.) tensor(-0.0231)\n",
      "tensor(52.0029) tensor(213.) tensor(-0.0019)\n",
      "tensor(48.5538) tensor(253.) tensor(-0.0136)\n",
      "tensor(56.4820) tensor(253.) tensor(0.0313)\n",
      "tensor(64.2169) tensor(253.) tensor(0.0306)\n",
      "tensor(63.9705) tensor(253.) tensor(-0.0010)\n",
      "tensor(61.5193) tensor(198.) tensor(-0.0124)\n",
      "tensor(59.7319) tensor(81.) tensor(-0.0221)\n",
      "tensor(59.7831) tensor(2.) tensor(0.0256)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0025)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0336)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0204)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0313)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0282)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0232)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0105)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0148)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0179)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0032)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0031)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0247)\n",
      "tensor(59.7831) tensor(0.) tensor(-0.0319)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0134)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0083)\n",
      "tensor(59.7831) tensor(0.) tensor(0.0031)\n",
      "tensor(60.2197) tensor(18.) tensor(0.0243)\n",
      "tensor(57.6695) tensor(171.) tensor(-0.0149)\n",
      "tensor(60.4460) tensor(219.) tensor(0.0127)\n",
      "tensor(55.4694) tensor(253.) tensor(-0.0197)\n",
      "tensor(57.3416) tensor(253.) tensor(0.0074)\n",
      "tensor(55.2788) tensor(253.) tensor(-0.0082)\n",
      "tensor(61.6067) tensor(253.) tensor(0.0250)\n",
      "tensor(67.6969) tensor(195.) tensor(0.0312)\n",
      "tensor(70.1784) tensor(80.) tensor(0.0310)\n",
      "tensor(70.1928) tensor(9.) tensor(0.0016)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0211)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0357)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0209)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0064)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0002)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0280)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0158)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0167)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0354)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0034)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0243)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0145)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0170)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0328)\n",
      "tensor(70.1928) tensor(0.) tensor(-0.0288)\n",
      "tensor(70.1928) tensor(0.) tensor(0.0007)\n",
      "tensor(71.5441) tensor(55.) tensor(0.0246)\n",
      "tensor(69.0564) tensor(172.) tensor(-0.0145)\n",
      "tensor(64.6644) tensor(226.) tensor(-0.0194)\n",
      "tensor(59.8442) tensor(253.) tensor(-0.0191)\n",
      "tensor(59.0620) tensor(253.) tensor(-0.0031)\n",
      "tensor(50.4812) tensor(253.) tensor(-0.0339)\n",
      "tensor(42.5077) tensor(253.) tensor(-0.0315)\n",
      "tensor(51.0860) tensor(244.) tensor(0.0352)\n",
      "tensor(48.8206) tensor(133.) tensor(-0.0170)\n",
      "tensor(48.7497) tensor(11.) tensor(-0.0064)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0096)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0314)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0063)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0109)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0145)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0015)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0002)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0356)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0260)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0288)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0244)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0117)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0187)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0335)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0088)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0073)\n",
      "tensor(48.7497) tensor(0.) tensor(-0.0126)\n",
      "tensor(48.7497) tensor(0.) tensor(0.0150)\n",
      "tensor(46.5365) tensor(136.) tensor(-0.0163)\n",
      "tensor(45.4893) tensor(253.) tensor(-0.0041)\n",
      "tensor(42.8802) tensor(253.) tensor(-0.0103)\n",
      "tensor(34.3949) tensor(253.) tensor(-0.0335)\n",
      "tensor(33.1445) tensor(212.) tensor(-0.0059)\n",
      "tensor(29.0585) tensor(135.) tensor(-0.0303)\n",
      "tensor(31.8074) tensor(132.) tensor(0.0208)\n",
      "tensor(32.1128) tensor(16.) tensor(0.0191)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0335)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0120)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0018)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0070)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0357)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0327)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0185)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0304)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0322)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0172)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0300)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0120)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0345)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0265)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0005)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0221)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0349)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0286)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0155)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0195)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0281)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0033)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0236)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0149)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0190)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0313)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0198)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0214)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0074)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0311)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0325)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0008)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0288)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0279)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0123)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0135)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0150)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0074)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0132)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0196)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0068)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0271)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0081)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0055)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0206)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0327)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0104)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0331)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0066)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0006)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0023)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0175)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0248)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0201)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0088)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0141)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0225)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0311)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0327)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0077)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0033)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0129)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0305)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0053)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0303)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0002)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0039)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0208)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0119)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0115)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0158)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0013)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0340)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0247)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0137)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0322)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0062)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0284)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0006)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0267)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0196)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0133)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0155)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0086)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0352)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0121)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0033)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0089)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0206)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0300)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0268)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0300)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0073)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0070)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0152)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0349)\n",
      "tensor(32.1128) tensor(0.) tensor(0.0193)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0203)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0060)\n",
      "tensor(32.1128) tensor(0.) tensor(-0.0299)\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(x1)):\n",
    "    sum += x1[i]*weights[0][1][i]\n",
    "    print(sum, x1[i], weights[0][1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8915bb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  27.6099,   32.1128,  -50.6857,  -10.6418,   22.0753,  -34.9349,\n",
       "        -113.4836,   20.4312,   54.3101,  -60.3901,   24.8315,    3.8750,\n",
       "         -66.7918,  -25.1650,   43.1287,   54.5816,    3.3011,  -17.7552])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1@weights[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa754dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18.,\n",
       "        126., 136., 175.,  26., 166., 255., 247., 127.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154.,\n",
       "        170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
       "        238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,\n",
       "         82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182.,\n",
       "        247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,\n",
       "        190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253.,\n",
       "        253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0., 249., 253., 249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253.,\n",
       "        253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253.,\n",
       "        253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171.,\n",
       "        219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7bc31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullprint(*args, **kwargs):\n",
    "  from pprint import pprint\n",
    "  import numpy\n",
    "  opt = numpy.get_printoptions()\n",
    "  numpy.set_printoptions(threshold=numpy.inf)\n",
    "  pprint(*args, **kwargs)\n",
    "  numpy.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c81f5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
      "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
      "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
      "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
      "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
      "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
      "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
      "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
      "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
      "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
      "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
      "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
      "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         0.,   0.,   0.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "fullprint(np.array(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bbba118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0112,  0.0016,  0.0017, -0.0255, -0.0138, -0.0105, -0.0166, -0.0136,\n",
       "         0.0042,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320, -0.0351,  0.0314,\n",
       "         0.0258,  0.0109])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a45e5f08-4e7b-4fa5-8ef9-509a57485627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27.6211, 32.1144,  0.0000,  0.0000, 22.0615,  0.0000,  0.0000, 20.4177,\n",
      "        54.3143,  0.0000, 24.8167,  3.8441,  0.0000,  0.0000, 43.0936, 54.6130,\n",
      "         3.3269,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "fullprint(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39c2e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -1.2346,  -5.9150,  -7.3258,   8.3320,  14.6910,  -6.4584,  -0.4364,\n",
      "         11.8747,  19.4332, -14.6356])\n"
     ]
    }
   ],
   "source": [
    "fullprint(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "id": "0976e3e5-3b18-4b74-bcba-bc3394a3a377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3730)"
      ]
     },
     "execution_count": 1341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward(a,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "d80995d4-97b5-4ac0-81ce-f6e1d7e59a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0112,  0.0013,  0.0017, -0.0254, -0.0139, -0.0106, -0.0166, -0.0132,\n",
       "         0.0040,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320, -0.0349,  0.0310,\n",
       "         0.0263,  0.0108])"
      ]
     },
     "execution_count": 1342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "1f46bf38-657d-4869-b2c9-cd74c1ad903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1410, -0.0624, -0.0507, -0.2053, -0.0986, -0.1225,  0.0111,  0.2333,\n",
       "         0.1426,  0.1192])"
      ]
     },
     "execution_count": 1343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "id": "30447233-f74a-4c21-a1e5-73d66b451d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
       "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
       "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
       "        ...,\n",
       "        [-0.0215,  0.0106,  0.0308,  ..., -0.0199,  0.0161, -0.0342],\n",
       "        [ 0.0350, -0.0297, -0.0037,  ...,  0.0171,  0.0238, -0.0001],\n",
       "        [ 0.0085,  0.0223, -0.0324,  ..., -0.0296,  0.0182, -0.0296]])"
      ]
     },
     "execution_count": 1336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "id": "4204f907-bd09-4c59-838a-5e65e4c2d256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1390, -0.1493, -0.2147,  0.2143,  0.1171, -0.1545, -0.1654,  0.0632,\n",
       "         0.1547,  0.2064,  0.0297,  0.1380,  0.0920,  0.0833, -0.1179, -0.1704,\n",
       "         0.2290,  0.0490])"
      ]
     },
     "execution_count": 1344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1][y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "c6d79033-4ef6-4e14-a1c7-ccb824f32ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28]), torch.Size([1]))"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = next(iter(train_dl))\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "2a83a76c-bfe5-4b2b-84f3-aefc023adad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0865, 0.0919, 0.0930, 0.0835, 0.0954, 0.0857, 0.1006, 0.1327, 0.1245,\n",
       "         0.1064]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "model(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f358c9-332b-41f5-be8d-20b9306ce7a1",
   "metadata": {},
   "source": [
    "### Test the full training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "40d4b6b0-4b80-4797-8399-ad796d57a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dataloader))\n",
    "x,y = x.flatten(), y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "id": "84a71331-792a-4e83-bdf1-45d5b9b7eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    correct = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for i, (x,y) in enumerate(dataloader):\n",
    "        x,y = x.flatten(), y.squeeze()\n",
    "        a = forward(x)\n",
    "        loss = cross_entropy(softmax(a[-1]),y)\n",
    "\n",
    "        backward(a,y)\n",
    "\n",
    "        correct += (a[-1].argmax() == y).type(torch.float).sum()\n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        loss, current = loss.item(), (i+1)\n",
    "        print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        if i > 2000:\n",
    "            return True\n",
    "        \n",
    "    correct /= size\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "e780d092-ec49-4bae-a2fc-ef7fb7a17741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for x,y in dataloader:\n",
    "        x,y = x.flatten(), y.squeeze()\n",
    "        a = forward(x)\n",
    "        test_loss += cross_entropy(softmax(a[-1]),y)\n",
    "        correct += (a[-1].argmax() == y).type(torch.float).sum()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "id": "e54a84ca-7781-4143-a266-7ee08b3dfae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "id": "65eae6c9-1147-4859-ab02-9fb58a668c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.457382 [    1/60000]\n",
      "loss: 2.373045 [    2/60000]\n",
      "loss: 2.397981 [    3/60000]\n",
      "loss: 2.376602 [    4/60000]\n",
      "loss: 2.280104 [    5/60000]\n",
      "loss: 2.347051 [    6/60000]\n",
      "loss: 2.386698 [    7/60000]\n",
      "loss: 2.507777 [    8/60000]\n",
      "loss: 2.379634 [    9/60000]\n",
      "loss: 2.355851 [   10/60000]\n",
      "loss: 2.437241 [   11/60000]\n",
      "loss: 2.384903 [   12/60000]\n",
      "loss: 2.462984 [   13/60000]\n",
      "loss: 2.358255 [   14/60000]\n",
      "loss: 2.352472 [   15/60000]\n",
      "loss: 2.039407 [   16/60000]\n",
      "loss: 2.390954 [   17/60000]\n",
      "loss: 2.078932 [   18/60000]\n",
      "loss: 2.301366 [   19/60000]\n",
      "loss: 2.144966 [   20/60000]\n",
      "loss: 2.441458 [   21/60000]\n",
      "loss: 2.397276 [   22/60000]\n",
      "loss: 2.190477 [   23/60000]\n",
      "loss: 2.356754 [   24/60000]\n",
      "loss: 2.463299 [   25/60000]\n",
      "loss: 2.285216 [   26/60000]\n",
      "loss: 2.384321 [   27/60000]\n",
      "loss: 2.388861 [   28/60000]\n",
      "loss: 2.317114 [   29/60000]\n",
      "loss: 2.032908 [   30/60000]\n",
      "loss: 2.382332 [   31/60000]\n",
      "loss: 2.028679 [   32/60000]\n",
      "loss: 2.347586 [   33/60000]\n",
      "loss: 2.172549 [   34/60000]\n",
      "loss: 2.464722 [   35/60000]\n",
      "loss: 2.426545 [   36/60000]\n",
      "loss: 2.404057 [   37/60000]\n",
      "loss: 2.401548 [   38/60000]\n",
      "loss: 1.999600 [   39/60000]\n",
      "loss: 2.401540 [   40/60000]\n",
      "loss: 2.375507 [   41/60000]\n",
      "loss: 2.009468 [   42/60000]\n",
      "loss: 2.056008 [   43/60000]\n",
      "loss: 2.191117 [   44/60000]\n",
      "loss: 2.444056 [   45/60000]\n",
      "loss: 2.179647 [   46/60000]\n",
      "loss: 2.031539 [   47/60000]\n",
      "loss: 2.431302 [   48/60000]\n",
      "loss: 2.154622 [   49/60000]\n",
      "loss: 2.373725 [   50/60000]\n",
      "loss: 2.432909 [   51/60000]\n",
      "loss: 2.271910 [   52/60000]\n",
      "loss: 2.049218 [   53/60000]\n",
      "loss: 2.380137 [   54/60000]\n",
      "loss: 2.297476 [   55/60000]\n",
      "loss: 2.046081 [   56/60000]\n",
      "loss: 2.407490 [   57/60000]\n",
      "loss: 2.133124 [   58/60000]\n",
      "loss: 2.415659 [   59/60000]\n",
      "loss: 2.350787 [   60/60000]\n",
      "loss: 2.314819 [   61/60000]\n",
      "loss: 2.334585 [   62/60000]\n",
      "loss: 2.362445 [   63/60000]\n",
      "loss: 2.278495 [   64/60000]\n",
      "loss: 2.299920 [   65/60000]\n",
      "loss: 2.319605 [   66/60000]\n",
      "loss: 2.323553 [   67/60000]\n",
      "loss: 2.350267 [   68/60000]\n",
      "loss: 2.512788 [   69/60000]\n",
      "loss: 2.243114 [   70/60000]\n",
      "loss: 2.289125 [   71/60000]\n",
      "loss: 2.095248 [   72/60000]\n",
      "loss: 2.308277 [   73/60000]\n",
      "loss: 2.405866 [   74/60000]\n",
      "loss: 2.411194 [   75/60000]\n",
      "loss: 2.273056 [   76/60000]\n",
      "loss: 2.322269 [   77/60000]\n",
      "loss: 2.383566 [   78/60000]\n",
      "loss: 2.369709 [   79/60000]\n",
      "loss: 2.095891 [   80/60000]\n",
      "loss: 2.201415 [   81/60000]\n",
      "loss: 2.289171 [   82/60000]\n",
      "loss: 2.352151 [   83/60000]\n",
      "loss: 2.371330 [   84/60000]\n",
      "loss: 2.051396 [   85/60000]\n",
      "loss: 1.998211 [   86/60000]\n",
      "loss: 2.476793 [   87/60000]\n",
      "loss: 2.194855 [   88/60000]\n",
      "loss: 2.362277 [   89/60000]\n",
      "loss: 2.327783 [   90/60000]\n",
      "loss: 2.229530 [   91/60000]\n",
      "loss: 2.098575 [   92/60000]\n",
      "loss: 2.380887 [   93/60000]\n",
      "loss: 2.348021 [   94/60000]\n",
      "loss: 2.082936 [   95/60000]\n",
      "loss: 2.287169 [   96/60000]\n",
      "loss: 2.107923 [   97/60000]\n",
      "loss: 2.056831 [   98/60000]\n",
      "loss: 2.382780 [   99/60000]\n",
      "loss: 2.389781 [  100/60000]\n",
      "loss: 2.407669 [  101/60000]\n",
      "loss: 2.132990 [  102/60000]\n",
      "loss: 2.300598 [  103/60000]\n",
      "loss: 2.057990 [  104/60000]\n",
      "loss: 2.298321 [  105/60000]\n",
      "loss: 2.269566 [  106/60000]\n",
      "loss: 2.279950 [  107/60000]\n",
      "loss: 2.414050 [  108/60000]\n",
      "loss: 2.456334 [  109/60000]\n",
      "loss: 2.241653 [  110/60000]\n",
      "loss: 2.222714 [  111/60000]\n",
      "loss: 2.388362 [  112/60000]\n",
      "loss: 2.348019 [  113/60000]\n",
      "loss: 2.357450 [  114/60000]\n",
      "loss: 2.219437 [  115/60000]\n",
      "loss: 2.314506 [  116/60000]\n",
      "loss: 2.221704 [  117/60000]\n",
      "loss: 2.237043 [  118/60000]\n",
      "loss: 2.253497 [  119/60000]\n",
      "loss: 2.092335 [  120/60000]\n",
      "loss: 2.239632 [  121/60000]\n",
      "loss: 2.297700 [  122/60000]\n",
      "loss: 2.305496 [  123/60000]\n",
      "loss: 2.074457 [  124/60000]\n",
      "loss: 2.290267 [  125/60000]\n",
      "loss: 2.092523 [  126/60000]\n",
      "loss: 2.378500 [  127/60000]\n",
      "loss: 2.337579 [  128/60000]\n",
      "loss: 2.332010 [  129/60000]\n",
      "loss: 2.388441 [  130/60000]\n",
      "loss: 2.463928 [  131/60000]\n",
      "loss: 2.298216 [  132/60000]\n",
      "loss: 2.389853 [  133/60000]\n",
      "loss: 2.197062 [  134/60000]\n",
      "loss: 2.405001 [  135/60000]\n",
      "loss: 2.378695 [  136/60000]\n",
      "loss: 2.347516 [  137/60000]\n",
      "loss: 2.094996 [  138/60000]\n",
      "loss: 2.402101 [  139/60000]\n",
      "loss: 2.313690 [  140/60000]\n",
      "loss: 2.086385 [  141/60000]\n",
      "loss: 2.077036 [  142/60000]\n",
      "loss: 2.324522 [  143/60000]\n",
      "loss: 2.350787 [  144/60000]\n",
      "loss: 2.113482 [  145/60000]\n",
      "loss: 2.349719 [  146/60000]\n",
      "loss: 2.128702 [  147/60000]\n",
      "loss: 2.364739 [  148/60000]\n",
      "loss: 2.105729 [  149/60000]\n",
      "loss: 2.317803 [  150/60000]\n",
      "loss: 2.348502 [  151/60000]\n",
      "loss: 2.270796 [  152/60000]\n",
      "loss: 2.300503 [  153/60000]\n",
      "loss: 2.210837 [  154/60000]\n",
      "loss: 2.303529 [  155/60000]\n",
      "loss: 2.346692 [  156/60000]\n",
      "loss: 2.239554 [  157/60000]\n",
      "loss: 2.394547 [  158/60000]\n",
      "loss: 2.104100 [  159/60000]\n",
      "loss: 2.258967 [  160/60000]\n",
      "loss: 2.137830 [  161/60000]\n",
      "loss: 2.264416 [  162/60000]\n",
      "loss: 2.264589 [  163/60000]\n",
      "loss: 2.273919 [  164/60000]\n",
      "loss: 2.347880 [  165/60000]\n",
      "loss: 2.310193 [  166/60000]\n",
      "loss: 2.352126 [  167/60000]\n",
      "loss: 2.211801 [  168/60000]\n",
      "loss: 2.191944 [  169/60000]\n",
      "loss: 2.221404 [  170/60000]\n",
      "loss: 2.185031 [  171/60000]\n",
      "loss: 2.270168 [  172/60000]\n",
      "loss: 2.275368 [  173/60000]\n",
      "loss: 2.352492 [  174/60000]\n",
      "loss: 2.306241 [  175/60000]\n",
      "loss: 2.324425 [  176/60000]\n",
      "loss: 2.236556 [  177/60000]\n",
      "loss: 2.269363 [  178/60000]\n",
      "loss: 2.272643 [  179/60000]\n",
      "loss: 2.348782 [  180/60000]\n",
      "loss: 2.329208 [  181/60000]\n",
      "loss: 2.365948 [  182/60000]\n",
      "loss: 2.456288 [  183/60000]\n",
      "loss: 2.209872 [  184/60000]\n",
      "loss: 2.263571 [  185/60000]\n",
      "loss: 2.123886 [  186/60000]\n",
      "loss: 2.353028 [  187/60000]\n",
      "loss: 2.227600 [  188/60000]\n",
      "loss: 2.143867 [  189/60000]\n",
      "loss: 2.196263 [  190/60000]\n",
      "loss: 2.234114 [  191/60000]\n",
      "loss: 2.286743 [  192/60000]\n",
      "loss: 2.255294 [  193/60000]\n",
      "loss: 2.077945 [  194/60000]\n",
      "loss: 2.307050 [  195/60000]\n",
      "loss: 2.231854 [  196/60000]\n",
      "loss: 2.100355 [  197/60000]\n",
      "loss: 2.091465 [  198/60000]\n",
      "loss: 2.396589 [  199/60000]\n",
      "loss: 2.239959 [  200/60000]\n",
      "loss: 2.263590 [  201/60000]\n",
      "loss: 2.317459 [  202/60000]\n",
      "loss: 2.088704 [  203/60000]\n",
      "loss: 2.353389 [  204/60000]\n",
      "loss: 2.325824 [  205/60000]\n",
      "loss: 2.319028 [  206/60000]\n",
      "loss: 2.193964 [  207/60000]\n",
      "loss: 2.364303 [  208/60000]\n",
      "loss: 2.253922 [  209/60000]\n",
      "loss: 2.227977 [  210/60000]\n",
      "loss: 2.287071 [  211/60000]\n",
      "loss: 2.273409 [  212/60000]\n",
      "loss: 2.134971 [  213/60000]\n",
      "loss: 2.196317 [  214/60000]\n",
      "loss: 2.116901 [  215/60000]\n",
      "loss: 2.196475 [  216/60000]\n",
      "loss: 2.190206 [  217/60000]\n",
      "loss: 2.383431 [  218/60000]\n",
      "loss: 2.373807 [  219/60000]\n",
      "loss: 2.351779 [  220/60000]\n",
      "loss: 2.309998 [  221/60000]\n",
      "loss: 2.333972 [  222/60000]\n",
      "loss: 2.322120 [  223/60000]\n",
      "loss: 2.117773 [  224/60000]\n",
      "loss: 2.371876 [  225/60000]\n",
      "loss: 2.139299 [  226/60000]\n",
      "loss: 2.283282 [  227/60000]\n",
      "loss: 2.269078 [  228/60000]\n",
      "loss: 2.482345 [  229/60000]\n",
      "loss: 2.132218 [  230/60000]\n",
      "loss: 2.121965 [  231/60000]\n",
      "loss: 2.266874 [  232/60000]\n",
      "loss: 2.423333 [  233/60000]\n",
      "loss: 2.257020 [  234/60000]\n",
      "loss: 2.197371 [  235/60000]\n",
      "loss: 2.272507 [  236/60000]\n",
      "loss: 2.342741 [  237/60000]\n",
      "loss: 2.292263 [  238/60000]\n",
      "loss: 2.312138 [  239/60000]\n",
      "loss: 2.334841 [  240/60000]\n",
      "loss: 2.175636 [  241/60000]\n",
      "loss: 2.313413 [  242/60000]\n",
      "loss: 2.247891 [  243/60000]\n",
      "loss: 2.142505 [  244/60000]\n",
      "loss: 2.387748 [  245/60000]\n",
      "loss: 2.083832 [  246/60000]\n",
      "loss: 2.356227 [  247/60000]\n",
      "loss: 2.235393 [  248/60000]\n",
      "loss: 2.288893 [  249/60000]\n",
      "loss: 2.199918 [  250/60000]\n",
      "loss: 2.292549 [  251/60000]\n",
      "loss: 2.248089 [  252/60000]\n",
      "loss: 2.222761 [  253/60000]\n",
      "loss: 2.184698 [  254/60000]\n",
      "loss: 2.342674 [  255/60000]\n",
      "loss: 2.260951 [  256/60000]\n",
      "loss: 2.379725 [  257/60000]\n",
      "loss: 2.227778 [  258/60000]\n",
      "loss: 2.148861 [  259/60000]\n",
      "loss: 2.327298 [  260/60000]\n",
      "loss: 2.336051 [  261/60000]\n",
      "loss: 2.293370 [  262/60000]\n",
      "loss: 2.218271 [  263/60000]\n",
      "loss: 2.145201 [  264/60000]\n",
      "loss: 2.282658 [  265/60000]\n",
      "loss: 2.154414 [  266/60000]\n",
      "loss: 2.421270 [  267/60000]\n",
      "loss: 2.266400 [  268/60000]\n",
      "loss: 2.360982 [  269/60000]\n",
      "loss: 2.255796 [  270/60000]\n",
      "loss: 2.248931 [  271/60000]\n",
      "loss: 2.348825 [  272/60000]\n",
      "loss: 2.247547 [  273/60000]\n",
      "loss: 2.305827 [  274/60000]\n",
      "loss: 2.408217 [  275/60000]\n",
      "loss: 2.320486 [  276/60000]\n",
      "loss: 2.243590 [  277/60000]\n",
      "loss: 2.214653 [  278/60000]\n",
      "loss: 2.337403 [  279/60000]\n",
      "loss: 2.226745 [  280/60000]\n",
      "loss: 2.224096 [  281/60000]\n",
      "loss: 2.264069 [  282/60000]\n",
      "loss: 2.293657 [  283/60000]\n",
      "loss: 2.175113 [  284/60000]\n",
      "loss: 2.358473 [  285/60000]\n",
      "loss: 2.257335 [  286/60000]\n",
      "loss: 2.500476 [  287/60000]\n",
      "loss: 2.296972 [  288/60000]\n",
      "loss: 2.154270 [  289/60000]\n",
      "loss: 2.275074 [  290/60000]\n",
      "loss: 2.219384 [  291/60000]\n",
      "loss: 2.241606 [  292/60000]\n",
      "loss: 2.294342 [  293/60000]\n",
      "loss: 2.165394 [  294/60000]\n",
      "loss: 2.195292 [  295/60000]\n",
      "loss: 2.032838 [  296/60000]\n",
      "loss: 2.216692 [  297/60000]\n",
      "loss: 2.130611 [  298/60000]\n",
      "loss: 2.255613 [  299/60000]\n",
      "loss: 2.317227 [  300/60000]\n",
      "loss: 2.105727 [  301/60000]\n",
      "loss: 2.183590 [  302/60000]\n",
      "loss: 2.372505 [  303/60000]\n",
      "loss: 2.093396 [  304/60000]\n",
      "loss: 2.223962 [  305/60000]\n",
      "loss: 2.156509 [  306/60000]\n",
      "loss: 2.392049 [  307/60000]\n",
      "loss: 2.170038 [  308/60000]\n",
      "loss: 2.257493 [  309/60000]\n",
      "loss: 2.272032 [  310/60000]\n",
      "loss: 2.257843 [  311/60000]\n",
      "loss: 2.249449 [  312/60000]\n",
      "loss: 2.150656 [  313/60000]\n",
      "loss: 2.261206 [  314/60000]\n",
      "loss: 2.325445 [  315/60000]\n",
      "loss: 2.214641 [  316/60000]\n",
      "loss: 2.342757 [  317/60000]\n",
      "loss: 2.206248 [  318/60000]\n",
      "loss: 2.271843 [  319/60000]\n",
      "loss: 2.221464 [  320/60000]\n",
      "loss: 2.160686 [  321/60000]\n",
      "loss: 2.259371 [  322/60000]\n",
      "loss: 2.210989 [  323/60000]\n",
      "loss: 2.309864 [  324/60000]\n",
      "loss: 2.160632 [  325/60000]\n",
      "loss: 2.159148 [  326/60000]\n",
      "loss: 2.055428 [  327/60000]\n",
      "loss: 2.282916 [  328/60000]\n",
      "loss: 2.343452 [  329/60000]\n",
      "loss: 2.193538 [  330/60000]\n",
      "loss: 2.303048 [  331/60000]\n",
      "loss: 2.328331 [  332/60000]\n",
      "loss: 2.319702 [  333/60000]\n",
      "loss: 2.143722 [  334/60000]\n",
      "loss: 2.211198 [  335/60000]\n",
      "loss: 2.353190 [  336/60000]\n",
      "loss: 2.343318 [  337/60000]\n",
      "loss: 2.199970 [  338/60000]\n",
      "loss: 2.224859 [  339/60000]\n",
      "loss: 2.179832 [  340/60000]\n",
      "loss: 2.180333 [  341/60000]\n",
      "loss: 2.117054 [  342/60000]\n",
      "loss: 2.326933 [  343/60000]\n",
      "loss: 2.139122 [  344/60000]\n",
      "loss: 2.224737 [  345/60000]\n",
      "loss: 2.203253 [  346/60000]\n",
      "loss: 2.296676 [  347/60000]\n",
      "loss: 2.160070 [  348/60000]\n",
      "loss: 2.116172 [  349/60000]\n",
      "loss: 2.149085 [  350/60000]\n",
      "loss: 2.235778 [  351/60000]\n",
      "loss: 2.205293 [  352/60000]\n",
      "loss: 2.148149 [  353/60000]\n",
      "loss: 2.178705 [  354/60000]\n",
      "loss: 2.404101 [  355/60000]\n",
      "loss: 2.241778 [  356/60000]\n",
      "loss: 2.332307 [  357/60000]\n",
      "loss: 2.190099 [  358/60000]\n",
      "loss: 2.233865 [  359/60000]\n",
      "loss: 2.115738 [  360/60000]\n",
      "loss: 2.239425 [  361/60000]\n",
      "loss: 2.173202 [  362/60000]\n",
      "loss: 2.338539 [  363/60000]\n",
      "loss: 2.288809 [  364/60000]\n",
      "loss: 2.270792 [  365/60000]\n",
      "loss: 2.197248 [  366/60000]\n",
      "loss: 2.210768 [  367/60000]\n",
      "loss: 2.260345 [  368/60000]\n",
      "loss: 2.107168 [  369/60000]\n",
      "loss: 2.169612 [  370/60000]\n",
      "loss: 2.181272 [  371/60000]\n",
      "loss: 2.184354 [  372/60000]\n",
      "loss: 2.253808 [  373/60000]\n",
      "loss: 2.200822 [  374/60000]\n",
      "loss: 2.273114 [  375/60000]\n",
      "loss: 2.125736 [  376/60000]\n",
      "loss: 2.299017 [  377/60000]\n",
      "loss: 2.179845 [  378/60000]\n",
      "loss: 2.220829 [  379/60000]\n",
      "loss: 2.261926 [  380/60000]\n",
      "loss: 2.181726 [  381/60000]\n",
      "loss: 2.117239 [  382/60000]\n",
      "loss: 2.223618 [  383/60000]\n",
      "loss: 2.286363 [  384/60000]\n",
      "loss: 2.110645 [  385/60000]\n",
      "loss: 2.139467 [  386/60000]\n",
      "loss: 2.187199 [  387/60000]\n",
      "loss: 2.159968 [  388/60000]\n",
      "loss: 2.354277 [  389/60000]\n",
      "loss: 2.253473 [  390/60000]\n",
      "loss: 2.195329 [  391/60000]\n",
      "loss: 2.139240 [  392/60000]\n",
      "loss: 2.275207 [  393/60000]\n",
      "loss: 2.142688 [  394/60000]\n",
      "loss: 2.249242 [  395/60000]\n",
      "loss: 2.283180 [  396/60000]\n",
      "loss: 2.359296 [  397/60000]\n",
      "loss: 2.238872 [  398/60000]\n",
      "loss: 2.243376 [  399/60000]\n",
      "loss: 2.098032 [  400/60000]\n",
      "loss: 2.179976 [  401/60000]\n",
      "loss: 2.302787 [  402/60000]\n",
      "loss: 2.166729 [  403/60000]\n",
      "loss: 2.280280 [  404/60000]\n",
      "loss: 2.202045 [  405/60000]\n",
      "loss: 2.216704 [  406/60000]\n",
      "loss: 2.238587 [  407/60000]\n",
      "loss: 2.274745 [  408/60000]\n",
      "loss: 2.201471 [  409/60000]\n",
      "loss: 2.242052 [  410/60000]\n",
      "loss: 2.277865 [  411/60000]\n",
      "loss: 2.168074 [  412/60000]\n",
      "loss: 2.366382 [  413/60000]\n",
      "loss: 2.091251 [  414/60000]\n",
      "loss: 2.322060 [  415/60000]\n",
      "loss: 2.214494 [  416/60000]\n",
      "loss: 2.184325 [  417/60000]\n",
      "loss: 2.270258 [  418/60000]\n",
      "loss: 2.271756 [  419/60000]\n",
      "loss: 2.214762 [  420/60000]\n",
      "loss: 2.349985 [  421/60000]\n",
      "loss: 2.247583 [  422/60000]\n",
      "loss: 2.226761 [  423/60000]\n",
      "loss: 2.223214 [  424/60000]\n",
      "loss: 2.329752 [  425/60000]\n",
      "loss: 2.208776 [  426/60000]\n",
      "loss: 2.167178 [  427/60000]\n",
      "loss: 2.029292 [  428/60000]\n",
      "loss: 2.202079 [  429/60000]\n",
      "loss: 2.069043 [  430/60000]\n",
      "loss: 2.340268 [  431/60000]\n",
      "loss: 2.244177 [  432/60000]\n",
      "loss: 2.362541 [  433/60000]\n",
      "loss: 2.166119 [  434/60000]\n",
      "loss: 2.161224 [  435/60000]\n",
      "loss: 2.113262 [  436/60000]\n",
      "loss: 2.183833 [  437/60000]\n",
      "loss: 2.324675 [  438/60000]\n",
      "loss: 2.246027 [  439/60000]\n",
      "loss: 2.128617 [  440/60000]\n",
      "loss: 2.161119 [  441/60000]\n",
      "loss: 2.207644 [  442/60000]\n",
      "loss: 2.221168 [  443/60000]\n",
      "loss: 2.274678 [  444/60000]\n",
      "loss: 2.323362 [  445/60000]\n",
      "loss: 2.096773 [  446/60000]\n",
      "loss: 2.219589 [  447/60000]\n",
      "loss: 2.220659 [  448/60000]\n",
      "loss: 2.316041 [  449/60000]\n",
      "loss: 2.255572 [  450/60000]\n",
      "loss: 2.199862 [  451/60000]\n",
      "loss: 2.057761 [  452/60000]\n",
      "loss: 2.310615 [  453/60000]\n",
      "loss: 2.119572 [  454/60000]\n",
      "loss: 2.240515 [  455/60000]\n",
      "loss: 2.181793 [  456/60000]\n",
      "loss: 2.099423 [  457/60000]\n",
      "loss: 2.229564 [  458/60000]\n",
      "loss: 1.988680 [  459/60000]\n",
      "loss: 2.320359 [  460/60000]\n",
      "loss: 2.171083 [  461/60000]\n",
      "loss: 2.194549 [  462/60000]\n",
      "loss: 1.978873 [  463/60000]\n",
      "loss: 2.262751 [  464/60000]\n",
      "loss: 2.002776 [  465/60000]\n",
      "loss: 2.254578 [  466/60000]\n",
      "loss: 2.216640 [  467/60000]\n",
      "loss: 2.144402 [  468/60000]\n",
      "loss: 2.092978 [  469/60000]\n",
      "loss: 2.142481 [  470/60000]\n",
      "loss: 2.275483 [  471/60000]\n",
      "loss: 2.219094 [  472/60000]\n",
      "loss: 2.246966 [  473/60000]\n",
      "loss: 2.050499 [  474/60000]\n",
      "loss: 2.409088 [  475/60000]\n",
      "loss: 2.252724 [  476/60000]\n",
      "loss: 2.273732 [  477/60000]\n",
      "loss: 2.128047 [  478/60000]\n",
      "loss: 2.155632 [  479/60000]\n",
      "loss: 2.233500 [  480/60000]\n",
      "loss: 2.336776 [  481/60000]\n",
      "loss: 2.224641 [  482/60000]\n",
      "loss: 2.309918 [  483/60000]\n",
      "loss: 2.135818 [  484/60000]\n",
      "loss: 2.213138 [  485/60000]\n",
      "loss: 2.205500 [  486/60000]\n",
      "loss: 2.290697 [  487/60000]\n",
      "loss: 2.240785 [  488/60000]\n",
      "loss: 2.451940 [  489/60000]\n",
      "loss: 2.146028 [  490/60000]\n",
      "loss: 2.167649 [  491/60000]\n",
      "loss: 2.332791 [  492/60000]\n",
      "loss: 2.137702 [  493/60000]\n",
      "loss: 2.170041 [  494/60000]\n",
      "loss: 2.433251 [  495/60000]\n",
      "loss: 2.163754 [  496/60000]\n",
      "loss: 2.375619 [  497/60000]\n",
      "loss: 2.145076 [  498/60000]\n",
      "loss: 2.273015 [  499/60000]\n",
      "loss: 2.088991 [  500/60000]\n",
      "loss: 2.429902 [  501/60000]\n",
      "loss: 2.270723 [  502/60000]\n",
      "loss: 2.376895 [  503/60000]\n",
      "loss: 2.186751 [  504/60000]\n",
      "loss: 2.407698 [  505/60000]\n",
      "loss: 2.139894 [  506/60000]\n",
      "loss: 2.270586 [  507/60000]\n",
      "loss: 2.214792 [  508/60000]\n",
      "loss: 2.185970 [  509/60000]\n",
      "loss: 2.325728 [  510/60000]\n",
      "loss: 2.171251 [  511/60000]\n",
      "loss: 2.199786 [  512/60000]\n",
      "loss: 2.276857 [  513/60000]\n",
      "loss: 2.269953 [  514/60000]\n",
      "loss: 2.366863 [  515/60000]\n",
      "loss: 2.138475 [  516/60000]\n",
      "loss: 2.270299 [  517/60000]\n",
      "loss: 2.188719 [  518/60000]\n",
      "loss: 2.172674 [  519/60000]\n",
      "loss: 2.035452 [  520/60000]\n",
      "loss: 2.289850 [  521/60000]\n",
      "loss: 2.186427 [  522/60000]\n",
      "loss: 2.160395 [  523/60000]\n",
      "loss: 2.289738 [  524/60000]\n",
      "loss: 2.065658 [  525/60000]\n",
      "loss: 2.281981 [  526/60000]\n",
      "loss: 2.054935 [  527/60000]\n",
      "loss: 2.286001 [  528/60000]\n",
      "loss: 2.270660 [  529/60000]\n",
      "loss: 2.250794 [  530/60000]\n",
      "loss: 2.153188 [  531/60000]\n",
      "loss: 2.225859 [  532/60000]\n",
      "loss: 2.169269 [  533/60000]\n",
      "loss: 2.306004 [  534/60000]\n",
      "loss: 2.284758 [  535/60000]\n",
      "loss: 2.257337 [  536/60000]\n",
      "loss: 2.280983 [  537/60000]\n",
      "loss: 2.242782 [  538/60000]\n",
      "loss: 2.171784 [  539/60000]\n",
      "loss: 2.122366 [  540/60000]\n",
      "loss: 2.310468 [  541/60000]\n",
      "loss: 2.208020 [  542/60000]\n",
      "loss: 2.255261 [  543/60000]\n",
      "loss: 2.273014 [  544/60000]\n",
      "loss: 2.416719 [  545/60000]\n",
      "loss: 2.073069 [  546/60000]\n",
      "loss: 2.262661 [  547/60000]\n",
      "loss: 2.420404 [  548/60000]\n",
      "loss: 2.234029 [  549/60000]\n",
      "loss: 2.175488 [  550/60000]\n",
      "loss: 2.201576 [  551/60000]\n",
      "loss: 2.140644 [  552/60000]\n",
      "loss: 2.175346 [  553/60000]\n",
      "loss: 2.193119 [  554/60000]\n",
      "loss: 2.323747 [  555/60000]\n",
      "loss: 2.200965 [  556/60000]\n",
      "loss: 2.139860 [  557/60000]\n",
      "loss: 2.153617 [  558/60000]\n",
      "loss: 2.351211 [  559/60000]\n",
      "loss: 2.187507 [  560/60000]\n",
      "loss: 2.237285 [  561/60000]\n",
      "loss: 2.305497 [  562/60000]\n",
      "loss: 2.172723 [  563/60000]\n",
      "loss: 2.080006 [  564/60000]\n",
      "loss: 2.225645 [  565/60000]\n",
      "loss: 2.313884 [  566/60000]\n",
      "loss: 2.256365 [  567/60000]\n",
      "loss: 2.143943 [  568/60000]\n",
      "loss: 2.305058 [  569/60000]\n",
      "loss: 2.223648 [  570/60000]\n",
      "loss: 2.188966 [  571/60000]\n",
      "loss: 2.088272 [  572/60000]\n",
      "loss: 2.166458 [  573/60000]\n",
      "loss: 2.166334 [  574/60000]\n",
      "loss: 2.300794 [  575/60000]\n",
      "loss: 2.114707 [  576/60000]\n",
      "loss: 2.269388 [  577/60000]\n",
      "loss: 1.972889 [  578/60000]\n",
      "loss: 2.285131 [  579/60000]\n",
      "loss: 2.330714 [  580/60000]\n",
      "loss: 2.174781 [  581/60000]\n",
      "loss: 2.207659 [  582/60000]\n",
      "loss: 2.154629 [  583/60000]\n",
      "loss: 2.246765 [  584/60000]\n",
      "loss: 2.270530 [  585/60000]\n",
      "loss: 2.240950 [  586/60000]\n",
      "loss: 2.136321 [  587/60000]\n",
      "loss: 2.211960 [  588/60000]\n",
      "loss: 2.318536 [  589/60000]\n",
      "loss: 2.296262 [  590/60000]\n",
      "loss: 2.221741 [  591/60000]\n",
      "loss: 2.197917 [  592/60000]\n",
      "loss: 2.244857 [  593/60000]\n",
      "loss: 2.197703 [  594/60000]\n",
      "loss: 2.156137 [  595/60000]\n",
      "loss: 2.224605 [  596/60000]\n",
      "loss: 2.043564 [  597/60000]\n",
      "loss: 2.378867 [  598/60000]\n",
      "loss: 2.261618 [  599/60000]\n",
      "loss: 2.202257 [  600/60000]\n",
      "loss: 2.222490 [  601/60000]\n",
      "loss: 2.273873 [  602/60000]\n",
      "loss: 2.242085 [  603/60000]\n",
      "loss: 2.189862 [  604/60000]\n",
      "loss: 2.233008 [  605/60000]\n",
      "loss: 2.199407 [  606/60000]\n",
      "loss: 2.250409 [  607/60000]\n",
      "loss: 2.290988 [  608/60000]\n",
      "loss: 2.261394 [  609/60000]\n",
      "loss: 2.163838 [  610/60000]\n",
      "loss: 2.449622 [  611/60000]\n",
      "loss: 2.244370 [  612/60000]\n",
      "loss: 2.026798 [  613/60000]\n",
      "loss: 2.272274 [  614/60000]\n",
      "loss: 2.302852 [  615/60000]\n",
      "loss: 2.227283 [  616/60000]\n",
      "loss: 2.154238 [  617/60000]\n",
      "loss: 2.177625 [  618/60000]\n",
      "loss: 2.172647 [  619/60000]\n",
      "loss: 2.202641 [  620/60000]\n",
      "loss: 2.296508 [  621/60000]\n",
      "loss: 2.254322 [  622/60000]\n",
      "loss: 2.304014 [  623/60000]\n",
      "loss: 2.455933 [  624/60000]\n",
      "loss: 2.216012 [  625/60000]\n",
      "loss: 2.365575 [  626/60000]\n",
      "loss: 2.299685 [  627/60000]\n",
      "loss: 2.198548 [  628/60000]\n",
      "loss: 2.274117 [  629/60000]\n",
      "loss: 2.204441 [  630/60000]\n",
      "loss: 2.319238 [  631/60000]\n",
      "loss: 2.280113 [  632/60000]\n",
      "loss: 2.213568 [  633/60000]\n",
      "loss: 2.100609 [  634/60000]\n",
      "loss: 2.173027 [  635/60000]\n",
      "loss: 2.259317 [  636/60000]\n",
      "loss: 2.193805 [  637/60000]\n",
      "loss: 2.186013 [  638/60000]\n",
      "loss: 2.170316 [  639/60000]\n",
      "loss: 2.003435 [  640/60000]\n",
      "loss: 2.128043 [  641/60000]\n",
      "loss: 2.254249 [  642/60000]\n",
      "loss: 2.158975 [  643/60000]\n",
      "loss: 2.195824 [  644/60000]\n",
      "loss: 2.195496 [  645/60000]\n",
      "loss: 2.158853 [  646/60000]\n",
      "loss: 2.258888 [  647/60000]\n",
      "loss: 2.273567 [  648/60000]\n",
      "loss: 2.162246 [  649/60000]\n",
      "loss: 2.289833 [  650/60000]\n",
      "loss: 2.271356 [  651/60000]\n",
      "loss: 2.138653 [  652/60000]\n",
      "loss: 2.364172 [  653/60000]\n",
      "loss: 2.326935 [  654/60000]\n",
      "loss: 2.203173 [  655/60000]\n",
      "loss: 2.239655 [  656/60000]\n",
      "loss: 2.113057 [  657/60000]\n",
      "loss: 2.260583 [  658/60000]\n",
      "loss: 2.250526 [  659/60000]\n",
      "loss: 2.267961 [  660/60000]\n",
      "loss: 2.288117 [  661/60000]\n",
      "loss: 2.260425 [  662/60000]\n",
      "loss: 2.098074 [  663/60000]\n",
      "loss: 2.234420 [  664/60000]\n",
      "loss: 2.323520 [  665/60000]\n",
      "loss: 2.426326 [  666/60000]\n",
      "loss: 2.154596 [  667/60000]\n",
      "loss: 2.230527 [  668/60000]\n",
      "loss: 2.072790 [  669/60000]\n",
      "loss: 1.915580 [  670/60000]\n",
      "loss: 2.431053 [  671/60000]\n",
      "loss: 2.167550 [  672/60000]\n",
      "loss: 2.290179 [  673/60000]\n",
      "loss: 2.235893 [  674/60000]\n",
      "loss: 2.305671 [  675/60000]\n",
      "loss: 2.080604 [  676/60000]\n",
      "loss: 2.146518 [  677/60000]\n",
      "loss: 2.256348 [  678/60000]\n",
      "loss: 2.168374 [  679/60000]\n",
      "loss: 2.309665 [  680/60000]\n",
      "loss: 2.186076 [  681/60000]\n",
      "loss: 2.172293 [  682/60000]\n",
      "loss: 2.244588 [  683/60000]\n",
      "loss: 2.137194 [  684/60000]\n",
      "loss: 2.272771 [  685/60000]\n",
      "loss: 2.129374 [  686/60000]\n",
      "loss: 2.129686 [  687/60000]\n",
      "loss: 2.261347 [  688/60000]\n",
      "loss: 2.214652 [  689/60000]\n",
      "loss: 1.905392 [  690/60000]\n",
      "loss: 2.366992 [  691/60000]\n",
      "loss: 2.149022 [  692/60000]\n",
      "loss: 2.236973 [  693/60000]\n",
      "loss: 2.230860 [  694/60000]\n",
      "loss: 2.200319 [  695/60000]\n",
      "loss: 2.137003 [  696/60000]\n",
      "loss: 2.230118 [  697/60000]\n",
      "loss: 2.303243 [  698/60000]\n",
      "loss: 2.111355 [  699/60000]\n",
      "loss: 2.341590 [  700/60000]\n",
      "loss: 2.170341 [  701/60000]\n",
      "loss: 2.163936 [  702/60000]\n",
      "loss: 2.106703 [  703/60000]\n",
      "loss: 2.155972 [  704/60000]\n",
      "loss: 2.457611 [  705/60000]\n",
      "loss: 2.165444 [  706/60000]\n",
      "loss: 2.214090 [  707/60000]\n",
      "loss: 2.260672 [  708/60000]\n",
      "loss: 2.241652 [  709/60000]\n",
      "loss: 1.895353 [  710/60000]\n",
      "loss: 2.145087 [  711/60000]\n",
      "loss: 2.174023 [  712/60000]\n",
      "loss: 2.187874 [  713/60000]\n",
      "loss: 2.133759 [  714/60000]\n",
      "loss: 2.199182 [  715/60000]\n",
      "loss: 2.105212 [  716/60000]\n",
      "loss: 2.192207 [  717/60000]\n",
      "loss: 2.044127 [  718/60000]\n",
      "loss: 2.227148 [  719/60000]\n",
      "loss: 2.352391 [  720/60000]\n",
      "loss: 2.237013 [  721/60000]\n",
      "loss: 2.235541 [  722/60000]\n",
      "loss: 2.325233 [  723/60000]\n",
      "loss: 2.152955 [  724/60000]\n",
      "loss: 2.155690 [  725/60000]\n",
      "loss: 2.113418 [  726/60000]\n",
      "loss: 2.255924 [  727/60000]\n",
      "loss: 2.234245 [  728/60000]\n",
      "loss: 2.200040 [  729/60000]\n",
      "loss: 2.171363 [  730/60000]\n",
      "loss: 2.236336 [  731/60000]\n",
      "loss: 2.127781 [  732/60000]\n",
      "loss: 2.117921 [  733/60000]\n",
      "loss: 1.993739 [  734/60000]\n",
      "loss: 2.205278 [  735/60000]\n",
      "loss: 2.175373 [  736/60000]\n",
      "loss: 2.249553 [  737/60000]\n",
      "loss: 2.274920 [  738/60000]\n",
      "loss: 2.220811 [  739/60000]\n",
      "loss: 2.227491 [  740/60000]\n",
      "loss: 2.187644 [  741/60000]\n",
      "loss: 2.261440 [  742/60000]\n",
      "loss: 2.209797 [  743/60000]\n",
      "loss: 1.913759 [  744/60000]\n",
      "loss: 2.225041 [  745/60000]\n",
      "loss: 2.017256 [  746/60000]\n",
      "loss: 2.291207 [  747/60000]\n",
      "loss: 2.179769 [  748/60000]\n",
      "loss: 2.353052 [  749/60000]\n",
      "loss: 2.326746 [  750/60000]\n",
      "loss: 2.204885 [  751/60000]\n",
      "loss: 2.266675 [  752/60000]\n",
      "loss: 2.135485 [  753/60000]\n",
      "loss: 2.183745 [  754/60000]\n",
      "loss: 2.274709 [  755/60000]\n",
      "loss: 2.222631 [  756/60000]\n",
      "loss: 2.274424 [  757/60000]\n",
      "loss: 2.298441 [  758/60000]\n",
      "loss: 2.113610 [  759/60000]\n",
      "loss: 2.214770 [  760/60000]\n",
      "loss: 2.257167 [  761/60000]\n",
      "loss: 2.076693 [  762/60000]\n",
      "loss: 2.207318 [  763/60000]\n",
      "loss: 2.238563 [  764/60000]\n",
      "loss: 2.318898 [  765/60000]\n",
      "loss: 2.124289 [  766/60000]\n",
      "loss: 2.001698 [  767/60000]\n",
      "loss: 2.153476 [  768/60000]\n",
      "loss: 2.312421 [  769/60000]\n",
      "loss: 2.032695 [  770/60000]\n",
      "loss: 2.249545 [  771/60000]\n",
      "loss: 2.288656 [  772/60000]\n",
      "loss: 2.245784 [  773/60000]\n",
      "loss: 2.196985 [  774/60000]\n",
      "loss: 2.156489 [  775/60000]\n",
      "loss: 2.303864 [  776/60000]\n",
      "loss: 2.161628 [  777/60000]\n",
      "loss: 2.117324 [  778/60000]\n",
      "loss: 2.358797 [  779/60000]\n",
      "loss: 2.156975 [  780/60000]\n",
      "loss: 2.155736 [  781/60000]\n",
      "loss: 1.940763 [  782/60000]\n",
      "loss: 2.248647 [  783/60000]\n",
      "loss: 2.118516 [  784/60000]\n",
      "loss: 2.134890 [  785/60000]\n",
      "loss: 2.174672 [  786/60000]\n",
      "loss: 2.152966 [  787/60000]\n",
      "loss: 1.952222 [  788/60000]\n",
      "loss: 2.185523 [  789/60000]\n",
      "loss: 2.182704 [  790/60000]\n",
      "loss: 2.106620 [  791/60000]\n",
      "loss: 1.995660 [  792/60000]\n",
      "loss: 2.199916 [  793/60000]\n",
      "loss: 2.266504 [  794/60000]\n",
      "loss: 2.302194 [  795/60000]\n",
      "loss: 2.228692 [  796/60000]\n",
      "loss: 2.101833 [  797/60000]\n",
      "loss: 2.076498 [  798/60000]\n",
      "loss: 2.156880 [  799/60000]\n",
      "loss: 2.311425 [  800/60000]\n",
      "loss: 2.185047 [  801/60000]\n",
      "loss: 2.356823 [  802/60000]\n",
      "loss: 2.217582 [  803/60000]\n",
      "loss: 2.206805 [  804/60000]\n",
      "loss: 2.156284 [  805/60000]\n",
      "loss: 2.310285 [  806/60000]\n",
      "loss: 2.204579 [  807/60000]\n",
      "loss: 2.323999 [  808/60000]\n",
      "loss: 2.333881 [  809/60000]\n",
      "loss: 2.133267 [  810/60000]\n",
      "loss: 2.266167 [  811/60000]\n",
      "loss: 2.140311 [  812/60000]\n",
      "loss: 2.099737 [  813/60000]\n",
      "loss: 2.271883 [  814/60000]\n",
      "loss: 2.095452 [  815/60000]\n",
      "loss: 2.364113 [  816/60000]\n",
      "loss: 2.173805 [  817/60000]\n",
      "loss: 2.080630 [  818/60000]\n",
      "loss: 2.092891 [  819/60000]\n",
      "loss: 2.176665 [  820/60000]\n",
      "loss: 2.164373 [  821/60000]\n",
      "loss: 2.173133 [  822/60000]\n",
      "loss: 2.178989 [  823/60000]\n",
      "loss: 2.062971 [  824/60000]\n",
      "loss: 2.163498 [  825/60000]\n",
      "loss: 2.056621 [  826/60000]\n",
      "loss: 2.300649 [  827/60000]\n",
      "loss: 2.149384 [  828/60000]\n",
      "loss: 2.247586 [  829/60000]\n",
      "loss: 2.180014 [  830/60000]\n",
      "loss: 2.168809 [  831/60000]\n",
      "loss: 2.069313 [  832/60000]\n",
      "loss: 2.314301 [  833/60000]\n",
      "loss: 2.115010 [  834/60000]\n",
      "loss: 2.223798 [  835/60000]\n",
      "loss: 2.130084 [  836/60000]\n",
      "loss: 2.379842 [  837/60000]\n",
      "loss: 2.206384 [  838/60000]\n",
      "loss: 2.172311 [  839/60000]\n",
      "loss: 2.178063 [  840/60000]\n",
      "loss: 2.194269 [  841/60000]\n",
      "loss: 2.162602 [  842/60000]\n",
      "loss: 2.235807 [  843/60000]\n",
      "loss: 2.332535 [  844/60000]\n",
      "loss: 2.288576 [  845/60000]\n",
      "loss: 2.178394 [  846/60000]\n",
      "loss: 2.321236 [  847/60000]\n",
      "loss: 1.995811 [  848/60000]\n",
      "loss: 2.120451 [  849/60000]\n",
      "loss: 2.048366 [  850/60000]\n",
      "loss: 2.206027 [  851/60000]\n",
      "loss: 2.070585 [  852/60000]\n",
      "loss: 2.287416 [  853/60000]\n",
      "loss: 2.163082 [  854/60000]\n",
      "loss: 2.391005 [  855/60000]\n",
      "loss: 2.135147 [  856/60000]\n",
      "loss: 2.202028 [  857/60000]\n",
      "loss: 2.174569 [  858/60000]\n",
      "loss: 2.190860 [  859/60000]\n",
      "loss: 1.930503 [  860/60000]\n",
      "loss: 2.042575 [  861/60000]\n",
      "loss: 2.203444 [  862/60000]\n",
      "loss: 2.207130 [  863/60000]\n",
      "loss: 2.345913 [  864/60000]\n",
      "loss: 2.217031 [  865/60000]\n",
      "loss: 2.141569 [  866/60000]\n",
      "loss: 2.214189 [  867/60000]\n",
      "loss: 2.122149 [  868/60000]\n",
      "loss: 2.205415 [  869/60000]\n",
      "loss: 2.133458 [  870/60000]\n",
      "loss: 2.212451 [  871/60000]\n",
      "loss: 2.121018 [  872/60000]\n",
      "loss: 1.975405 [  873/60000]\n",
      "loss: 2.228872 [  874/60000]\n",
      "loss: 1.982925 [  875/60000]\n",
      "loss: 2.244561 [  876/60000]\n",
      "loss: 2.190469 [  877/60000]\n",
      "loss: 2.100243 [  878/60000]\n",
      "loss: 2.268026 [  879/60000]\n",
      "loss: 2.292463 [  880/60000]\n",
      "loss: 2.153320 [  881/60000]\n",
      "loss: 2.222716 [  882/60000]\n",
      "loss: 2.191087 [  883/60000]\n",
      "loss: 2.200534 [  884/60000]\n",
      "loss: 2.209855 [  885/60000]\n",
      "loss: 2.088261 [  886/60000]\n",
      "loss: 2.305680 [  887/60000]\n",
      "loss: 2.246974 [  888/60000]\n",
      "loss: 2.180770 [  889/60000]\n",
      "loss: 2.075488 [  890/60000]\n",
      "loss: 2.048933 [  891/60000]\n",
      "loss: 2.113460 [  892/60000]\n",
      "loss: 2.312676 [  893/60000]\n",
      "loss: 2.297082 [  894/60000]\n",
      "loss: 2.202735 [  895/60000]\n",
      "loss: 2.086385 [  896/60000]\n",
      "loss: 2.128012 [  897/60000]\n",
      "loss: 2.153379 [  898/60000]\n",
      "loss: 2.158745 [  899/60000]\n",
      "loss: 2.252486 [  900/60000]\n",
      "loss: 2.224689 [  901/60000]\n",
      "loss: 2.132513 [  902/60000]\n",
      "loss: 2.348711 [  903/60000]\n",
      "loss: 2.130738 [  904/60000]\n",
      "loss: 2.155262 [  905/60000]\n",
      "loss: 2.178182 [  906/60000]\n",
      "loss: 2.116820 [  907/60000]\n",
      "loss: 2.136687 [  908/60000]\n",
      "loss: 2.130539 [  909/60000]\n",
      "loss: 2.000179 [  910/60000]\n",
      "loss: 2.242722 [  911/60000]\n",
      "loss: 2.095049 [  912/60000]\n",
      "loss: 2.351363 [  913/60000]\n",
      "loss: 2.245013 [  914/60000]\n",
      "loss: 2.098170 [  915/60000]\n",
      "loss: 2.269623 [  916/60000]\n",
      "loss: 2.318414 [  917/60000]\n",
      "loss: 2.218602 [  918/60000]\n",
      "loss: 2.182332 [  919/60000]\n",
      "loss: 2.123902 [  920/60000]\n",
      "loss: 2.108393 [  921/60000]\n",
      "loss: 2.253573 [  922/60000]\n",
      "loss: 2.260662 [  923/60000]\n",
      "loss: 2.105280 [  924/60000]\n",
      "loss: 2.235772 [  925/60000]\n",
      "loss: 2.053258 [  926/60000]\n",
      "loss: 2.088400 [  927/60000]\n",
      "loss: 2.060900 [  928/60000]\n",
      "loss: 2.088595 [  929/60000]\n",
      "loss: 2.242823 [  930/60000]\n",
      "loss: 2.374286 [  931/60000]\n",
      "loss: 2.178014 [  932/60000]\n",
      "loss: 2.187559 [  933/60000]\n",
      "loss: 2.193111 [  934/60000]\n",
      "loss: 2.218171 [  935/60000]\n",
      "loss: 2.264151 [  936/60000]\n",
      "loss: 2.218733 [  937/60000]\n",
      "loss: 2.119972 [  938/60000]\n",
      "loss: 2.245907 [  939/60000]\n",
      "loss: 2.328969 [  940/60000]\n",
      "loss: 2.199248 [  941/60000]\n",
      "loss: 2.176075 [  942/60000]\n",
      "loss: 2.172068 [  943/60000]\n",
      "loss: 2.281666 [  944/60000]\n",
      "loss: 2.259671 [  945/60000]\n",
      "loss: 2.124947 [  946/60000]\n",
      "loss: 2.266443 [  947/60000]\n",
      "loss: 2.304094 [  948/60000]\n",
      "loss: 2.199917 [  949/60000]\n",
      "loss: 1.966151 [  950/60000]\n",
      "loss: 2.185579 [  951/60000]\n",
      "loss: 2.141699 [  952/60000]\n",
      "loss: 2.113995 [  953/60000]\n",
      "loss: 2.109370 [  954/60000]\n",
      "loss: 2.081454 [  955/60000]\n",
      "loss: 2.288604 [  956/60000]\n",
      "loss: 2.250742 [  957/60000]\n",
      "loss: 2.221208 [  958/60000]\n",
      "loss: 2.307719 [  959/60000]\n",
      "loss: 2.113226 [  960/60000]\n",
      "loss: 2.243944 [  961/60000]\n",
      "loss: 2.147371 [  962/60000]\n",
      "loss: 2.104976 [  963/60000]\n",
      "loss: 2.077155 [  964/60000]\n",
      "loss: 2.262698 [  965/60000]\n",
      "loss: 2.100219 [  966/60000]\n",
      "loss: 2.345594 [  967/60000]\n",
      "loss: 2.152615 [  968/60000]\n",
      "loss: 2.133088 [  969/60000]\n",
      "loss: 2.138431 [  970/60000]\n",
      "loss: 2.330214 [  971/60000]\n",
      "loss: 2.075149 [  972/60000]\n",
      "loss: 2.301549 [  973/60000]\n",
      "loss: 2.193049 [  974/60000]\n",
      "loss: 2.381029 [  975/60000]\n",
      "loss: 1.997123 [  976/60000]\n",
      "loss: 2.062864 [  977/60000]\n",
      "loss: 2.233275 [  978/60000]\n",
      "loss: 2.183704 [  979/60000]\n",
      "loss: 2.128752 [  980/60000]\n",
      "loss: 2.278080 [  981/60000]\n",
      "loss: 2.057971 [  982/60000]\n",
      "loss: 2.151215 [  983/60000]\n",
      "loss: 2.105867 [  984/60000]\n",
      "loss: 2.133155 [  985/60000]\n",
      "loss: 2.229726 [  986/60000]\n",
      "loss: 2.230466 [  987/60000]\n",
      "loss: 2.053942 [  988/60000]\n",
      "loss: 2.204499 [  989/60000]\n",
      "loss: 2.120338 [  990/60000]\n",
      "loss: 2.074951 [  991/60000]\n",
      "loss: 2.161743 [  992/60000]\n",
      "loss: 2.180731 [  993/60000]\n",
      "loss: 2.159222 [  994/60000]\n",
      "loss: 2.128683 [  995/60000]\n",
      "loss: 2.144442 [  996/60000]\n",
      "loss: 2.222530 [  997/60000]\n",
      "loss: 2.075492 [  998/60000]\n",
      "loss: 2.185933 [  999/60000]\n",
      "loss: 2.234266 [ 1000/60000]\n",
      "loss: 2.146423 [ 1001/60000]\n",
      "loss: 2.105747 [ 1002/60000]\n",
      "loss: 2.202270 [ 1003/60000]\n",
      "loss: 2.154929 [ 1004/60000]\n",
      "loss: 2.318704 [ 1005/60000]\n",
      "loss: 2.189356 [ 1006/60000]\n",
      "loss: 2.205350 [ 1007/60000]\n",
      "loss: 2.180989 [ 1008/60000]\n",
      "loss: 2.039730 [ 1009/60000]\n",
      "loss: 2.221618 [ 1010/60000]\n",
      "loss: 2.047840 [ 1011/60000]\n",
      "loss: 2.235836 [ 1012/60000]\n",
      "loss: 2.089576 [ 1013/60000]\n",
      "loss: 2.245436 [ 1014/60000]\n",
      "loss: 2.175505 [ 1015/60000]\n",
      "loss: 2.152379 [ 1016/60000]\n",
      "loss: 2.158752 [ 1017/60000]\n",
      "loss: 2.150788 [ 1018/60000]\n",
      "loss: 1.911483 [ 1019/60000]\n",
      "loss: 2.245162 [ 1020/60000]\n",
      "loss: 2.202022 [ 1021/60000]\n",
      "loss: 2.153327 [ 1022/60000]\n",
      "loss: 2.116310 [ 1023/60000]\n",
      "loss: 2.144891 [ 1024/60000]\n",
      "loss: 2.370315 [ 1025/60000]\n",
      "loss: 2.098043 [ 1026/60000]\n",
      "loss: 2.174371 [ 1027/60000]\n",
      "loss: 2.192938 [ 1028/60000]\n",
      "loss: 2.204758 [ 1029/60000]\n",
      "loss: 2.130996 [ 1030/60000]\n",
      "loss: 2.191321 [ 1031/60000]\n",
      "loss: 2.197073 [ 1032/60000]\n",
      "loss: 2.283018 [ 1033/60000]\n",
      "loss: 2.211349 [ 1034/60000]\n",
      "loss: 2.233787 [ 1035/60000]\n",
      "loss: 2.274372 [ 1036/60000]\n",
      "loss: 2.278768 [ 1037/60000]\n",
      "loss: 2.154178 [ 1038/60000]\n",
      "loss: 2.318215 [ 1039/60000]\n",
      "loss: 2.270208 [ 1040/60000]\n",
      "loss: 1.884744 [ 1041/60000]\n",
      "loss: 2.171738 [ 1042/60000]\n",
      "loss: 2.160424 [ 1043/60000]\n",
      "loss: 2.126250 [ 1044/60000]\n",
      "loss: 2.095425 [ 1045/60000]\n",
      "loss: 2.160137 [ 1046/60000]\n",
      "loss: 2.170264 [ 1047/60000]\n",
      "loss: 2.298110 [ 1048/60000]\n",
      "loss: 2.211547 [ 1049/60000]\n",
      "loss: 2.063211 [ 1050/60000]\n",
      "loss: 2.115399 [ 1051/60000]\n",
      "loss: 2.104952 [ 1052/60000]\n",
      "loss: 2.089534 [ 1053/60000]\n",
      "loss: 2.261425 [ 1054/60000]\n",
      "loss: 2.081815 [ 1055/60000]\n",
      "loss: 2.239658 [ 1056/60000]\n",
      "loss: 2.178968 [ 1057/60000]\n",
      "loss: 2.125639 [ 1058/60000]\n",
      "loss: 2.283553 [ 1059/60000]\n",
      "loss: 2.124924 [ 1060/60000]\n",
      "loss: 2.142323 [ 1061/60000]\n",
      "loss: 2.194725 [ 1062/60000]\n",
      "loss: 2.314574 [ 1063/60000]\n",
      "loss: 2.173465 [ 1064/60000]\n",
      "loss: 2.286072 [ 1065/60000]\n",
      "loss: 2.119887 [ 1066/60000]\n",
      "loss: 2.246394 [ 1067/60000]\n",
      "loss: 2.147811 [ 1068/60000]\n",
      "loss: 2.128729 [ 1069/60000]\n",
      "loss: 2.181315 [ 1070/60000]\n",
      "loss: 2.310382 [ 1071/60000]\n",
      "loss: 2.195165 [ 1072/60000]\n",
      "loss: 2.124659 [ 1073/60000]\n",
      "loss: 2.096599 [ 1074/60000]\n",
      "loss: 2.267816 [ 1075/60000]\n",
      "loss: 2.251024 [ 1076/60000]\n",
      "loss: 1.976991 [ 1077/60000]\n",
      "loss: 2.209167 [ 1078/60000]\n",
      "loss: 1.919255 [ 1079/60000]\n",
      "loss: 2.449022 [ 1080/60000]\n",
      "loss: 2.267134 [ 1081/60000]\n",
      "loss: 2.149474 [ 1082/60000]\n",
      "loss: 2.073877 [ 1083/60000]\n",
      "loss: 2.345063 [ 1084/60000]\n",
      "loss: 2.095367 [ 1085/60000]\n",
      "loss: 2.110828 [ 1086/60000]\n",
      "loss: 2.256979 [ 1087/60000]\n",
      "loss: 2.260353 [ 1088/60000]\n",
      "loss: 2.240564 [ 1089/60000]\n",
      "loss: 2.319704 [ 1090/60000]\n",
      "loss: 1.932831 [ 1091/60000]\n",
      "loss: 2.167116 [ 1092/60000]\n",
      "loss: 2.172609 [ 1093/60000]\n",
      "loss: 1.953614 [ 1094/60000]\n",
      "loss: 2.170029 [ 1095/60000]\n",
      "loss: 2.133632 [ 1096/60000]\n",
      "loss: 2.237763 [ 1097/60000]\n",
      "loss: 2.096931 [ 1098/60000]\n",
      "loss: 2.187525 [ 1099/60000]\n",
      "loss: 2.060372 [ 1100/60000]\n",
      "loss: 2.213426 [ 1101/60000]\n",
      "loss: 2.148212 [ 1102/60000]\n",
      "loss: 2.150546 [ 1103/60000]\n",
      "loss: 2.274305 [ 1104/60000]\n",
      "loss: 2.280261 [ 1105/60000]\n",
      "loss: 2.100167 [ 1106/60000]\n",
      "loss: 2.203948 [ 1107/60000]\n",
      "loss: 1.905689 [ 1108/60000]\n",
      "loss: 2.254982 [ 1109/60000]\n",
      "loss: 2.273257 [ 1110/60000]\n",
      "loss: 2.214017 [ 1111/60000]\n",
      "loss: 2.284308 [ 1112/60000]\n",
      "loss: 2.276114 [ 1113/60000]\n",
      "loss: 2.251364 [ 1114/60000]\n",
      "loss: 2.129615 [ 1115/60000]\n",
      "loss: 2.121405 [ 1116/60000]\n",
      "loss: 2.213834 [ 1117/60000]\n",
      "loss: 2.249707 [ 1118/60000]\n",
      "loss: 2.094766 [ 1119/60000]\n",
      "loss: 2.224715 [ 1120/60000]\n",
      "loss: 2.273913 [ 1121/60000]\n",
      "loss: 2.362688 [ 1122/60000]\n",
      "loss: 2.251956 [ 1123/60000]\n",
      "loss: 2.148232 [ 1124/60000]\n",
      "loss: 2.296389 [ 1125/60000]\n",
      "loss: 2.154587 [ 1126/60000]\n",
      "loss: 2.209703 [ 1127/60000]\n",
      "loss: 2.243862 [ 1128/60000]\n",
      "loss: 1.990143 [ 1129/60000]\n",
      "loss: 2.292401 [ 1130/60000]\n",
      "loss: 2.059622 [ 1131/60000]\n",
      "loss: 2.149832 [ 1132/60000]\n",
      "loss: 2.092270 [ 1133/60000]\n",
      "loss: 2.199661 [ 1134/60000]\n",
      "loss: 2.159755 [ 1135/60000]\n",
      "loss: 2.259778 [ 1136/60000]\n",
      "loss: 2.297194 [ 1137/60000]\n",
      "loss: 2.321845 [ 1138/60000]\n",
      "loss: 2.236464 [ 1139/60000]\n",
      "loss: 2.204850 [ 1140/60000]\n",
      "loss: 2.367731 [ 1141/60000]\n",
      "loss: 2.308422 [ 1142/60000]\n",
      "loss: 2.197320 [ 1143/60000]\n",
      "loss: 2.277833 [ 1144/60000]\n",
      "loss: 2.133552 [ 1145/60000]\n",
      "loss: 2.069031 [ 1146/60000]\n",
      "loss: 2.182149 [ 1147/60000]\n",
      "loss: 2.248954 [ 1148/60000]\n",
      "loss: 2.141925 [ 1149/60000]\n",
      "loss: 2.291633 [ 1150/60000]\n",
      "loss: 2.221658 [ 1151/60000]\n",
      "loss: 2.110918 [ 1152/60000]\n",
      "loss: 2.003982 [ 1153/60000]\n",
      "loss: 2.125923 [ 1154/60000]\n",
      "loss: 2.207713 [ 1155/60000]\n",
      "loss: 2.128904 [ 1156/60000]\n",
      "loss: 2.135763 [ 1157/60000]\n",
      "loss: 2.094343 [ 1158/60000]\n",
      "loss: 2.238185 [ 1159/60000]\n",
      "loss: 2.137551 [ 1160/60000]\n",
      "loss: 2.212963 [ 1161/60000]\n",
      "loss: 2.143692 [ 1162/60000]\n",
      "loss: 2.387877 [ 1163/60000]\n",
      "loss: 2.138976 [ 1164/60000]\n",
      "loss: 2.224310 [ 1165/60000]\n",
      "loss: 2.263327 [ 1166/60000]\n",
      "loss: 2.152013 [ 1167/60000]\n",
      "loss: 2.211043 [ 1168/60000]\n",
      "loss: 2.245264 [ 1169/60000]\n",
      "loss: 2.199809 [ 1170/60000]\n",
      "loss: 2.275642 [ 1171/60000]\n",
      "loss: 2.154052 [ 1172/60000]\n",
      "loss: 2.095142 [ 1173/60000]\n",
      "loss: 2.193276 [ 1174/60000]\n",
      "loss: 2.069082 [ 1175/60000]\n",
      "loss: 2.241856 [ 1176/60000]\n",
      "loss: 2.002734 [ 1177/60000]\n",
      "loss: 2.265274 [ 1178/60000]\n",
      "loss: 2.257542 [ 1179/60000]\n",
      "loss: 1.811005 [ 1180/60000]\n",
      "loss: 2.083286 [ 1181/60000]\n",
      "loss: 1.992214 [ 1182/60000]\n",
      "loss: 2.215853 [ 1183/60000]\n",
      "loss: 2.120402 [ 1184/60000]\n",
      "loss: 2.190939 [ 1185/60000]\n",
      "loss: 2.018791 [ 1186/60000]\n",
      "loss: 2.091508 [ 1187/60000]\n",
      "loss: 1.985522 [ 1188/60000]\n",
      "loss: 2.244563 [ 1189/60000]\n",
      "loss: 2.159910 [ 1190/60000]\n",
      "loss: 2.264194 [ 1191/60000]\n",
      "loss: 2.093281 [ 1192/60000]\n",
      "loss: 2.108087 [ 1193/60000]\n",
      "loss: 2.227882 [ 1194/60000]\n",
      "loss: 2.331514 [ 1195/60000]\n",
      "loss: 1.996768 [ 1196/60000]\n",
      "loss: 2.185764 [ 1197/60000]\n",
      "loss: 2.016303 [ 1198/60000]\n",
      "loss: 2.075643 [ 1199/60000]\n",
      "loss: 2.096329 [ 1200/60000]\n",
      "loss: 2.074602 [ 1201/60000]\n",
      "loss: 2.064379 [ 1202/60000]\n",
      "loss: 2.121084 [ 1203/60000]\n",
      "loss: 2.116852 [ 1204/60000]\n",
      "loss: 2.124376 [ 1205/60000]\n",
      "loss: 2.124004 [ 1206/60000]\n",
      "loss: 2.253454 [ 1207/60000]\n",
      "loss: 2.255055 [ 1208/60000]\n",
      "loss: 2.100101 [ 1209/60000]\n",
      "loss: 1.760196 [ 1210/60000]\n",
      "loss: 2.283807 [ 1211/60000]\n",
      "loss: 2.010282 [ 1212/60000]\n",
      "loss: 2.106738 [ 1213/60000]\n",
      "loss: 2.261361 [ 1214/60000]\n",
      "loss: 2.236113 [ 1215/60000]\n",
      "loss: 1.934512 [ 1216/60000]\n",
      "loss: 2.144594 [ 1217/60000]\n",
      "loss: 2.066430 [ 1218/60000]\n",
      "loss: 2.109739 [ 1219/60000]\n",
      "loss: 2.255972 [ 1220/60000]\n",
      "loss: 2.042202 [ 1221/60000]\n",
      "loss: 2.245977 [ 1222/60000]\n",
      "loss: 2.395080 [ 1223/60000]\n",
      "loss: 2.275598 [ 1224/60000]\n",
      "loss: 2.140847 [ 1225/60000]\n",
      "loss: 2.109587 [ 1226/60000]\n",
      "loss: 2.018048 [ 1227/60000]\n",
      "loss: 2.249744 [ 1228/60000]\n",
      "loss: 2.206035 [ 1229/60000]\n",
      "loss: 2.131476 [ 1230/60000]\n",
      "loss: 2.078454 [ 1231/60000]\n",
      "loss: 2.059419 [ 1232/60000]\n",
      "loss: 2.111894 [ 1233/60000]\n",
      "loss: 2.028520 [ 1234/60000]\n",
      "loss: 2.147722 [ 1235/60000]\n",
      "loss: 2.281352 [ 1236/60000]\n",
      "loss: 2.256807 [ 1237/60000]\n",
      "loss: 2.042032 [ 1238/60000]\n",
      "loss: 2.122598 [ 1239/60000]\n",
      "loss: 2.342760 [ 1240/60000]\n",
      "loss: 2.225256 [ 1241/60000]\n",
      "loss: 1.948666 [ 1242/60000]\n",
      "loss: 2.158064 [ 1243/60000]\n",
      "loss: 2.106395 [ 1244/60000]\n",
      "loss: 2.383657 [ 1245/60000]\n",
      "loss: 2.318863 [ 1246/60000]\n",
      "loss: 2.271810 [ 1247/60000]\n",
      "loss: 2.141560 [ 1248/60000]\n",
      "loss: 2.188836 [ 1249/60000]\n",
      "loss: 2.190264 [ 1250/60000]\n",
      "loss: 2.203304 [ 1251/60000]\n",
      "loss: 2.150299 [ 1252/60000]\n",
      "loss: 2.091604 [ 1253/60000]\n",
      "loss: 2.060941 [ 1254/60000]\n",
      "loss: 2.189162 [ 1255/60000]\n",
      "loss: 2.184722 [ 1256/60000]\n",
      "loss: 2.170781 [ 1257/60000]\n",
      "loss: 2.192690 [ 1258/60000]\n",
      "loss: 2.046716 [ 1259/60000]\n",
      "loss: 2.114294 [ 1260/60000]\n",
      "loss: 2.349873 [ 1261/60000]\n",
      "loss: 2.016207 [ 1262/60000]\n",
      "loss: 2.122349 [ 1263/60000]\n",
      "loss: 2.350873 [ 1264/60000]\n",
      "loss: 2.251948 [ 1265/60000]\n",
      "loss: 2.296319 [ 1266/60000]\n",
      "loss: 2.332470 [ 1267/60000]\n",
      "loss: 2.267151 [ 1268/60000]\n",
      "loss: 2.114266 [ 1269/60000]\n",
      "loss: 2.233448 [ 1270/60000]\n",
      "loss: 2.304016 [ 1271/60000]\n",
      "loss: 1.994773 [ 1272/60000]\n",
      "loss: 2.309171 [ 1273/60000]\n",
      "loss: 1.926973 [ 1274/60000]\n",
      "loss: 2.268180 [ 1275/60000]\n",
      "loss: 1.999246 [ 1276/60000]\n",
      "loss: 2.383153 [ 1277/60000]\n",
      "loss: 2.037686 [ 1278/60000]\n",
      "loss: 2.301394 [ 1279/60000]\n",
      "loss: 2.341228 [ 1280/60000]\n",
      "loss: 2.034106 [ 1281/60000]\n",
      "loss: 2.229689 [ 1282/60000]\n",
      "loss: 2.280933 [ 1283/60000]\n",
      "loss: 2.228619 [ 1284/60000]\n",
      "loss: 2.142604 [ 1285/60000]\n",
      "loss: 2.199938 [ 1286/60000]\n",
      "loss: 2.147630 [ 1287/60000]\n",
      "loss: 2.107442 [ 1288/60000]\n",
      "loss: 2.326467 [ 1289/60000]\n",
      "loss: 2.227192 [ 1290/60000]\n",
      "loss: 2.103207 [ 1291/60000]\n",
      "loss: 2.192555 [ 1292/60000]\n",
      "loss: 2.153716 [ 1293/60000]\n",
      "loss: 2.058524 [ 1294/60000]\n",
      "loss: 2.187998 [ 1295/60000]\n",
      "loss: 2.006882 [ 1296/60000]\n",
      "loss: 2.072102 [ 1297/60000]\n",
      "loss: 2.054285 [ 1298/60000]\n",
      "loss: 2.348335 [ 1299/60000]\n",
      "loss: 2.222909 [ 1300/60000]\n",
      "loss: 2.151377 [ 1301/60000]\n",
      "loss: 2.198591 [ 1302/60000]\n",
      "loss: 2.085837 [ 1303/60000]\n",
      "loss: 2.067760 [ 1304/60000]\n",
      "loss: 2.049977 [ 1305/60000]\n",
      "loss: 2.158558 [ 1306/60000]\n",
      "loss: 2.133260 [ 1307/60000]\n",
      "loss: 2.170580 [ 1308/60000]\n",
      "loss: 2.125243 [ 1309/60000]\n",
      "loss: 2.134971 [ 1310/60000]\n",
      "loss: 2.012217 [ 1311/60000]\n",
      "loss: 2.209121 [ 1312/60000]\n",
      "loss: 2.112600 [ 1313/60000]\n",
      "loss: 2.315689 [ 1314/60000]\n",
      "loss: 2.152798 [ 1315/60000]\n",
      "loss: 1.955312 [ 1316/60000]\n",
      "loss: 2.098899 [ 1317/60000]\n",
      "loss: 2.035975 [ 1318/60000]\n",
      "loss: 2.217169 [ 1319/60000]\n",
      "loss: 2.335874 [ 1320/60000]\n",
      "loss: 2.036781 [ 1321/60000]\n",
      "loss: 2.025143 [ 1322/60000]\n",
      "loss: 2.125504 [ 1323/60000]\n",
      "loss: 2.002052 [ 1324/60000]\n",
      "loss: 2.295855 [ 1325/60000]\n",
      "loss: 2.201166 [ 1326/60000]\n",
      "loss: 2.107665 [ 1327/60000]\n",
      "loss: 2.015488 [ 1328/60000]\n",
      "loss: 2.358763 [ 1329/60000]\n",
      "loss: 2.102694 [ 1330/60000]\n",
      "loss: 2.202349 [ 1331/60000]\n",
      "loss: 1.990746 [ 1332/60000]\n",
      "loss: 2.071021 [ 1333/60000]\n",
      "loss: 2.194429 [ 1334/60000]\n",
      "loss: 2.216884 [ 1335/60000]\n",
      "loss: 2.119973 [ 1336/60000]\n",
      "loss: 2.219367 [ 1337/60000]\n",
      "loss: 2.137501 [ 1338/60000]\n",
      "loss: 2.113006 [ 1339/60000]\n",
      "loss: 2.159443 [ 1340/60000]\n",
      "loss: 2.364580 [ 1341/60000]\n",
      "loss: 2.173747 [ 1342/60000]\n",
      "loss: 2.174489 [ 1343/60000]\n",
      "loss: 2.094849 [ 1344/60000]\n",
      "loss: 2.299877 [ 1345/60000]\n",
      "loss: 1.972818 [ 1346/60000]\n",
      "loss: 2.057182 [ 1347/60000]\n",
      "loss: 2.081413 [ 1348/60000]\n",
      "loss: 2.267629 [ 1349/60000]\n",
      "loss: 2.103067 [ 1350/60000]\n",
      "loss: 2.216439 [ 1351/60000]\n",
      "loss: 2.046265 [ 1352/60000]\n",
      "loss: 2.493377 [ 1353/60000]\n",
      "loss: 2.054595 [ 1354/60000]\n",
      "loss: 2.103292 [ 1355/60000]\n",
      "loss: 2.154309 [ 1356/60000]\n",
      "loss: 2.355489 [ 1357/60000]\n",
      "loss: 2.127764 [ 1358/60000]\n",
      "loss: 2.135793 [ 1359/60000]\n",
      "loss: 2.003717 [ 1360/60000]\n",
      "loss: 2.150345 [ 1361/60000]\n",
      "loss: 2.312570 [ 1362/60000]\n",
      "loss: 2.137933 [ 1363/60000]\n",
      "loss: 1.897735 [ 1364/60000]\n",
      "loss: 2.334638 [ 1365/60000]\n",
      "loss: 2.033290 [ 1366/60000]\n",
      "loss: 2.214116 [ 1367/60000]\n",
      "loss: 1.749327 [ 1368/60000]\n",
      "loss: 1.935465 [ 1369/60000]\n",
      "loss: 2.008425 [ 1370/60000]\n",
      "loss: 2.254157 [ 1371/60000]\n",
      "loss: 1.789607 [ 1372/60000]\n",
      "loss: 1.769733 [ 1373/60000]\n",
      "loss: 2.047684 [ 1374/60000]\n",
      "loss: 2.295851 [ 1375/60000]\n",
      "loss: 2.100854 [ 1376/60000]\n",
      "loss: 2.199474 [ 1377/60000]\n",
      "loss: 2.124826 [ 1378/60000]\n",
      "loss: 2.210658 [ 1379/60000]\n",
      "loss: 2.013788 [ 1380/60000]\n",
      "loss: 2.203114 [ 1381/60000]\n",
      "loss: 2.054557 [ 1382/60000]\n",
      "loss: 2.268023 [ 1383/60000]\n",
      "loss: 2.174207 [ 1384/60000]\n",
      "loss: 2.117519 [ 1385/60000]\n",
      "loss: 2.216880 [ 1386/60000]\n",
      "loss: 1.911078 [ 1387/60000]\n",
      "loss: 2.142698 [ 1388/60000]\n",
      "loss: 2.056075 [ 1389/60000]\n",
      "loss: 2.151020 [ 1390/60000]\n",
      "loss: 2.260617 [ 1391/60000]\n",
      "loss: 2.166173 [ 1392/60000]\n",
      "loss: 2.097520 [ 1393/60000]\n",
      "loss: 2.091900 [ 1394/60000]\n",
      "loss: 2.074074 [ 1395/60000]\n",
      "loss: 2.130605 [ 1396/60000]\n",
      "loss: 2.154804 [ 1397/60000]\n",
      "loss: 2.105114 [ 1398/60000]\n",
      "loss: 2.193608 [ 1399/60000]\n",
      "loss: 2.209559 [ 1400/60000]\n",
      "loss: 2.154402 [ 1401/60000]\n",
      "loss: 2.263231 [ 1402/60000]\n",
      "loss: 2.328166 [ 1403/60000]\n",
      "loss: 1.846246 [ 1404/60000]\n",
      "loss: 2.314301 [ 1405/60000]\n",
      "loss: 2.045716 [ 1406/60000]\n",
      "loss: 2.309815 [ 1407/60000]\n",
      "loss: 2.164931 [ 1408/60000]\n",
      "loss: 2.120525 [ 1409/60000]\n",
      "loss: 2.024497 [ 1410/60000]\n",
      "loss: 2.125762 [ 1411/60000]\n",
      "loss: 2.179797 [ 1412/60000]\n",
      "loss: 2.211385 [ 1413/60000]\n",
      "loss: 2.292987 [ 1414/60000]\n",
      "loss: 2.255797 [ 1415/60000]\n",
      "loss: 2.102118 [ 1416/60000]\n",
      "loss: 2.058647 [ 1417/60000]\n",
      "loss: 2.022596 [ 1418/60000]\n",
      "loss: 2.088314 [ 1419/60000]\n",
      "loss: 2.140328 [ 1420/60000]\n",
      "loss: 2.285202 [ 1421/60000]\n",
      "loss: 2.184465 [ 1422/60000]\n",
      "loss: 2.127696 [ 1423/60000]\n",
      "loss: 2.049784 [ 1424/60000]\n",
      "loss: 2.141271 [ 1425/60000]\n",
      "loss: 2.035784 [ 1426/60000]\n",
      "loss: 2.204943 [ 1427/60000]\n",
      "loss: 2.158462 [ 1428/60000]\n",
      "loss: 2.224643 [ 1429/60000]\n",
      "loss: 2.012996 [ 1430/60000]\n",
      "loss: 2.372656 [ 1431/60000]\n",
      "loss: 2.004946 [ 1432/60000]\n",
      "loss: 2.038951 [ 1433/60000]\n",
      "loss: 2.335827 [ 1434/60000]\n",
      "loss: 2.192298 [ 1435/60000]\n",
      "loss: 2.165243 [ 1436/60000]\n",
      "loss: 2.072834 [ 1437/60000]\n",
      "loss: 2.024744 [ 1438/60000]\n",
      "loss: 2.192334 [ 1439/60000]\n",
      "loss: 2.241709 [ 1440/60000]\n",
      "loss: 2.167089 [ 1441/60000]\n",
      "loss: 2.214735 [ 1442/60000]\n",
      "loss: 2.223396 [ 1443/60000]\n",
      "loss: 1.919183 [ 1444/60000]\n",
      "loss: 2.283636 [ 1445/60000]\n",
      "loss: 2.002124 [ 1446/60000]\n",
      "loss: 2.113230 [ 1447/60000]\n",
      "loss: 2.256469 [ 1448/60000]\n",
      "loss: 1.970286 [ 1449/60000]\n",
      "loss: 2.071910 [ 1450/60000]\n",
      "loss: 2.053747 [ 1451/60000]\n",
      "loss: 2.055640 [ 1452/60000]\n",
      "loss: 2.159510 [ 1453/60000]\n",
      "loss: 2.241035 [ 1454/60000]\n",
      "loss: 2.186897 [ 1455/60000]\n",
      "loss: 2.261555 [ 1456/60000]\n",
      "loss: 2.028141 [ 1457/60000]\n",
      "loss: 2.155727 [ 1458/60000]\n",
      "loss: 2.068054 [ 1459/60000]\n",
      "loss: 2.148854 [ 1460/60000]\n",
      "loss: 2.227472 [ 1461/60000]\n",
      "loss: 2.163440 [ 1462/60000]\n",
      "loss: 2.113253 [ 1463/60000]\n",
      "loss: 2.119852 [ 1464/60000]\n",
      "loss: 2.075291 [ 1465/60000]\n",
      "loss: 2.194246 [ 1466/60000]\n",
      "loss: 2.247470 [ 1467/60000]\n",
      "loss: 2.250747 [ 1468/60000]\n",
      "loss: 2.325729 [ 1469/60000]\n",
      "loss: 2.096377 [ 1470/60000]\n",
      "loss: 2.212867 [ 1471/60000]\n",
      "loss: 1.840242 [ 1472/60000]\n",
      "loss: 2.038277 [ 1473/60000]\n",
      "loss: 2.325547 [ 1474/60000]\n",
      "loss: 2.125198 [ 1475/60000]\n",
      "loss: 2.161149 [ 1476/60000]\n",
      "loss: 2.105019 [ 1477/60000]\n",
      "loss: 2.265344 [ 1478/60000]\n",
      "loss: 2.266850 [ 1479/60000]\n",
      "loss: 1.769469 [ 1480/60000]\n",
      "loss: 2.123223 [ 1481/60000]\n",
      "loss: 2.249528 [ 1482/60000]\n",
      "loss: 2.382249 [ 1483/60000]\n",
      "loss: 2.097590 [ 1484/60000]\n",
      "loss: 2.054862 [ 1485/60000]\n",
      "loss: 2.303262 [ 1486/60000]\n",
      "loss: 2.158103 [ 1487/60000]\n",
      "loss: 2.066582 [ 1488/60000]\n",
      "loss: 2.148799 [ 1489/60000]\n",
      "loss: 1.748939 [ 1490/60000]\n",
      "loss: 2.233963 [ 1491/60000]\n",
      "loss: 2.191024 [ 1492/60000]\n",
      "loss: 2.383202 [ 1493/60000]\n",
      "loss: 2.036026 [ 1494/60000]\n",
      "loss: 2.232488 [ 1495/60000]\n",
      "loss: 1.884831 [ 1496/60000]\n",
      "loss: 2.063764 [ 1497/60000]\n",
      "loss: 2.059486 [ 1498/60000]\n",
      "loss: 2.206628 [ 1499/60000]\n",
      "loss: 2.109022 [ 1500/60000]\n",
      "loss: 2.184553 [ 1501/60000]\n",
      "loss: 1.976538 [ 1502/60000]\n",
      "loss: 1.867172 [ 1503/60000]\n",
      "loss: 2.123477 [ 1504/60000]\n",
      "loss: 2.131330 [ 1505/60000]\n",
      "loss: 2.171214 [ 1506/60000]\n",
      "loss: 2.201836 [ 1507/60000]\n",
      "loss: 2.119890 [ 1508/60000]\n",
      "loss: 2.093886 [ 1509/60000]\n",
      "loss: 2.390725 [ 1510/60000]\n",
      "loss: 2.269657 [ 1511/60000]\n",
      "loss: 1.984586 [ 1512/60000]\n",
      "loss: 2.405606 [ 1513/60000]\n",
      "loss: 2.059103 [ 1514/60000]\n",
      "loss: 2.269085 [ 1515/60000]\n",
      "loss: 2.173883 [ 1516/60000]\n",
      "loss: 2.381271 [ 1517/60000]\n",
      "loss: 1.710998 [ 1518/60000]\n",
      "loss: 2.263401 [ 1519/60000]\n",
      "loss: 2.160437 [ 1520/60000]\n",
      "loss: 2.227832 [ 1521/60000]\n",
      "loss: 2.090321 [ 1522/60000]\n",
      "loss: 2.181867 [ 1523/60000]\n",
      "loss: 2.094348 [ 1524/60000]\n",
      "loss: 2.388171 [ 1525/60000]\n",
      "loss: 2.108084 [ 1526/60000]\n",
      "loss: 2.295124 [ 1527/60000]\n",
      "loss: 2.090438 [ 1528/60000]\n",
      "loss: 2.090259 [ 1529/60000]\n",
      "loss: 2.156872 [ 1530/60000]\n",
      "loss: 2.075555 [ 1531/60000]\n",
      "loss: 2.313094 [ 1532/60000]\n",
      "loss: 1.990910 [ 1533/60000]\n",
      "loss: 2.024097 [ 1534/60000]\n",
      "loss: 2.273992 [ 1535/60000]\n",
      "loss: 2.173037 [ 1536/60000]\n",
      "loss: 2.219350 [ 1537/60000]\n",
      "loss: 2.003760 [ 1538/60000]\n",
      "loss: 2.393466 [ 1539/60000]\n",
      "loss: 2.323839 [ 1540/60000]\n",
      "loss: 2.168145 [ 1541/60000]\n",
      "loss: 2.047664 [ 1542/60000]\n",
      "loss: 2.118812 [ 1543/60000]\n",
      "loss: 2.064635 [ 1544/60000]\n",
      "loss: 2.105292 [ 1545/60000]\n",
      "loss: 2.246316 [ 1546/60000]\n",
      "loss: 1.961581 [ 1547/60000]\n",
      "loss: 2.147966 [ 1548/60000]\n",
      "loss: 2.105328 [ 1549/60000]\n",
      "loss: 2.170745 [ 1550/60000]\n",
      "loss: 2.052977 [ 1551/60000]\n",
      "loss: 2.001009 [ 1552/60000]\n",
      "loss: 2.133084 [ 1553/60000]\n",
      "loss: 2.116264 [ 1554/60000]\n",
      "loss: 2.360170 [ 1555/60000]\n",
      "loss: 2.116227 [ 1556/60000]\n",
      "loss: 2.277928 [ 1557/60000]\n",
      "loss: 2.242855 [ 1558/60000]\n",
      "loss: 2.013451 [ 1559/60000]\n",
      "loss: 2.225696 [ 1560/60000]\n",
      "loss: 2.258709 [ 1561/60000]\n",
      "loss: 2.270070 [ 1562/60000]\n",
      "loss: 2.166512 [ 1563/60000]\n",
      "loss: 2.166784 [ 1564/60000]\n",
      "loss: 2.122558 [ 1565/60000]\n",
      "loss: 2.065005 [ 1566/60000]\n",
      "loss: 1.908859 [ 1567/60000]\n",
      "loss: 2.061042 [ 1568/60000]\n",
      "loss: 2.151575 [ 1569/60000]\n",
      "loss: 2.193111 [ 1570/60000]\n",
      "loss: 2.114808 [ 1571/60000]\n",
      "loss: 1.841811 [ 1572/60000]\n",
      "loss: 2.266768 [ 1573/60000]\n",
      "loss: 2.335625 [ 1574/60000]\n",
      "loss: 2.208491 [ 1575/60000]\n",
      "loss: 2.220684 [ 1576/60000]\n",
      "loss: 2.130310 [ 1577/60000]\n",
      "loss: 2.274597 [ 1578/60000]\n",
      "loss: 1.924145 [ 1579/60000]\n",
      "loss: 2.038543 [ 1580/60000]\n",
      "loss: 2.070781 [ 1581/60000]\n",
      "loss: 2.014790 [ 1582/60000]\n",
      "loss: 2.203897 [ 1583/60000]\n",
      "loss: 1.874454 [ 1584/60000]\n",
      "loss: 2.235262 [ 1585/60000]\n",
      "loss: 2.176551 [ 1586/60000]\n",
      "loss: 2.255302 [ 1587/60000]\n",
      "loss: 2.237386 [ 1588/60000]\n",
      "loss: 2.140466 [ 1589/60000]\n",
      "loss: 2.096594 [ 1590/60000]\n",
      "loss: 2.064561 [ 1591/60000]\n",
      "loss: 2.236322 [ 1592/60000]\n",
      "loss: 2.293858 [ 1593/60000]\n",
      "loss: 2.215371 [ 1594/60000]\n",
      "loss: 2.089044 [ 1595/60000]\n",
      "loss: 2.354364 [ 1596/60000]\n",
      "loss: 2.011456 [ 1597/60000]\n",
      "loss: 2.105459 [ 1598/60000]\n",
      "loss: 2.136734 [ 1599/60000]\n",
      "loss: 1.956751 [ 1600/60000]\n",
      "loss: 2.244714 [ 1601/60000]\n",
      "loss: 2.106146 [ 1602/60000]\n",
      "loss: 2.137187 [ 1603/60000]\n",
      "loss: 1.890151 [ 1604/60000]\n",
      "loss: 2.136829 [ 1605/60000]\n",
      "loss: 1.869473 [ 1606/60000]\n",
      "loss: 1.954131 [ 1607/60000]\n",
      "loss: 2.096362 [ 1608/60000]\n",
      "loss: 2.192977 [ 1609/60000]\n",
      "loss: 2.108440 [ 1610/60000]\n",
      "loss: 2.267287 [ 1611/60000]\n",
      "loss: 2.044081 [ 1612/60000]\n",
      "loss: 2.150480 [ 1613/60000]\n",
      "loss: 2.092436 [ 1614/60000]\n",
      "loss: 2.302343 [ 1615/60000]\n",
      "loss: 2.246135 [ 1616/60000]\n",
      "loss: 2.268208 [ 1617/60000]\n",
      "loss: 2.111291 [ 1618/60000]\n",
      "loss: 2.252324 [ 1619/60000]\n",
      "loss: 2.112264 [ 1620/60000]\n",
      "loss: 2.320910 [ 1621/60000]\n",
      "loss: 2.085422 [ 1622/60000]\n",
      "loss: 2.278450 [ 1623/60000]\n",
      "loss: 2.207300 [ 1624/60000]\n",
      "loss: 2.087159 [ 1625/60000]\n",
      "loss: 1.886669 [ 1626/60000]\n",
      "loss: 2.000470 [ 1627/60000]\n",
      "loss: 2.069126 [ 1628/60000]\n",
      "loss: 2.053195 [ 1629/60000]\n",
      "loss: 2.090504 [ 1630/60000]\n",
      "loss: 2.125164 [ 1631/60000]\n",
      "loss: 1.925554 [ 1632/60000]\n",
      "loss: 2.139065 [ 1633/60000]\n",
      "loss: 2.134311 [ 1634/60000]\n",
      "loss: 1.969590 [ 1635/60000]\n",
      "loss: 2.283564 [ 1636/60000]\n",
      "loss: 2.083274 [ 1637/60000]\n",
      "loss: 2.098159 [ 1638/60000]\n",
      "loss: 2.379064 [ 1639/60000]\n",
      "loss: 2.082944 [ 1640/60000]\n",
      "loss: 2.057493 [ 1641/60000]\n",
      "loss: 2.054202 [ 1642/60000]\n",
      "loss: 2.187824 [ 1643/60000]\n",
      "loss: 2.100310 [ 1644/60000]\n",
      "loss: 2.188122 [ 1645/60000]\n",
      "loss: 1.879908 [ 1646/60000]\n",
      "loss: 1.963638 [ 1647/60000]\n",
      "loss: 2.066423 [ 1648/60000]\n",
      "loss: 2.198873 [ 1649/60000]\n",
      "loss: 2.116077 [ 1650/60000]\n",
      "loss: 2.267198 [ 1651/60000]\n",
      "loss: 2.009679 [ 1652/60000]\n",
      "loss: 2.153306 [ 1653/60000]\n",
      "loss: 2.009623 [ 1654/60000]\n",
      "loss: 2.120997 [ 1655/60000]\n",
      "loss: 2.089636 [ 1656/60000]\n",
      "loss: 2.204250 [ 1657/60000]\n",
      "loss: 1.978288 [ 1658/60000]\n",
      "loss: 2.050143 [ 1659/60000]\n",
      "loss: 2.116972 [ 1660/60000]\n",
      "loss: 2.288870 [ 1661/60000]\n",
      "loss: 2.045050 [ 1662/60000]\n",
      "loss: 2.271408 [ 1663/60000]\n",
      "loss: 2.115521 [ 1664/60000]\n",
      "loss: 1.747146 [ 1665/60000]\n",
      "loss: 2.112175 [ 1666/60000]\n",
      "loss: 2.177130 [ 1667/60000]\n",
      "loss: 2.135346 [ 1668/60000]\n",
      "loss: 2.049904 [ 1669/60000]\n",
      "loss: 2.090786 [ 1670/60000]\n",
      "loss: 2.336707 [ 1671/60000]\n",
      "loss: 2.119401 [ 1672/60000]\n",
      "loss: 2.063224 [ 1673/60000]\n",
      "loss: 2.058041 [ 1674/60000]\n",
      "loss: 2.504326 [ 1675/60000]\n",
      "loss: 1.925957 [ 1676/60000]\n",
      "loss: 2.161949 [ 1677/60000]\n",
      "loss: 1.986956 [ 1678/60000]\n",
      "loss: 1.773367 [ 1679/60000]\n",
      "loss: 2.242779 [ 1680/60000]\n",
      "loss: 2.038438 [ 1681/60000]\n",
      "loss: 2.043551 [ 1682/60000]\n",
      "loss: 2.044018 [ 1683/60000]\n",
      "loss: 2.209447 [ 1684/60000]\n",
      "loss: 2.116490 [ 1685/60000]\n",
      "loss: 2.039636 [ 1686/60000]\n",
      "loss: 2.128519 [ 1687/60000]\n",
      "loss: 2.062290 [ 1688/60000]\n",
      "loss: 2.292666 [ 1689/60000]\n",
      "loss: 2.035066 [ 1690/60000]\n",
      "loss: 2.339278 [ 1691/60000]\n",
      "loss: 2.299400 [ 1692/60000]\n",
      "loss: 1.937478 [ 1693/60000]\n",
      "loss: 2.019399 [ 1694/60000]\n",
      "loss: 2.144766 [ 1695/60000]\n",
      "loss: 2.147094 [ 1696/60000]\n",
      "loss: 2.075589 [ 1697/60000]\n",
      "loss: 2.056663 [ 1698/60000]\n",
      "loss: 1.857487 [ 1699/60000]\n",
      "loss: 2.038013 [ 1700/60000]\n",
      "loss: 2.047253 [ 1701/60000]\n",
      "loss: 1.808190 [ 1702/60000]\n",
      "loss: 2.229624 [ 1703/60000]\n",
      "loss: 2.041435 [ 1704/60000]\n",
      "loss: 2.255914 [ 1705/60000]\n",
      "loss: 2.131209 [ 1706/60000]\n",
      "loss: 2.022648 [ 1707/60000]\n",
      "loss: 2.036375 [ 1708/60000]\n",
      "loss: 2.109185 [ 1709/60000]\n",
      "loss: 1.756853 [ 1710/60000]\n",
      "loss: 2.032951 [ 1711/60000]\n",
      "loss: 2.139421 [ 1712/60000]\n",
      "loss: 1.962276 [ 1713/60000]\n",
      "loss: 2.100607 [ 1714/60000]\n",
      "loss: 2.272125 [ 1715/60000]\n",
      "loss: 2.137929 [ 1716/60000]\n",
      "loss: 2.163213 [ 1717/60000]\n",
      "loss: 2.037945 [ 1718/60000]\n",
      "loss: 2.198051 [ 1719/60000]\n",
      "loss: 2.083472 [ 1720/60000]\n",
      "loss: 2.114603 [ 1721/60000]\n",
      "loss: 2.229440 [ 1722/60000]\n",
      "loss: 1.956346 [ 1723/60000]\n",
      "loss: 1.866365 [ 1724/60000]\n",
      "loss: 2.170710 [ 1725/60000]\n",
      "loss: 1.873119 [ 1726/60000]\n",
      "loss: 1.997543 [ 1727/60000]\n",
      "loss: 2.098168 [ 1728/60000]\n",
      "loss: 2.126835 [ 1729/60000]\n",
      "loss: 1.834103 [ 1730/60000]\n",
      "loss: 2.135858 [ 1731/60000]\n",
      "loss: 2.032391 [ 1732/60000]\n",
      "loss: 2.172057 [ 1733/60000]\n",
      "loss: 2.114793 [ 1734/60000]\n",
      "loss: 2.005874 [ 1735/60000]\n",
      "loss: 2.126697 [ 1736/60000]\n",
      "loss: 2.137740 [ 1737/60000]\n",
      "loss: 2.070797 [ 1738/60000]\n",
      "loss: 2.087373 [ 1739/60000]\n",
      "loss: 2.062326 [ 1740/60000]\n",
      "loss: 2.225374 [ 1741/60000]\n",
      "loss: 2.134629 [ 1742/60000]\n",
      "loss: 1.896931 [ 1743/60000]\n",
      "loss: 2.100328 [ 1744/60000]\n",
      "loss: 2.256990 [ 1745/60000]\n",
      "loss: 2.075154 [ 1746/60000]\n",
      "loss: 2.067335 [ 1747/60000]\n",
      "loss: 2.072592 [ 1748/60000]\n",
      "loss: 2.155178 [ 1749/60000]\n",
      "loss: 2.070014 [ 1750/60000]\n",
      "loss: 2.245944 [ 1751/60000]\n",
      "loss: 2.109847 [ 1752/60000]\n",
      "loss: 2.350653 [ 1753/60000]\n",
      "loss: 2.131685 [ 1754/60000]\n",
      "loss: 2.141801 [ 1755/60000]\n",
      "loss: 2.146555 [ 1756/60000]\n",
      "loss: 1.994720 [ 1757/60000]\n",
      "loss: 2.093930 [ 1758/60000]\n",
      "loss: 2.169334 [ 1759/60000]\n",
      "loss: 2.208687 [ 1760/60000]\n",
      "loss: 2.213102 [ 1761/60000]\n",
      "loss: 2.089180 [ 1762/60000]\n",
      "loss: 2.122458 [ 1763/60000]\n",
      "loss: 2.076559 [ 1764/60000]\n",
      "loss: 2.015630 [ 1765/60000]\n",
      "loss: 2.099953 [ 1766/60000]\n",
      "loss: 2.239424 [ 1767/60000]\n",
      "loss: 2.065480 [ 1768/60000]\n",
      "loss: 2.092968 [ 1769/60000]\n",
      "loss: 1.927446 [ 1770/60000]\n",
      "loss: 2.135820 [ 1771/60000]\n",
      "loss: 1.819586 [ 1772/60000]\n",
      "loss: 2.194060 [ 1773/60000]\n",
      "loss: 2.083823 [ 1774/60000]\n",
      "loss: 2.189623 [ 1775/60000]\n",
      "loss: 1.758461 [ 1776/60000]\n",
      "loss: 2.243662 [ 1777/60000]\n",
      "loss: 2.045188 [ 1778/60000]\n",
      "loss: 2.111814 [ 1779/60000]\n",
      "loss: 2.083466 [ 1780/60000]\n",
      "loss: 1.992020 [ 1781/60000]\n",
      "loss: 2.029208 [ 1782/60000]\n",
      "loss: 2.216256 [ 1783/60000]\n",
      "loss: 1.979513 [ 1784/60000]\n",
      "loss: 2.244961 [ 1785/60000]\n",
      "loss: 2.290006 [ 1786/60000]\n",
      "loss: 2.159025 [ 1787/60000]\n",
      "loss: 2.056026 [ 1788/60000]\n",
      "loss: 2.106543 [ 1789/60000]\n",
      "loss: 2.083483 [ 1790/60000]\n",
      "loss: 2.173997 [ 1791/60000]\n",
      "loss: 1.986696 [ 1792/60000]\n",
      "loss: 2.151484 [ 1793/60000]\n",
      "loss: 2.056946 [ 1794/60000]\n",
      "loss: 2.062604 [ 1795/60000]\n",
      "loss: 2.410762 [ 1796/60000]\n",
      "loss: 2.079167 [ 1797/60000]\n",
      "loss: 1.930578 [ 1798/60000]\n",
      "loss: 2.022705 [ 1799/60000]\n",
      "loss: 2.057070 [ 1800/60000]\n",
      "loss: 2.248436 [ 1801/60000]\n",
      "loss: 2.086637 [ 1802/60000]\n",
      "loss: 2.097610 [ 1803/60000]\n",
      "loss: 2.061010 [ 1804/60000]\n",
      "loss: 2.199995 [ 1805/60000]\n",
      "loss: 2.019493 [ 1806/60000]\n",
      "loss: 2.435236 [ 1807/60000]\n",
      "loss: 2.036834 [ 1808/60000]\n",
      "loss: 2.061750 [ 1809/60000]\n",
      "loss: 1.842266 [ 1810/60000]\n",
      "loss: 2.299420 [ 1811/60000]\n",
      "loss: 2.110462 [ 1812/60000]\n",
      "loss: 2.276849 [ 1813/60000]\n",
      "loss: 2.238996 [ 1814/60000]\n",
      "loss: 2.245963 [ 1815/60000]\n",
      "loss: 2.109158 [ 1816/60000]\n",
      "loss: 2.083298 [ 1817/60000]\n",
      "loss: 2.230662 [ 1818/60000]\n",
      "loss: 2.165829 [ 1819/60000]\n",
      "loss: 1.831228 [ 1820/60000]\n",
      "loss: 2.238900 [ 1821/60000]\n",
      "loss: 2.171411 [ 1822/60000]\n",
      "loss: 2.059150 [ 1823/60000]\n",
      "loss: 2.139618 [ 1824/60000]\n",
      "loss: 2.362026 [ 1825/60000]\n",
      "loss: 2.089201 [ 1826/60000]\n",
      "loss: 2.215505 [ 1827/60000]\n",
      "loss: 2.019215 [ 1828/60000]\n",
      "loss: 1.759298 [ 1829/60000]\n",
      "loss: 2.034460 [ 1830/60000]\n",
      "loss: 2.007680 [ 1831/60000]\n",
      "loss: 2.033155 [ 1832/60000]\n",
      "loss: 2.171327 [ 1833/60000]\n",
      "loss: 2.028390 [ 1834/60000]\n",
      "loss: 2.284595 [ 1835/60000]\n",
      "loss: 2.122980 [ 1836/60000]\n",
      "loss: 2.080211 [ 1837/60000]\n",
      "loss: 1.716481 [ 1838/60000]\n",
      "loss: 2.045511 [ 1839/60000]\n",
      "loss: 2.016826 [ 1840/60000]\n",
      "loss: 2.216766 [ 1841/60000]\n",
      "loss: 2.182811 [ 1842/60000]\n",
      "loss: 2.028238 [ 1843/60000]\n",
      "loss: 1.702836 [ 1844/60000]\n",
      "loss: 1.968231 [ 1845/60000]\n",
      "loss: 1.906815 [ 1846/60000]\n",
      "loss: 2.205393 [ 1847/60000]\n",
      "loss: 2.248919 [ 1848/60000]\n",
      "loss: 2.079808 [ 1849/60000]\n",
      "loss: 2.063985 [ 1850/60000]\n",
      "loss: 2.184366 [ 1851/60000]\n",
      "loss: 2.110795 [ 1852/60000]\n",
      "loss: 2.408029 [ 1853/60000]\n",
      "loss: 2.028547 [ 1854/60000]\n",
      "loss: 2.158158 [ 1855/60000]\n",
      "loss: 2.177557 [ 1856/60000]\n",
      "loss: 2.143327 [ 1857/60000]\n",
      "loss: 1.775290 [ 1858/60000]\n",
      "loss: 2.084430 [ 1859/60000]\n",
      "loss: 2.036001 [ 1860/60000]\n",
      "loss: 2.073673 [ 1861/60000]\n",
      "loss: 2.250952 [ 1862/60000]\n",
      "loss: 1.951566 [ 1863/60000]\n",
      "loss: 2.053982 [ 1864/60000]\n",
      "loss: 2.308763 [ 1865/60000]\n",
      "loss: 2.027970 [ 1866/60000]\n",
      "loss: 2.144562 [ 1867/60000]\n",
      "loss: 2.344726 [ 1868/60000]\n",
      "loss: 1.920744 [ 1869/60000]\n",
      "loss: 2.040305 [ 1870/60000]\n",
      "loss: 2.144825 [ 1871/60000]\n",
      "loss: 2.052758 [ 1872/60000]\n",
      "loss: 2.016252 [ 1873/60000]\n",
      "loss: 2.338955 [ 1874/60000]\n",
      "loss: 2.091528 [ 1875/60000]\n",
      "loss: 2.270640 [ 1876/60000]\n",
      "loss: 2.151060 [ 1877/60000]\n",
      "loss: 1.898598 [ 1878/60000]\n",
      "loss: 2.142929 [ 1879/60000]\n",
      "loss: 2.031824 [ 1880/60000]\n",
      "loss: 2.191502 [ 1881/60000]\n",
      "loss: 2.352639 [ 1882/60000]\n",
      "loss: 2.079148 [ 1883/60000]\n",
      "loss: 1.741013 [ 1884/60000]\n",
      "loss: 2.105020 [ 1885/60000]\n",
      "loss: 1.922897 [ 1886/60000]\n",
      "loss: 2.141359 [ 1887/60000]\n",
      "loss: 2.360149 [ 1888/60000]\n",
      "loss: 2.259504 [ 1889/60000]\n",
      "loss: 2.106759 [ 1890/60000]\n",
      "loss: 2.116358 [ 1891/60000]\n",
      "loss: 2.015682 [ 1892/60000]\n",
      "loss: 2.147418 [ 1893/60000]\n",
      "loss: 2.087950 [ 1894/60000]\n",
      "loss: 2.089276 [ 1895/60000]\n",
      "loss: 2.144907 [ 1896/60000]\n",
      "loss: 2.130028 [ 1897/60000]\n",
      "loss: 1.790178 [ 1898/60000]\n",
      "loss: 2.088305 [ 1899/60000]\n",
      "loss: 2.132043 [ 1900/60000]\n",
      "loss: 2.200553 [ 1901/60000]\n",
      "loss: 2.143260 [ 1902/60000]\n",
      "loss: 2.113129 [ 1903/60000]\n",
      "loss: 2.124174 [ 1904/60000]\n",
      "loss: 2.222983 [ 1905/60000]\n",
      "loss: 2.183352 [ 1906/60000]\n",
      "loss: 2.194132 [ 1907/60000]\n",
      "loss: 1.762851 [ 1908/60000]\n",
      "loss: 2.249422 [ 1909/60000]\n",
      "loss: 1.985730 [ 1910/60000]\n",
      "loss: 2.013808 [ 1911/60000]\n",
      "loss: 1.977708 [ 1912/60000]\n",
      "loss: 2.182854 [ 1913/60000]\n",
      "loss: 2.419363 [ 1914/60000]\n",
      "loss: 2.218573 [ 1915/60000]\n",
      "loss: 1.978966 [ 1916/60000]\n",
      "loss: 2.133482 [ 1917/60000]\n",
      "loss: 2.079678 [ 1918/60000]\n",
      "loss: 2.152606 [ 1919/60000]\n",
      "loss: 2.095097 [ 1920/60000]\n",
      "loss: 2.280013 [ 1921/60000]\n",
      "loss: 1.918830 [ 1922/60000]\n",
      "loss: 2.095742 [ 1923/60000]\n",
      "loss: 2.121603 [ 1924/60000]\n",
      "loss: 1.953575 [ 1925/60000]\n",
      "loss: 2.140999 [ 1926/60000]\n",
      "loss: 1.726948 [ 1927/60000]\n",
      "loss: 1.775595 [ 1928/60000]\n",
      "loss: 2.340991 [ 1929/60000]\n",
      "loss: 2.356886 [ 1930/60000]\n",
      "loss: 1.887446 [ 1931/60000]\n",
      "loss: 2.189466 [ 1932/60000]\n",
      "loss: 1.985685 [ 1933/60000]\n",
      "loss: 2.007482 [ 1934/60000]\n",
      "loss: 2.081769 [ 1935/60000]\n",
      "loss: 2.118676 [ 1936/60000]\n",
      "loss: 2.059780 [ 1937/60000]\n",
      "loss: 2.162323 [ 1938/60000]\n",
      "loss: 2.142776 [ 1939/60000]\n",
      "loss: 2.135876 [ 1940/60000]\n",
      "loss: 2.274530 [ 1941/60000]\n",
      "loss: 1.975460 [ 1942/60000]\n",
      "loss: 2.022667 [ 1943/60000]\n",
      "loss: 2.096813 [ 1944/60000]\n",
      "loss: 2.115176 [ 1945/60000]\n",
      "loss: 2.003106 [ 1946/60000]\n",
      "loss: 2.128648 [ 1947/60000]\n",
      "loss: 2.119651 [ 1948/60000]\n",
      "loss: 2.393225 [ 1949/60000]\n",
      "loss: 2.099862 [ 1950/60000]\n",
      "loss: 2.102765 [ 1951/60000]\n",
      "loss: 2.042796 [ 1952/60000]\n",
      "loss: 2.194695 [ 1953/60000]\n",
      "loss: 2.020403 [ 1954/60000]\n",
      "loss: 2.289216 [ 1955/60000]\n",
      "loss: 2.339575 [ 1956/60000]\n",
      "loss: 1.821931 [ 1957/60000]\n",
      "loss: 2.052849 [ 1958/60000]\n",
      "loss: 2.076293 [ 1959/60000]\n",
      "loss: 2.054133 [ 1960/60000]\n",
      "loss: 2.060063 [ 1961/60000]\n",
      "loss: 2.334946 [ 1962/60000]\n",
      "loss: 2.133042 [ 1963/60000]\n",
      "loss: 1.685590 [ 1964/60000]\n",
      "loss: 2.324971 [ 1965/60000]\n",
      "loss: 2.265013 [ 1966/60000]\n",
      "loss: 2.231040 [ 1967/60000]\n",
      "loss: 1.933268 [ 1968/60000]\n",
      "loss: 2.212641 [ 1969/60000]\n",
      "loss: 1.787495 [ 1970/60000]\n",
      "loss: 2.286686 [ 1971/60000]\n",
      "loss: 1.866870 [ 1972/60000]\n",
      "loss: 2.246794 [ 1973/60000]\n",
      "loss: 2.083729 [ 1974/60000]\n",
      "loss: 2.309212 [ 1975/60000]\n",
      "loss: 2.135951 [ 1976/60000]\n",
      "loss: 2.090696 [ 1977/60000]\n",
      "loss: 2.022951 [ 1978/60000]\n",
      "loss: 2.360152 [ 1979/60000]\n",
      "loss: 1.852557 [ 1980/60000]\n",
      "loss: 2.106802 [ 1981/60000]\n",
      "loss: 2.195243 [ 1982/60000]\n",
      "loss: 2.182949 [ 1983/60000]\n",
      "loss: 1.983357 [ 1984/60000]\n",
      "loss: 2.147032 [ 1985/60000]\n",
      "loss: 2.085078 [ 1986/60000]\n",
      "loss: 1.933213 [ 1987/60000]\n",
      "loss: 2.203548 [ 1988/60000]\n",
      "loss: 2.296216 [ 1989/60000]\n",
      "loss: 2.015131 [ 1990/60000]\n",
      "loss: 2.122969 [ 1991/60000]\n",
      "loss: 2.131614 [ 1992/60000]\n",
      "loss: 2.203852 [ 1993/60000]\n",
      "loss: 2.152499 [ 1994/60000]\n",
      "loss: 2.200681 [ 1995/60000]\n",
      "loss: 1.696239 [ 1996/60000]\n",
      "loss: 2.290345 [ 1997/60000]\n",
      "loss: 2.246106 [ 1998/60000]\n",
      "loss: 2.263835 [ 1999/60000]\n",
      "loss: 1.728926 [ 2000/60000]\n",
      "loss: 2.361667 [ 2001/60000]\n",
      "loss: 2.084547 [ 2002/60000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "id": "65aa6140-7727-47d8-82bf-86c7dbb78afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 2.107958 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6720)"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "07eeddf3-477d-45e6-8350-741c2dfbd219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0086,  0.0015,  0.0017, -0.0255, -0.0140, -0.0105, -0.0166,  0.0336,\n",
       "         0.0040,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320,  0.0282,  0.0313,\n",
       "         0.0468,  0.0109])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "b1c77e31-daa3-4108-8307-56ff38fce7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x,y) = train_dataloader.dataset[500]\n",
    "x,y = x.flatten(), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "3d0908c7-e944-4760-889d-d4bb27890283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3171)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = forward(x)\n",
    "cross_entropy(a[-1],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb53441-be1d-484e-9861-3161779e2d14",
   "metadata": {},
   "source": [
    "## Claude code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "id": "146a1c94-766d-4283-94dc-cd1e25ccb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradients(f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical gradients of a function f at a point x.\n",
    "    \"\"\"\n",
    "    x = x.detach().requires_grad_(True)\n",
    "    grads = []\n",
    "    for i in range(x.nelement()):\n",
    "        ori_x = x.clone()\n",
    "        ori_x.flatten()[i] += eps\n",
    "        y_pos = f(ori_x)\n",
    "        ori_x.flatten()[i] -= 2 * eps\n",
    "        y_neg = f(ori_x)\n",
    "        grads.append((y_pos - y_neg) / (2 * eps))\n",
    "    return torch.stack(grads).view_as(x)\n",
    "\n",
    "def compare_gradients(f, x, eps=1e-3):\n",
    "    \"\"\"\n",
    "    Compare analytical and numerical gradients of a function f at a point x.\n",
    "    \"\"\"\n",
    "    x = x.detach().requires_grad_(True)\n",
    "    analytical_grad = torch.autograd.grad(f(x), x, create_graph=True)[0]\n",
    "    numerical_grad = numerical_gradients(f, x, eps)\n",
    "    diff = torch.abs(analytical_grad - numerical_grad)\n",
    "    # return analytical_grad,numerical_grad\n",
    "    return torch.allclose(analytical_grad, numerical_grad, atol=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "id": "53a4e29a-4da7-42fb-bbf1-c3150428df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, y):\n",
    "    y_pred = softmax(forward(x)[-1])\n",
    "    return cross_entropy(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "a1177b34-1b31-4772-bc23-ba5d78359b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check before backward pass: True\n",
      "Gradient check after backward pass: True\n"
     ]
    }
   ],
   "source": [
    "iter_ = iter(train_dl)\n",
    "state_dict = torch.load('model.pth')\n",
    "state_dict.keys()\n",
    "\n",
    "weights = []\n",
    "weights.append(state_dict['linear_relu_stack.0.weight'].clone())\n",
    "weights.append(state_dict['linear_relu_stack.2.weight'].clone())\n",
    "\n",
    "bias = []\n",
    "bias.append(state_dict['linear_relu_stack.0.bias'].clone())\n",
    "bias.append(state_dict['linear_relu_stack.2.bias'].clone())\n",
    "\n",
    "x1, y1 = next(iter_)\n",
    "x1, y1 = x1.flatten(), y1.squeeze()\n",
    "\n",
    "# Check gradients before the backward pass\n",
    "# a1,n1 = compare_gradients(lambda x: compute_loss(x, y1), x1)\n",
    "print(\"Gradient check before backward pass:\", compare_gradients(lambda x: compute_loss(x, y1), x1))\n",
    "\n",
    "a = forward(x1)\n",
    "loss = cross_entropy(softmax(a[-1]), y1)\n",
    "backward(a, y1)\n",
    "\n",
    "# Check gradients after the backward pass\n",
    "# a2,n2 = compare_gradients(lambda x: compute_loss(x, y1), x1)\n",
    "print(\"Gradient check after backward pass:\", compare_gradients(lambda x: compute_loss(x, y1), x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "9c8466e5-176e-4cf5-86d8-9c052f8cfeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1448, -0.0842, -0.0713, -0.1799, -0.0463, -0.1540,  0.0068,  0.2833,\n",
       "         0.2197,  0.0625])"
      ]
     },
     "execution_count": 1324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "8d66e5fd-d49d-4210-82ea-4146ff55c923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4099e-03, -5.8973e-03, -7.6170e-03,  9.3721e-03, -1.3587e-02,\n",
       "         1.8532e-02,  1.4251e-02,  3.1801e-04,  3.0352e-03, -3.2623e-03,\n",
       "        -1.5701e-04,  3.3171e-03,  1.7362e-03, -1.6741e-02, -1.0981e-02,\n",
       "         8.7031e-03, -1.7773e-03, -1.4019e-02,  4.1619e-03, -1.4817e-02,\n",
       "         1.1686e-02,  2.3578e-02,  5.8366e-03, -2.1297e-02,  1.2433e-02,\n",
       "         6.0295e-03, -3.1840e-03,  1.0132e-02,  4.2972e-03,  1.6210e-03,\n",
       "         1.0653e-02,  7.5712e-03, -5.2470e-03,  5.2288e-03, -1.8128e-03,\n",
       "         1.1832e-02,  3.6505e-04, -7.4102e-03,  4.9205e-03, -2.8104e-03,\n",
       "         2.3235e-03,  5.7258e-03,  5.6098e-03, -2.7119e-02,  3.5684e-03,\n",
       "        -4.9104e-03, -2.0233e-02, -1.5040e-02,  1.9762e-03,  1.2869e-02,\n",
       "        -1.3593e-02,  2.6663e-03,  8.3546e-04,  1.1420e-04, -2.2234e-03,\n",
       "        -7.7329e-03,  1.3043e-02,  1.0374e-03, -3.6343e-03,  1.6006e-02,\n",
       "        -4.2107e-03,  6.9430e-03,  3.9188e-03, -1.1052e-02, -1.0793e-02,\n",
       "        -2.7822e-03,  6.6480e-03, -1.3865e-02, -6.4596e-03, -4.3614e-03,\n",
       "         7.9239e-03, -1.7550e-02,  8.4070e-03,  1.7274e-03,  2.3647e-04,\n",
       "         9.5655e-03,  7.8308e-03,  3.3702e-03, -1.5272e-02,  1.1356e-02,\n",
       "         9.2493e-04, -1.6476e-02, -3.4568e-03,  4.0190e-03,  6.6195e-03,\n",
       "         1.8404e-03,  2.3134e-03, -2.6442e-03, -8.5248e-03, -8.0906e-03,\n",
       "        -1.2538e-02, -5.4577e-03, -1.3421e-03,  6.5981e-03,  7.6074e-03,\n",
       "        -4.5457e-03, -1.8668e-03, -1.7189e-02, -8.5475e-03,  2.5048e-03,\n",
       "        -1.0515e-03, -3.4921e-03,  5.5269e-03,  8.9754e-03, -2.3449e-03,\n",
       "        -6.3000e-03,  1.8980e-03,  1.6653e-02, -6.6904e-03, -5.8078e-03,\n",
       "        -3.4411e-04, -1.0169e-03, -1.2014e-02,  9.5344e-03,  1.4412e-02,\n",
       "        -3.7068e-03, -2.3374e-03, -1.2199e-03, -1.1700e-04, -5.9668e-03,\n",
       "         1.1483e-04, -7.1202e-03, -2.7524e-03,  4.5145e-03,  4.5223e-03,\n",
       "         2.9892e-03, -6.6641e-03,  1.5674e-03,  2.4297e-04,  2.2726e-03,\n",
       "        -1.5749e-03, -2.9594e-04, -4.5969e-03, -1.0677e-03, -2.4302e-03,\n",
       "         1.1766e-02,  2.9497e-03, -5.6257e-03, -1.7224e-02, -2.3138e-02,\n",
       "         1.1275e-02, -3.0647e-03, -1.0379e-02, -1.1667e-02, -7.0770e-03,\n",
       "         9.9773e-03, -4.6209e-03, -4.5869e-04, -3.5971e-03,  3.9822e-03,\n",
       "        -5.7489e-03,  1.0972e-02,  2.1936e-04, -3.8459e-04,  6.6689e-03,\n",
       "         1.5479e-02,  1.0327e-02, -2.6382e-03,  3.2197e-03, -6.8177e-03,\n",
       "         3.0987e-03,  1.3005e-02, -2.7875e-02,  3.3079e-03,  8.7369e-04,\n",
       "        -7.1141e-03, -1.3656e-02, -2.9257e-03,  2.4826e-04, -4.9001e-03,\n",
       "        -4.8084e-03,  2.8515e-03,  1.1481e-02,  9.2251e-03, -8.1557e-04,\n",
       "        -1.3310e-02, -4.6017e-03,  1.9773e-02,  6.0751e-03, -1.1226e-02,\n",
       "         3.2344e-04,  8.0715e-03,  4.6232e-03, -9.9197e-03, -3.3058e-03,\n",
       "        -6.0332e-03,  1.3313e-02,  1.4637e-02,  1.2196e-02,  4.7105e-03,\n",
       "         1.6894e-02, -1.4839e-02,  6.4173e-03, -8.8699e-03, -1.2259e-03,\n",
       "         1.0395e-02, -6.1168e-03, -1.3281e-02, -4.9896e-03, -1.7774e-03,\n",
       "         2.8195e-03, -8.3501e-03, -3.9648e-03, -7.2498e-03,  1.0444e-02,\n",
       "         5.2402e-03, -7.5102e-04, -1.3946e-02, -1.9806e-02, -2.3433e-03,\n",
       "         1.0891e-02,  1.7984e-04,  1.9758e-03,  3.3495e-03,  1.8413e-04,\n",
       "         5.0109e-03,  5.0441e-03, -1.6611e-03,  5.2713e-03,  1.8397e-02,\n",
       "        -6.5242e-03,  9.3545e-03,  5.1630e-03, -7.2108e-03, -1.8343e-02,\n",
       "        -1.7217e-02, -8.1912e-03,  1.2153e-03, -3.4784e-04, -1.0236e-02,\n",
       "        -1.8553e-03, -1.5326e-02,  4.8288e-03,  1.1620e-02,  6.3111e-03,\n",
       "         4.1687e-03,  7.4551e-04, -2.3931e-03, -1.1667e-02,  8.7616e-03,\n",
       "        -1.5229e-02, -4.5863e-03,  7.6415e-03,  5.5382e-03,  4.8949e-03,\n",
       "         1.2130e-02,  6.0513e-03, -6.9740e-04, -1.3843e-02, -2.0388e-03,\n",
       "        -8.8167e-05,  1.6524e-03, -7.2370e-03,  8.0734e-03,  1.3498e-03,\n",
       "         4.7087e-03, -4.5088e-03,  9.4298e-04,  2.4713e-03, -1.4918e-03,\n",
       "        -7.9033e-03, -3.0955e-03,  2.7307e-04,  6.1059e-03, -4.6420e-03,\n",
       "        -1.4155e-02, -1.9085e-02, -3.2429e-04,  1.2520e-02,  5.4182e-04,\n",
       "        -1.8567e-03,  4.5324e-03,  1.4740e-03,  4.3761e-05,  9.8130e-03,\n",
       "        -8.0744e-04, -4.0639e-03,  1.7967e-02, -1.6276e-02,  2.1985e-03,\n",
       "        -3.6617e-03, -2.4022e-02, -8.2151e-03, -1.3133e-02, -5.6604e-03,\n",
       "        -9.6414e-04,  1.6027e-02, -7.3653e-03,  1.0229e-02,  6.1541e-03,\n",
       "         2.0389e-03, -6.2851e-03, -5.1559e-03,  1.1564e-02, -7.8008e-03,\n",
       "         1.0226e-03, -7.3790e-03,  2.0597e-02, -1.2173e-02,  1.4342e-02,\n",
       "         8.9762e-03, -8.0176e-03, -8.1114e-03, -6.0139e-03,  8.9137e-03,\n",
       "        -8.7085e-03,  4.6991e-04,  9.0568e-03, -1.0402e-03,  9.5042e-03,\n",
       "         1.0163e-03, -1.8754e-02, -1.8111e-03, -9.1153e-03,  5.2916e-03,\n",
       "        -3.6385e-03,  2.5126e-03,  7.7073e-03,  2.1627e-02,  5.2083e-03,\n",
       "        -1.2255e-02, -3.4397e-03,  1.5714e-03, -1.2761e-02,  3.0959e-03,\n",
       "        -1.0912e-02, -4.1552e-03, -1.8927e-03,  2.7701e-04, -1.1190e-02,\n",
       "        -1.8567e-02, -1.8549e-03, -5.4450e-03, -1.3004e-02, -1.4727e-02,\n",
       "        -5.9650e-03,  2.0184e-03, -1.1849e-03, -7.6520e-03,  8.7759e-04,\n",
       "        -2.1067e-03, -7.4472e-03, -7.2434e-04,  2.6528e-03, -9.0449e-03,\n",
       "        -1.3383e-02,  2.0514e-03, -1.2844e-02,  8.7124e-05, -1.5537e-02,\n",
       "        -1.1127e-02, -5.3911e-03, -4.3707e-04,  6.9538e-03,  1.5854e-03,\n",
       "        -1.2558e-02, -5.1060e-03, -1.4002e-02,  3.8105e-04, -5.0221e-03,\n",
       "        -1.2166e-03,  2.7381e-03, -1.7397e-03,  7.3918e-04,  5.1693e-03,\n",
       "        -1.8123e-02, -2.9255e-04,  3.2706e-03, -1.3848e-02, -1.1469e-03,\n",
       "        -5.1215e-03,  8.4054e-03,  1.5089e-02,  2.3299e-04, -4.6183e-03,\n",
       "        -1.0062e-02, -6.5577e-03, -1.8872e-02,  5.3024e-03,  6.6901e-03,\n",
       "        -3.6001e-03,  5.5449e-03,  7.6214e-03,  1.9560e-02,  3.5908e-04,\n",
       "        -1.2380e-02,  3.5623e-03,  2.0561e-03,  5.0804e-03,  7.0595e-03,\n",
       "        -8.1178e-04, -3.5618e-03, -1.0151e-02, -4.2405e-03,  4.1027e-03,\n",
       "        -1.3565e-02, -6.0112e-03,  5.8766e-03,  2.3024e-03, -5.5064e-03,\n",
       "        -1.3278e-02, -3.5608e-03, -1.3636e-02,  1.9163e-04,  1.0642e-02,\n",
       "         8.4645e-03,  1.8459e-03, -6.2956e-03, -1.5858e-02,  1.0532e-02,\n",
       "         1.3651e-03,  5.8069e-03, -4.3518e-03, -1.1449e-03,  6.0804e-03,\n",
       "         7.4888e-03,  3.7276e-04, -3.7614e-03,  8.1770e-03,  2.8113e-04,\n",
       "         7.2803e-03, -1.3594e-02,  1.4523e-03,  8.8777e-03,  5.5844e-03,\n",
       "        -7.5109e-03,  8.2332e-04,  1.1009e-02,  1.7314e-03, -6.5563e-03,\n",
       "         1.1770e-03, -2.5648e-02, -1.4050e-02, -4.9626e-03, -6.6830e-03,\n",
       "         8.5499e-03,  1.1658e-02,  1.5588e-03, -2.9875e-03,  2.7139e-03,\n",
       "        -2.3051e-03,  3.1574e-03, -5.0225e-03,  4.9964e-03,  3.2355e-03,\n",
       "        -1.6583e-03, -1.7263e-03,  2.7383e-03, -5.1868e-03, -6.9864e-03,\n",
       "         9.7265e-03, -3.7918e-04,  1.1752e-02, -1.4173e-03,  6.8879e-03,\n",
       "         8.4187e-03,  6.7893e-03,  8.3180e-04,  7.9386e-04,  1.6547e-02,\n",
       "        -8.7560e-03,  1.6030e-03, -1.6827e-02,  3.2125e-03, -6.4778e-03,\n",
       "        -4.4987e-03, -1.5815e-02,  9.1193e-03, -6.2894e-03,  4.1059e-03,\n",
       "         1.5121e-03,  5.9670e-03, -8.6802e-03, -7.8893e-03,  1.3727e-03,\n",
       "        -4.5477e-04,  9.9684e-03,  5.2702e-03,  4.9441e-03, -1.0778e-02,\n",
       "        -2.2038e-03, -7.5814e-03,  1.3740e-02,  1.3796e-02,  5.2578e-03,\n",
       "        -1.1510e-02, -4.6208e-03,  5.9539e-04,  1.6319e-03, -5.1588e-03,\n",
       "        -5.4758e-03, -7.0699e-03,  6.6627e-03,  1.6765e-04,  4.7632e-03,\n",
       "        -1.7272e-03,  6.3051e-03, -1.1557e-02,  3.2794e-03, -6.7268e-04,\n",
       "        -5.9919e-03, -7.3828e-03,  1.5227e-02, -4.6141e-03, -1.1370e-02,\n",
       "        -1.3739e-02,  2.0686e-03,  1.6610e-02,  7.0146e-03,  1.7413e-03,\n",
       "        -5.2371e-03,  1.5631e-03,  4.8202e-03,  1.0403e-02,  2.9125e-03,\n",
       "        -9.0912e-03,  3.2042e-04,  3.7332e-03, -1.0017e-02, -4.4182e-03,\n",
       "         2.7649e-02,  5.1258e-03, -2.3627e-04,  4.3986e-03,  2.5446e-04,\n",
       "        -1.8107e-04,  1.8142e-02,  1.3015e-02,  1.0839e-02,  9.5545e-03,\n",
       "         2.5547e-03,  4.3382e-03, -5.6168e-03,  3.7425e-03,  8.9823e-03,\n",
       "         1.4866e-02, -1.0268e-02, -1.5245e-02, -1.7416e-02, -1.7034e-03,\n",
       "         8.4295e-03,  9.5766e-03, -1.5505e-02,  6.2981e-03,  7.6794e-03,\n",
       "        -8.6596e-03, -1.6815e-02, -2.1939e-03,  4.8445e-03,  1.8321e-04,\n",
       "        -3.2825e-03,  6.2843e-03,  1.6510e-03,  1.5077e-02, -1.8540e-04,\n",
       "        -7.4759e-03,  5.3255e-03, -1.4583e-02, -6.8708e-03,  8.8637e-03,\n",
       "        -9.9750e-03, -1.1274e-03,  8.8297e-03, -1.1857e-02, -5.3389e-04,\n",
       "         3.0451e-03, -5.5876e-03,  1.4719e-03,  4.0652e-03, -1.1362e-02,\n",
       "        -7.7890e-06, -4.2905e-03, -6.5442e-03,  1.4229e-02, -6.5763e-03,\n",
       "        -3.5385e-04,  1.5765e-03,  5.3690e-03, -8.2157e-03, -6.6164e-03,\n",
       "         2.6804e-03, -7.1458e-04, -1.6770e-02, -1.7920e-02,  7.7999e-03,\n",
       "         1.8601e-02, -2.9108e-03, -6.7457e-03, -3.7183e-03, -1.9293e-02,\n",
       "        -9.8036e-03,  7.5849e-03, -7.0472e-03,  2.4486e-03,  1.3608e-03,\n",
       "         1.0098e-02, -3.1317e-03, -1.6971e-03,  8.9465e-04, -2.1870e-03,\n",
       "         9.5605e-03,  1.7231e-02,  3.3754e-03, -5.6542e-03,  7.2483e-03,\n",
       "         1.1144e-02, -6.1963e-03, -4.6786e-04,  4.3460e-03, -2.2002e-03,\n",
       "         3.4251e-03, -6.7473e-03,  1.4087e-02, -1.8993e-02,  1.7786e-02,\n",
       "        -9.8379e-03, -5.4634e-03,  1.2873e-03, -2.0041e-02, -1.1377e-02,\n",
       "         2.6995e-02,  7.2226e-05, -7.9888e-03,  2.3203e-03, -1.8296e-02,\n",
       "         1.5127e-03,  2.0797e-02, -3.5582e-03,  9.1086e-03, -7.3239e-04,\n",
       "         5.7407e-03,  4.2886e-03,  9.8906e-03, -8.6811e-03, -6.2283e-03,\n",
       "        -7.7144e-03,  1.8123e-03, -6.2366e-03,  1.7517e-02, -5.0139e-03,\n",
       "        -1.9107e-02, -9.8813e-03, -8.3628e-03,  3.8114e-03, -8.3182e-03,\n",
       "         1.0935e-02, -1.2879e-02,  1.3855e-02, -2.2307e-03, -2.0496e-03,\n",
       "        -2.9741e-04, -2.0361e-02, -1.2161e-02,  3.3543e-03, -8.0695e-03,\n",
       "         9.3348e-03,  2.1811e-02, -6.3629e-03, -1.9528e-02, -9.8233e-03,\n",
       "         3.7842e-03,  2.1074e-02, -6.8755e-03,  1.2912e-02,  1.0363e-02,\n",
       "         7.3825e-03, -1.1114e-02, -6.4901e-03, -1.2932e-02,  1.6797e-02,\n",
       "         6.2250e-03, -1.0451e-03,  1.0846e-02,  6.7275e-03,  4.1699e-03,\n",
       "         1.2190e-02,  6.5010e-05, -7.4951e-04,  7.9796e-03,  1.2950e-02,\n",
       "        -1.0756e-02,  5.9763e-03,  5.2459e-03, -1.2635e-02, -1.5376e-03,\n",
       "         5.4463e-03,  2.0340e-03, -5.0741e-04,  7.7490e-04,  1.5005e-02,\n",
       "        -2.2254e-03, -6.9149e-04, -4.5582e-05, -2.0616e-02, -5.0587e-03,\n",
       "         1.2123e-02, -3.6046e-03,  1.1589e-02,  2.3413e-03, -1.3134e-02,\n",
       "         1.7374e-02,  3.9336e-03, -8.8667e-03, -6.5015e-03, -3.7057e-03,\n",
       "        -1.3497e-02,  1.2065e-02,  4.5793e-03, -8.3119e-03,  6.9646e-03,\n",
       "         1.5163e-03,  3.8914e-04, -1.5402e-02,  8.7180e-03,  1.4165e-02,\n",
       "         7.2567e-03,  5.0829e-03,  9.2566e-03,  8.6523e-03,  1.8037e-02,\n",
       "         2.3719e-03, -8.1502e-04,  7.0155e-03,  6.5872e-03,  1.0450e-02,\n",
       "        -4.2342e-03,  1.0670e-03,  6.1607e-04,  1.0470e-03,  2.7049e-03,\n",
       "        -4.8361e-03, -1.2859e-02,  9.5885e-03, -8.0827e-03,  1.4245e-03,\n",
       "        -2.9118e-03,  8.2315e-03,  9.5470e-03, -2.8359e-03,  1.5556e-02,\n",
       "         1.0611e-02, -7.4114e-03,  2.7416e-03, -1.1110e-02, -1.4414e-02,\n",
       "         5.9046e-03,  7.5263e-03, -1.0933e-02,  4.5891e-03,  8.0689e-03,\n",
       "         1.9201e-02,  7.4032e-03,  6.1166e-03,  2.5036e-04, -1.9781e-02,\n",
       "        -5.2692e-03,  2.3589e-03, -2.0159e-02, -9.3808e-03, -1.1069e-03,\n",
       "        -6.5599e-03, -5.3730e-03, -1.9683e-02,  1.1728e-04,  4.2114e-03,\n",
       "         1.6481e-02,  6.3046e-03,  1.0214e-02,  1.4000e-02,  9.2786e-03,\n",
       "         7.8380e-03, -1.5826e-02, -1.5823e-03,  1.0538e-02,  9.7204e-03,\n",
       "         5.3063e-03,  1.6528e-02,  6.3525e-03, -4.7120e-03, -1.9497e-03,\n",
       "         1.2910e-02, -2.0331e-02, -2.2953e-03, -2.3942e-03],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 1325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "id": "7152b67d-015d-44e9-8164-379b47dbcd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0035, -0.0058, -0.0076,  0.0093, -0.0137,  0.0186,  0.0142,  0.0004,\n",
       "         0.0030, -0.0032, -0.0001,  0.0032,  0.0018, -0.0168, -0.0111,  0.0088,\n",
       "        -0.0017, -0.0141,  0.0042, -0.0148,  0.0117,  0.0236,  0.0058, -0.0213,\n",
       "         0.0125,  0.0061, -0.0031,  0.0103,  0.0042,  0.0015,  0.0107,  0.0077,\n",
       "        -0.0052,  0.0051, -0.0018,  0.0119,  0.0004, -0.0074,  0.0050, -0.0029,\n",
       "         0.0023,  0.0057,  0.0056, -0.0273,  0.0036, -0.0050, -0.0204, -0.0151,\n",
       "         0.0020,  0.0130, -0.0136,  0.0026,  0.0007,  0.0001, -0.0023, -0.0077,\n",
       "         0.0132,  0.0011, -0.0038,  0.0161, -0.0042,  0.0070,  0.0039, -0.0112,\n",
       "        -0.0108, -0.0029,  0.0067, -0.0141, -0.0064, -0.0043,  0.0080, -0.0175,\n",
       "         0.0083,  0.0017,  0.0002,  0.0097,  0.0079,  0.0033, -0.0154,  0.0113,\n",
       "         0.0008, -0.0165, -0.0035,  0.0041,  0.0066,  0.0018,  0.0023, -0.0026,\n",
       "        -0.0085, -0.0080, -0.0125, -0.0055, -0.0013,  0.0067,  0.0077, -0.0045,\n",
       "        -0.0018, -0.0172, -0.0086,  0.0025, -0.0011, -0.0035,  0.0055,  0.0089,\n",
       "        -0.0024, -0.0062,  0.0019,  0.0167, -0.0068, -0.0058, -0.0004, -0.0011,\n",
       "        -0.0120,  0.0097,  0.0144, -0.0038, -0.0024, -0.0012, -0.0001, -0.0061,\n",
       "         0.0001, -0.0072, -0.0027,  0.0045,  0.0045,  0.0030, -0.0067,  0.0017,\n",
       "         0.0002,  0.0023, -0.0015, -0.0002, -0.0045, -0.0012, -0.0025,  0.0118,\n",
       "         0.0031, -0.0057, -0.0173, -0.0231,  0.0112, -0.0031, -0.0104, -0.0117,\n",
       "        -0.0070,  0.0099, -0.0045, -0.0006, -0.0036,  0.0039, -0.0058,  0.0111,\n",
       "         0.0002, -0.0004,  0.0068,  0.0155,  0.0104, -0.0026,  0.0035, -0.0069,\n",
       "         0.0032,  0.0132, -0.0278,  0.0035,  0.0008, -0.0070, -0.0137, -0.0030,\n",
       "         0.0004, -0.0049, -0.0049,  0.0029,  0.0114,  0.0093, -0.0008, -0.0134,\n",
       "        -0.0048,  0.0199,  0.0061, -0.0112,  0.0004,  0.0082,  0.0048, -0.0098,\n",
       "        -0.0031, -0.0058,  0.0135,  0.0148,  0.0125,  0.0049,  0.0172, -0.0149,\n",
       "         0.0064, -0.0089, -0.0013,  0.0104, -0.0061, -0.0134, -0.0051, -0.0017,\n",
       "         0.0029, -0.0083, -0.0041, -0.0073,  0.0106,  0.0055, -0.0006, -0.0138,\n",
       "        -0.0198, -0.0021,  0.0111,  0.0002,  0.0023,  0.0036,  0.0002,  0.0051,\n",
       "         0.0050, -0.0015,  0.0052,  0.0185, -0.0066,  0.0093,  0.0051, -0.0070,\n",
       "        -0.0184, -0.0172, -0.0082,  0.0012, -0.0004, -0.0103, -0.0017, -0.0154,\n",
       "         0.0050,  0.0118,  0.0064,  0.0044,  0.0008, -0.0023, -0.0116,  0.0089,\n",
       "        -0.0151, -0.0043,  0.0077,  0.0055,  0.0049,  0.0123,  0.0061, -0.0008,\n",
       "        -0.0139, -0.0021, -0.0001,  0.0017, -0.0074,  0.0081,  0.0014,  0.0048,\n",
       "        -0.0045,  0.0010,  0.0025, -0.0014, -0.0079, -0.0030,  0.0004,  0.0064,\n",
       "        -0.0045, -0.0141, -0.0192, -0.0004,  0.0125,  0.0006, -0.0018,  0.0044,\n",
       "         0.0015, -0.0001,  0.0098, -0.0008, -0.0041,  0.0180, -0.0165,  0.0023,\n",
       "        -0.0037, -0.0241, -0.0083, -0.0132, -0.0057, -0.0008,  0.0161, -0.0074,\n",
       "         0.0103,  0.0063,  0.0020, -0.0063, -0.0050,  0.0116, -0.0079,  0.0010,\n",
       "        -0.0074,  0.0206, -0.0123,  0.0143,  0.0089, -0.0081, -0.0081, -0.0061,\n",
       "         0.0089, -0.0087,  0.0006,  0.0089, -0.0012,  0.0095,  0.0011, -0.0188,\n",
       "        -0.0019, -0.0093,  0.0052, -0.0036,  0.0026,  0.0079,  0.0217,  0.0054,\n",
       "        -0.0122, -0.0032,  0.0015, -0.0128,  0.0031, -0.0110, -0.0041, -0.0019,\n",
       "         0.0002, -0.0112, -0.0187, -0.0018, -0.0055, -0.0130, -0.0147, -0.0060,\n",
       "         0.0020, -0.0012, -0.0076,  0.0008, -0.0021, -0.0074, -0.0006,  0.0027,\n",
       "        -0.0091, -0.0135,  0.0020, -0.0129,  0.0002, -0.0154, -0.0112, -0.0054,\n",
       "        -0.0004,  0.0069,  0.0015, -0.0126, -0.0051, -0.0141,  0.0004, -0.0050,\n",
       "        -0.0013,  0.0027, -0.0017,  0.0007,  0.0051, -0.0181, -0.0004,  0.0033,\n",
       "        -0.0138, -0.0012, -0.0050,  0.0083,  0.0151,  0.0002, -0.0045, -0.0103,\n",
       "        -0.0066, -0.0188,  0.0055,  0.0069, -0.0036,  0.0056,  0.0077,  0.0197,\n",
       "         0.0004, -0.0125,  0.0036,  0.0021,  0.0051,  0.0072, -0.0007, -0.0036,\n",
       "        -0.0103, -0.0043,  0.0042, -0.0136, -0.0060,  0.0060,  0.0023, -0.0055,\n",
       "        -0.0132, -0.0036, -0.0136,  0.0002,  0.0106,  0.0086,  0.0021, -0.0061,\n",
       "        -0.0159,  0.0106,  0.0013,  0.0058, -0.0043, -0.0011,  0.0061,  0.0075,\n",
       "         0.0004, -0.0039,  0.0082,  0.0002,  0.0072, -0.0136,  0.0015,  0.0089,\n",
       "         0.0055, -0.0075,  0.0008,  0.0112,  0.0017, -0.0066,  0.0013, -0.0257,\n",
       "        -0.0142, -0.0050, -0.0067,  0.0087,  0.0119,  0.0018, -0.0030,  0.0026,\n",
       "        -0.0024,  0.0030, -0.0050,  0.0050,  0.0032, -0.0015, -0.0017,  0.0026,\n",
       "        -0.0052, -0.0070,  0.0098, -0.0004,  0.0118, -0.0013,  0.0069,  0.0085,\n",
       "         0.0068,  0.0008,  0.0007,  0.0167, -0.0087,  0.0015, -0.0169,  0.0033,\n",
       "        -0.0064, -0.0044, -0.0156,  0.0093, -0.0063,  0.0042,  0.0015,  0.0060,\n",
       "        -0.0088, -0.0079,  0.0013, -0.0005,  0.0100,  0.0055,  0.0050, -0.0108,\n",
       "        -0.0023, -0.0076,  0.0138,  0.0138,  0.0052, -0.0116, -0.0046,  0.0006,\n",
       "         0.0015, -0.0052, -0.0055, -0.0070,  0.0068,  0.0004,  0.0049, -0.0015,\n",
       "         0.0064, -0.0116,  0.0033, -0.0007, -0.0060, -0.0074,  0.0153, -0.0046,\n",
       "        -0.0113, -0.0137,  0.0021,  0.0166,  0.0070,  0.0018, -0.0054,  0.0015,\n",
       "         0.0049,  0.0105,  0.0030, -0.0091,  0.0002,  0.0038, -0.0100, -0.0044,\n",
       "         0.0279,  0.0054, -0.0001,  0.0045,  0.0002, -0.0002,  0.0184,  0.0131,\n",
       "         0.0108,  0.0097,  0.0026,  0.0043, -0.0056,  0.0038,  0.0089,  0.0149,\n",
       "        -0.0103, -0.0154, -0.0175, -0.0017,  0.0083,  0.0097, -0.0156,  0.0063,\n",
       "         0.0077, -0.0087, -0.0167, -0.0020,  0.0050,  0.0004, -0.0031,  0.0064,\n",
       "         0.0015,  0.0151, -0.0001, -0.0075,  0.0055, -0.0147, -0.0069,  0.0089,\n",
       "        -0.0101, -0.0012,  0.0088, -0.0118, -0.0006,  0.0030, -0.0056,  0.0015,\n",
       "         0.0041, -0.0114,  0.0000, -0.0042, -0.0064,  0.0145, -0.0064, -0.0001,\n",
       "         0.0018,  0.0055, -0.0082, -0.0068,  0.0026, -0.0006, -0.0168, -0.0180,\n",
       "         0.0079,  0.0187, -0.0030, -0.0068, -0.0038, -0.0194, -0.0098,  0.0076,\n",
       "        -0.0072,  0.0025,  0.0013,  0.0101, -0.0031, -0.0017,  0.0010, -0.0020,\n",
       "         0.0098,  0.0174,  0.0036, -0.0055,  0.0074,  0.0111, -0.0062, -0.0005,\n",
       "         0.0043, -0.0023,  0.0035, -0.0068,  0.0142, -0.0191,  0.0179, -0.0099,\n",
       "        -0.0055,  0.0013, -0.0200, -0.0114,  0.0272,  0.0001, -0.0080,  0.0024,\n",
       "        -0.0181,  0.0018,  0.0210, -0.0032,  0.0093, -0.0006,  0.0058,  0.0043,\n",
       "         0.0099, -0.0087, -0.0062, -0.0077,  0.0018, -0.0063,  0.0175, -0.0050,\n",
       "        -0.0192, -0.0100, -0.0083,  0.0038, -0.0083,  0.0110, -0.0130,  0.0139,\n",
       "        -0.0023, -0.0018, -0.0001, -0.0203, -0.0120,  0.0035, -0.0079,  0.0094,\n",
       "         0.0221, -0.0063, -0.0197, -0.0098,  0.0038,  0.0212, -0.0068,  0.0130,\n",
       "         0.0104,  0.0074, -0.0112, -0.0064, -0.0130,  0.0169,  0.0062, -0.0011,\n",
       "         0.0108,  0.0067,  0.0042,  0.0123,  0.0001, -0.0006,  0.0081,  0.0132,\n",
       "        -0.0106,  0.0061,  0.0054, -0.0128, -0.0015,  0.0055,  0.0020, -0.0005,\n",
       "         0.0008,  0.0150, -0.0023, -0.0007,  0.0000, -0.0206, -0.0051,  0.0123,\n",
       "        -0.0036,  0.0117,  0.0024, -0.0132,  0.0174,  0.0039, -0.0089, -0.0064,\n",
       "        -0.0038, -0.0135,  0.0120,  0.0046, -0.0083,  0.0070,  0.0015,  0.0004,\n",
       "        -0.0154,  0.0088,  0.0142,  0.0073,  0.0051,  0.0093,  0.0087,  0.0181,\n",
       "         0.0023, -0.0008,  0.0070,  0.0066,  0.0105, -0.0043,  0.0011,  0.0006,\n",
       "         0.0011,  0.0026, -0.0049, -0.0129,  0.0095, -0.0081,  0.0014, -0.0030,\n",
       "         0.0083,  0.0097, -0.0029,  0.0156,  0.0106, -0.0074,  0.0026, -0.0112,\n",
       "        -0.0144,  0.0058,  0.0075, -0.0111,  0.0046,  0.0081,  0.0192,  0.0074,\n",
       "         0.0061,  0.0002, -0.0198, -0.0054,  0.0024, -0.0203, -0.0093, -0.0011,\n",
       "        -0.0067, -0.0055, -0.0197,  0.0001,  0.0043,  0.0165,  0.0063,  0.0103,\n",
       "         0.0141,  0.0093,  0.0079, -0.0159, -0.0015,  0.0106,  0.0098,  0.0054,\n",
       "         0.0167,  0.0063, -0.0048, -0.0020,  0.0131, -0.0205, -0.0023, -0.0024],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b45a0-68af-4c88-9381-3dcd2a9725a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
