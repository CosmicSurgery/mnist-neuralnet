{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ccde100-704b-443e-90b3-100855cd94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f91008-c4e3-4355-b11e-bfae14fa3aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1747e3843b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4cc1f9-00ed-43de-aba1-dbd6540496bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c4cc97a3-ff6b-488b-83cb-4b86ca414324",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4a125aec-b1d8-482a-8198-645ad6657abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ace84846-9e28-4fe3-bd71-8bcef49864c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1, 1, 28, 28])\n",
      "Shape of y: torch.Size([1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "a3ad7e7c-aa0f-421d-8625-55ed516e6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "7591b7bb-a598-48fb-82ed-44ae0b6d74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if batch % 100 == 1:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        if batch > 2000:\n",
    "            return True\n",
    "\n",
    "    correct /= size\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1bad07ea-741c-49a7-9bfe-9d1497287da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_one(dataloader, model, loss_fn, optimizer):\n",
    "    X, y1 = next(iter(dataloader))\n",
    "    X =  X[0][None,:]\n",
    "    y1 = y1[None,:][:,0]\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "853730be-4cd7-4ac3-8045-ccbae7c1391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(x1,y1, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    pred = model(x1)\n",
    "    loss = loss_fn(pred, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "2f044e20-dd72-404d-97cc-2657695623c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(train_dataloader))\n",
    "x1 =  x1[0][None,:]\n",
    "y1 = y1[None,:][:,0]\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ea2e893f-871b-4c18-89f5-479029789b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_one(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "b9037264-c6cb-42e5-8151-8bf7ad2e8727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn_image as isns\n",
    "isns.imgplot(x1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "3f4fde39-b752-4c3d-9b7d-dd66cc604278",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_=iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "3e476d3d-7986-4687-b5a3-9110299344ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "d470c0f2-29a5-4250-9813-2d5df2fa3847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter_)\n",
    "x1 =  x1[0][None,:]\n",
    "y1 = y1[None,:][:,0]\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "c776ea7d-454a-4bed-8a5e-267f98e3ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0525, -0.0595,  0.0721, -0.1558, -0.0971, -0.0192, -0.0878,  0.2483,\n",
       "          0.1709,  0.0990]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "660553e4-d864-4d13-863a-c02e9e998fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3817, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one(x1,y1, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "471a1013-f7bb-4ed9-bd1c-f8348ce16e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0105,  0.0017,  0.0017, -0.0253, -0.0138, -0.0099, -0.0168, -0.0133,\n",
       "         0.0035,  0.0342, -0.0145, -0.0313,  0.0099,  0.0316, -0.0350,  0.0311,\n",
       "         0.0259,  0.0107])"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "d173ab29-d37b-4ca7-85a6-5a40ebb213ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1420, -0.0604, -0.0507, -0.2054, -0.0986, -0.1237,  0.0103,  0.2327,\n",
       "         0.1419,  0.1194])"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "67972596-e89f-490a-9b7e-435938def73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
       "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
       "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
       "        ...,\n",
       "        [-0.0215,  0.0106,  0.0308,  ..., -0.0199,  0.0161, -0.0342],\n",
       "        [ 0.0350, -0.0297, -0.0037,  ...,  0.0171,  0.0238, -0.0001],\n",
       "        [ 0.0085,  0.0223, -0.0324,  ..., -0.0296,  0.0182, -0.0296]])"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "e3f5553c-0fe8-4607-91e7-6ab4febd60d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2118, -0.0872,  0.0237,  0.0396, -0.1763,  0.2065, -0.2173,  0.1158,\n",
       "         -0.1041, -0.1668,  0.1045, -0.0560, -0.0937, -0.0247, -0.0842,  0.2194,\n",
       "         -0.2216, -0.0808]])"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.weight'][y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "c5b1edf5-898f-49a8-8155-8102af1c6655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-model(x1)[0][5].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "10b4f616-7310-4676-88c3-d769295865c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0453, -0.1708, -0.0112, -0.1373, -0.1919,  0.1684, -0.0890,  0.1894,\n",
       "         -0.1370,  0.0977, -0.0159, -0.0262,  0.1361, -0.2134,  0.1970, -0.0483,\n",
       "          0.1064, -0.1399]])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.weight'][y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e96a9e85-3749-4b63-b873-3ca22e3cd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "c25197a6-4b17-47a8-bcd0-5c86331f55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "f3d5f233-e32a-44a5-bac3-16f85ee389d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "bc893a84-cffa-443f-b260-62215e7cdcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.457382  [    1/60000]\n",
      "loss: 2.374321  [    2/60000]\n",
      "loss: 2.398252  [    3/60000]\n",
      "loss: 2.376534  [    4/60000]\n",
      "loss: 2.279009  [    5/60000]\n",
      "loss: 2.354060  [    6/60000]\n",
      "loss: 2.389974  [    7/60000]\n",
      "loss: 2.512438  [    8/60000]\n",
      "loss: 2.381724  [    9/60000]\n",
      "loss: 2.361736  [   10/60000]\n",
      "loss: 2.445279  [   11/60000]\n",
      "loss: 2.393084  [   12/60000]\n",
      "loss: 2.463694  [   13/60000]\n",
      "loss: 2.353185  [   14/60000]\n",
      "loss: 2.354204  [   15/60000]\n",
      "loss: 2.029841  [   16/60000]\n",
      "loss: 2.406065  [   17/60000]\n",
      "loss: 2.070472  [   18/60000]\n",
      "loss: 2.292971  [   19/60000]\n",
      "loss: 2.138663  [   20/60000]\n",
      "loss: 2.431557  [   21/60000]\n",
      "loss: 2.403752  [   22/60000]\n",
      "loss: 2.184052  [   23/60000]\n",
      "loss: 2.353106  [   24/60000]\n",
      "loss: 2.457611  [   25/60000]\n",
      "loss: 2.308816  [   26/60000]\n",
      "loss: 2.388967  [   27/60000]\n",
      "loss: 2.393305  [   28/60000]\n",
      "loss: 2.347595  [   29/60000]\n",
      "loss: 2.007248  [   30/60000]\n",
      "loss: 2.381086  [   31/60000]\n",
      "loss: 2.015050  [   32/60000]\n",
      "loss: 2.344524  [   33/60000]\n",
      "loss: 2.173007  [   34/60000]\n",
      "loss: 2.507094  [   35/60000]\n",
      "loss: 2.461276  [   36/60000]\n",
      "loss: 2.359147  [   37/60000]\n",
      "loss: 2.428228  [   38/60000]\n",
      "loss: 1.962420  [   39/60000]\n",
      "loss: 2.368456  [   40/60000]\n",
      "loss: 2.373413  [   41/60000]\n",
      "loss: 1.990771  [   42/60000]\n",
      "loss: 2.030597  [   43/60000]\n",
      "loss: 2.185883  [   44/60000]\n",
      "loss: 2.446616  [   45/60000]\n",
      "loss: 2.175831  [   46/60000]\n",
      "loss: 2.021182  [   47/60000]\n",
      "loss: 2.481621  [   48/60000]\n",
      "loss: 2.153817  [   49/60000]\n",
      "loss: 2.370799  [   50/60000]\n",
      "loss: 2.428045  [   51/60000]\n",
      "loss: 2.298961  [   52/60000]\n",
      "loss: 2.004873  [   53/60000]\n",
      "loss: 2.380420  [   54/60000]\n",
      "loss: 2.303694  [   55/60000]\n",
      "loss: 2.023952  [   56/60000]\n",
      "loss: 2.436422  [   57/60000]\n",
      "loss: 2.114005  [   58/60000]\n",
      "loss: 2.423149  [   59/60000]\n",
      "loss: 2.344003  [   60/60000]\n",
      "loss: 2.336373  [   61/60000]\n",
      "loss: 2.344895  [   62/60000]\n",
      "loss: 2.290793  [   63/60000]\n",
      "loss: 2.301963  [   64/60000]\n",
      "loss: 2.311668  [   65/60000]\n",
      "loss: 2.364532  [   66/60000]\n",
      "loss: 2.284589  [   67/60000]\n",
      "loss: 2.357118  [   68/60000]\n",
      "loss: 2.556758  [   69/60000]\n",
      "loss: 2.270796  [   70/60000]\n",
      "loss: 2.305754  [   71/60000]\n",
      "loss: 2.056557  [   72/60000]\n",
      "loss: 2.321007  [   73/60000]\n",
      "loss: 2.363550  [   74/60000]\n",
      "loss: 2.409006  [   75/60000]\n",
      "loss: 2.303288  [   76/60000]\n",
      "loss: 2.372838  [   77/60000]\n",
      "loss: 2.393176  [   78/60000]\n",
      "loss: 2.379480  [   79/60000]\n",
      "loss: 2.037939  [   80/60000]\n",
      "loss: 2.196802  [   81/60000]\n",
      "loss: 2.297050  [   82/60000]\n",
      "loss: 2.404132  [   83/60000]\n",
      "loss: 2.319688  [   84/60000]\n",
      "loss: 2.012230  [   85/60000]\n",
      "loss: 1.966024  [   86/60000]\n",
      "loss: 2.493802  [   87/60000]\n",
      "loss: 2.186703  [   88/60000]\n",
      "loss: 2.366231  [   89/60000]\n",
      "loss: 2.337330  [   90/60000]\n",
      "loss: 2.183945  [   91/60000]\n",
      "loss: 2.058703  [   92/60000]\n",
      "loss: 2.383113  [   93/60000]\n",
      "loss: 2.279875  [   94/60000]\n",
      "loss: 2.058636  [   95/60000]\n",
      "loss: 2.303416  [   96/60000]\n",
      "loss: 2.047011  [   97/60000]\n",
      "loss: 2.022220  [   98/60000]\n",
      "loss: 2.401495  [   99/60000]\n",
      "loss: 2.395959  [  100/60000]\n",
      "loss: 2.469971  [  101/60000]\n",
      "loss: 2.061147  [  102/60000]\n",
      "loss: 2.316692  [  103/60000]\n",
      "loss: 2.019716  [  104/60000]\n",
      "loss: 2.316088  [  105/60000]\n",
      "loss: 2.306842  [  106/60000]\n",
      "loss: 2.238658  [  107/60000]\n",
      "loss: 2.449841  [  108/60000]\n",
      "loss: 2.497422  [  109/60000]\n",
      "loss: 2.290148  [  110/60000]\n",
      "loss: 2.175290  [  111/60000]\n",
      "loss: 2.407884  [  112/60000]\n",
      "loss: 2.371191  [  113/60000]\n",
      "loss: 2.367791  [  114/60000]\n",
      "loss: 2.265563  [  115/60000]\n",
      "loss: 2.339309  [  116/60000]\n",
      "loss: 2.189725  [  117/60000]\n",
      "loss: 2.314351  [  118/60000]\n",
      "loss: 2.267811  [  119/60000]\n",
      "loss: 2.113630  [  120/60000]\n",
      "loss: 2.322234  [  121/60000]\n",
      "loss: 2.328642  [  122/60000]\n",
      "loss: 2.380949  [  123/60000]\n",
      "loss: 2.006341  [  124/60000]\n",
      "loss: 2.312154  [  125/60000]\n",
      "loss: 2.023949  [  126/60000]\n",
      "loss: 2.338248  [  127/60000]\n",
      "loss: 2.333175  [  128/60000]\n",
      "loss: 2.355299  [  129/60000]\n",
      "loss: 2.348243  [  130/60000]\n",
      "loss: 2.465834  [  131/60000]\n",
      "loss: 2.319063  [  132/60000]\n",
      "loss: 2.467157  [  133/60000]\n",
      "loss: 2.144565  [  134/60000]\n",
      "loss: 2.408011  [  135/60000]\n",
      "loss: 2.410102  [  136/60000]\n",
      "loss: 2.362372  [  137/60000]\n",
      "loss: 2.027791  [  138/60000]\n",
      "loss: 2.511208  [  139/60000]\n",
      "loss: 2.346364  [  140/60000]\n",
      "loss: 2.024697  [  141/60000]\n",
      "loss: 1.988924  [  142/60000]\n",
      "loss: 2.334469  [  143/60000]\n",
      "loss: 2.404268  [  144/60000]\n",
      "loss: 2.074089  [  145/60000]\n",
      "loss: 2.423167  [  146/60000]\n",
      "loss: 2.073210  [  147/60000]\n",
      "loss: 2.293417  [  148/60000]\n",
      "loss: 2.036835  [  149/60000]\n",
      "loss: 2.342754  [  150/60000]\n",
      "loss: 2.313711  [  151/60000]\n",
      "loss: 2.206053  [  152/60000]\n",
      "loss: 2.302864  [  153/60000]\n",
      "loss: 2.168279  [  154/60000]\n",
      "loss: 2.259636  [  155/60000]\n",
      "loss: 2.272234  [  156/60000]\n",
      "loss: 2.254425  [  157/60000]\n",
      "loss: 2.415483  [  158/60000]\n",
      "loss: 2.066455  [  159/60000]\n",
      "loss: 2.320844  [  160/60000]\n",
      "loss: 2.130740  [  161/60000]\n",
      "loss: 2.326095  [  162/60000]\n",
      "loss: 2.233023  [  163/60000]\n",
      "loss: 2.309264  [  164/60000]\n",
      "loss: 2.353132  [  165/60000]\n",
      "loss: 2.228647  [  166/60000]\n",
      "loss: 2.369086  [  167/60000]\n",
      "loss: 2.145125  [  168/60000]\n",
      "loss: 2.080335  [  169/60000]\n",
      "loss: 2.246302  [  170/60000]\n",
      "loss: 2.158643  [  171/60000]\n",
      "loss: 2.342619  [  172/60000]\n",
      "loss: 2.223605  [  173/60000]\n",
      "loss: 2.429008  [  174/60000]\n",
      "loss: 2.312516  [  175/60000]\n",
      "loss: 2.433983  [  176/60000]\n",
      "loss: 2.196336  [  177/60000]\n",
      "loss: 2.279613  [  178/60000]\n",
      "loss: 2.338908  [  179/60000]\n",
      "loss: 2.371718  [  180/60000]\n",
      "loss: 2.395352  [  181/60000]\n",
      "loss: 2.355333  [  182/60000]\n",
      "loss: 2.584203  [  183/60000]\n",
      "loss: 2.156023  [  184/60000]\n",
      "loss: 2.272957  [  185/60000]\n",
      "loss: 2.064751  [  186/60000]\n",
      "loss: 2.306755  [  187/60000]\n",
      "loss: 2.329756  [  188/60000]\n",
      "loss: 2.091464  [  189/60000]\n",
      "loss: 2.311039  [  190/60000]\n",
      "loss: 2.320044  [  191/60000]\n",
      "loss: 2.385747  [  192/60000]\n",
      "loss: 2.299354  [  193/60000]\n",
      "loss: 2.019513  [  194/60000]\n",
      "loss: 2.324955  [  195/60000]\n",
      "loss: 2.187784  [  196/60000]\n",
      "loss: 2.017511  [  197/60000]\n",
      "loss: 2.033314  [  198/60000]\n",
      "loss: 2.391086  [  199/60000]\n",
      "loss: 2.335300  [  200/60000]\n",
      "loss: 2.292928  [  201/60000]\n",
      "loss: 2.308666  [  202/60000]\n",
      "loss: 2.036043  [  203/60000]\n",
      "loss: 2.367980  [  204/60000]\n",
      "loss: 2.279077  [  205/60000]\n",
      "loss: 2.300560  [  206/60000]\n",
      "loss: 2.237979  [  207/60000]\n",
      "loss: 2.366932  [  208/60000]\n",
      "loss: 2.249532  [  209/60000]\n",
      "loss: 2.316673  [  210/60000]\n",
      "loss: 2.310466  [  211/60000]\n",
      "loss: 2.255971  [  212/60000]\n",
      "loss: 2.048611  [  213/60000]\n",
      "loss: 2.301674  [  214/60000]\n",
      "loss: 2.019328  [  215/60000]\n",
      "loss: 2.202767  [  216/60000]\n",
      "loss: 2.234571  [  217/60000]\n",
      "loss: 2.394231  [  218/60000]\n",
      "loss: 2.312960  [  219/60000]\n",
      "loss: 2.522362  [  220/60000]\n",
      "loss: 2.372709  [  221/60000]\n",
      "loss: 2.243963  [  222/60000]\n",
      "loss: 2.309927  [  223/60000]\n",
      "loss: 2.026025  [  224/60000]\n",
      "loss: 2.386992  [  225/60000]\n",
      "loss: 2.032337  [  226/60000]\n",
      "loss: 2.243532  [  227/60000]\n",
      "loss: 2.201359  [  228/60000]\n",
      "loss: 2.522944  [  229/60000]\n",
      "loss: 2.192489  [  230/60000]\n",
      "loss: 2.032807  [  231/60000]\n",
      "loss: 2.257997  [  232/60000]\n",
      "loss: 2.429440  [  233/60000]\n",
      "loss: 2.340889  [  234/60000]\n",
      "loss: 2.254264  [  235/60000]\n",
      "loss: 2.291470  [  236/60000]\n",
      "loss: 2.515186  [  237/60000]\n",
      "loss: 2.298668  [  238/60000]\n",
      "loss: 2.235412  [  239/60000]\n",
      "loss: 2.514005  [  240/60000]\n",
      "loss: 2.094147  [  241/60000]\n",
      "loss: 2.271759  [  242/60000]\n",
      "loss: 2.213790  [  243/60000]\n",
      "loss: 2.025351  [  244/60000]\n",
      "loss: 2.623137  [  245/60000]\n",
      "loss: 1.962612  [  246/60000]\n",
      "loss: 2.422785  [  247/60000]\n",
      "loss: 2.166323  [  248/60000]\n",
      "loss: 2.307359  [  249/60000]\n",
      "loss: 2.281417  [  250/60000]\n",
      "loss: 2.300456  [  251/60000]\n",
      "loss: 2.235035  [  252/60000]\n",
      "loss: 2.308139  [  253/60000]\n",
      "loss: 2.237556  [  254/60000]\n",
      "loss: 2.384926  [  255/60000]\n",
      "loss: 2.269296  [  256/60000]\n",
      "loss: 2.364707  [  257/60000]\n",
      "loss: 2.288064  [  258/60000]\n",
      "loss: 2.054354  [  259/60000]\n",
      "loss: 2.444150  [  260/60000]\n",
      "loss: 2.366834  [  261/60000]\n",
      "loss: 2.212544  [  262/60000]\n",
      "loss: 2.266366  [  263/60000]\n",
      "loss: 2.022424  [  264/60000]\n",
      "loss: 2.221100  [  265/60000]\n",
      "loss: 2.044041  [  266/60000]\n",
      "loss: 2.561689  [  267/60000]\n",
      "loss: 2.195846  [  268/60000]\n",
      "loss: 2.427497  [  269/60000]\n",
      "loss: 2.247304  [  270/60000]\n",
      "loss: 2.276663  [  271/60000]\n",
      "loss: 2.401438  [  272/60000]\n",
      "loss: 2.315652  [  273/60000]\n",
      "loss: 2.474183  [  274/60000]\n",
      "loss: 2.347353  [  275/60000]\n",
      "loss: 2.336153  [  276/60000]\n",
      "loss: 2.259283  [  277/60000]\n",
      "loss: 2.303716  [  278/60000]\n",
      "loss: 2.468861  [  279/60000]\n",
      "loss: 2.213876  [  280/60000]\n",
      "loss: 2.134178  [  281/60000]\n",
      "loss: 2.267612  [  282/60000]\n",
      "loss: 2.228287  [  283/60000]\n",
      "loss: 2.247148  [  284/60000]\n",
      "loss: 2.492845  [  285/60000]\n",
      "loss: 2.183136  [  286/60000]\n",
      "loss: 2.336345  [  287/60000]\n",
      "loss: 2.380968  [  288/60000]\n",
      "loss: 2.052409  [  289/60000]\n",
      "loss: 2.322821  [  290/60000]\n",
      "loss: 2.236202  [  291/60000]\n",
      "loss: 2.214498  [  292/60000]\n",
      "loss: 2.335090  [  293/60000]\n",
      "loss: 2.189132  [  294/60000]\n",
      "loss: 2.247332  [  295/60000]\n",
      "loss: 1.932665  [  296/60000]\n",
      "loss: 2.232287  [  297/60000]\n",
      "loss: 2.230895  [  298/60000]\n",
      "loss: 2.230873  [  299/60000]\n",
      "loss: 2.228970  [  300/60000]\n",
      "loss: 1.991651  [  301/60000]\n",
      "loss: 2.051780  [  302/60000]\n",
      "loss: 2.318078  [  303/60000]\n",
      "loss: 2.099221  [  304/60000]\n",
      "loss: 2.184304  [  305/60000]\n",
      "loss: 2.029400  [  306/60000]\n",
      "loss: 2.530914  [  307/60000]\n",
      "loss: 2.046019  [  308/60000]\n",
      "loss: 2.317926  [  309/60000]\n",
      "loss: 2.276999  [  310/60000]\n",
      "loss: 2.251832  [  311/60000]\n",
      "loss: 2.182726  [  312/60000]\n",
      "loss: 2.076457  [  313/60000]\n",
      "loss: 2.158663  [  314/60000]\n",
      "loss: 2.353414  [  315/60000]\n",
      "loss: 2.195194  [  316/60000]\n",
      "loss: 2.437894  [  317/60000]\n",
      "loss: 2.257284  [  318/60000]\n",
      "loss: 2.335767  [  319/60000]\n",
      "loss: 2.125422  [  320/60000]\n",
      "loss: 2.176360  [  321/60000]\n",
      "loss: 2.255587  [  322/60000]\n",
      "loss: 2.161626  [  323/60000]\n",
      "loss: 2.282906  [  324/60000]\n",
      "loss: 2.029853  [  325/60000]\n",
      "loss: 2.232698  [  326/60000]\n",
      "loss: 2.063941  [  327/60000]\n",
      "loss: 2.277652  [  328/60000]\n",
      "loss: 2.554286  [  329/60000]\n",
      "loss: 2.235163  [  330/60000]\n",
      "loss: 2.305232  [  331/60000]\n",
      "loss: 2.271907  [  332/60000]\n",
      "loss: 2.490983  [  333/60000]\n",
      "loss: 2.037170  [  334/60000]\n",
      "loss: 2.155040  [  335/60000]\n",
      "loss: 2.476042  [  336/60000]\n",
      "loss: 2.389611  [  337/60000]\n",
      "loss: 2.053799  [  338/60000]\n",
      "loss: 2.263244  [  339/60000]\n",
      "loss: 2.242073  [  340/60000]\n",
      "loss: 2.072158  [  341/60000]\n",
      "loss: 2.130533  [  342/60000]\n",
      "loss: 2.372390  [  343/60000]\n",
      "loss: 2.041506  [  344/60000]\n",
      "loss: 2.175680  [  345/60000]\n",
      "loss: 2.193845  [  346/60000]\n",
      "loss: 2.234701  [  347/60000]\n",
      "loss: 2.201565  [  348/60000]\n",
      "loss: 2.041003  [  349/60000]\n",
      "loss: 2.012800  [  350/60000]\n",
      "loss: 2.150093  [  351/60000]\n",
      "loss: 2.193283  [  352/60000]\n",
      "loss: 2.056737  [  353/60000]\n",
      "loss: 2.027172  [  354/60000]\n",
      "loss: 2.414009  [  355/60000]\n",
      "loss: 2.220572  [  356/60000]\n",
      "loss: 2.324439  [  357/60000]\n",
      "loss: 2.154643  [  358/60000]\n",
      "loss: 2.250189  [  359/60000]\n",
      "loss: 2.131268  [  360/60000]\n",
      "loss: 2.303393  [  361/60000]\n",
      "loss: 2.155532  [  362/60000]\n",
      "loss: 2.235633  [  363/60000]\n",
      "loss: 2.322598  [  364/60000]\n",
      "loss: 2.195662  [  365/60000]\n",
      "loss: 2.253099  [  366/60000]\n",
      "loss: 2.232501  [  367/60000]\n",
      "loss: 2.211748  [  368/60000]\n",
      "loss: 2.004610  [  369/60000]\n",
      "loss: 2.218576  [  370/60000]\n",
      "loss: 2.024360  [  371/60000]\n",
      "loss: 2.057159  [  372/60000]\n",
      "loss: 2.288752  [  373/60000]\n",
      "loss: 2.207927  [  374/60000]\n",
      "loss: 2.184226  [  375/60000]\n",
      "loss: 2.182865  [  376/60000]\n",
      "loss: 2.430659  [  377/60000]\n",
      "loss: 2.019284  [  378/60000]\n",
      "loss: 2.278839  [  379/60000]\n",
      "loss: 2.306427  [  380/60000]\n",
      "loss: 2.245924  [  381/60000]\n",
      "loss: 2.189244  [  382/60000]\n",
      "loss: 2.234742  [  383/60000]\n",
      "loss: 2.155040  [  384/60000]\n",
      "loss: 1.973298  [  385/60000]\n",
      "loss: 2.154372  [  386/60000]\n",
      "loss: 2.115338  [  387/60000]\n",
      "loss: 2.007849  [  388/60000]\n",
      "loss: 2.355105  [  389/60000]\n",
      "loss: 2.123666  [  390/60000]\n",
      "loss: 2.238215  [  391/60000]\n",
      "loss: 2.206268  [  392/60000]\n",
      "loss: 2.248947  [  393/60000]\n",
      "loss: 1.995990  [  394/60000]\n",
      "loss: 2.286512  [  395/60000]\n",
      "loss: 2.237507  [  396/60000]\n",
      "loss: 2.495175  [  397/60000]\n",
      "loss: 2.214107  [  398/60000]\n",
      "loss: 2.245860  [  399/60000]\n",
      "loss: 2.081010  [  400/60000]\n",
      "loss: 2.224533  [  401/60000]\n",
      "loss: 2.279436  [  402/60000]\n",
      "loss: 2.232467  [  403/60000]\n",
      "loss: 2.425774  [  404/60000]\n",
      "loss: 2.100479  [  405/60000]\n",
      "loss: 2.226676  [  406/60000]\n",
      "loss: 2.264773  [  407/60000]\n",
      "loss: 2.420487  [  408/60000]\n",
      "loss: 2.197839  [  409/60000]\n",
      "loss: 2.117297  [  410/60000]\n",
      "loss: 2.282953  [  411/60000]\n",
      "loss: 2.025105  [  412/60000]\n",
      "loss: 2.411655  [  413/60000]\n",
      "loss: 2.163718  [  414/60000]\n",
      "loss: 2.350177  [  415/60000]\n",
      "loss: 2.117488  [  416/60000]\n",
      "loss: 2.149481  [  417/60000]\n",
      "loss: 2.356821  [  418/60000]\n",
      "loss: 2.187631  [  419/60000]\n",
      "loss: 2.128031  [  420/60000]\n",
      "loss: 2.476484  [  421/60000]\n",
      "loss: 2.206368  [  422/60000]\n",
      "loss: 2.067759  [  423/60000]\n",
      "loss: 2.102193  [  424/60000]\n",
      "loss: 2.207519  [  425/60000]\n",
      "loss: 2.210449  [  426/60000]\n",
      "loss: 1.962342  [  427/60000]\n",
      "loss: 2.010329  [  428/60000]\n",
      "loss: 2.145807  [  429/60000]\n",
      "loss: 2.101276  [  430/60000]\n",
      "loss: 2.307892  [  431/60000]\n",
      "loss: 2.187919  [  432/60000]\n",
      "loss: 2.389779  [  433/60000]\n",
      "loss: 2.171427  [  434/60000]\n",
      "loss: 2.118009  [  435/60000]\n",
      "loss: 2.139003  [  436/60000]\n",
      "loss: 2.097702  [  437/60000]\n",
      "loss: 2.457355  [  438/60000]\n",
      "loss: 2.296606  [  439/60000]\n",
      "loss: 2.015151  [  440/60000]\n",
      "loss: 2.161704  [  441/60000]\n",
      "loss: 2.093476  [  442/60000]\n",
      "loss: 2.249498  [  443/60000]\n",
      "loss: 2.249048  [  444/60000]\n",
      "loss: 2.378964  [  445/60000]\n",
      "loss: 1.929963  [  446/60000]\n",
      "loss: 2.138836  [  447/60000]\n",
      "loss: 2.189777  [  448/60000]\n",
      "loss: 2.334350  [  449/60000]\n",
      "loss: 2.227086  [  450/60000]\n",
      "loss: 2.167815  [  451/60000]\n",
      "loss: 2.012939  [  452/60000]\n",
      "loss: 2.286418  [  453/60000]\n",
      "loss: 2.079649  [  454/60000]\n",
      "loss: 2.225787  [  455/60000]\n",
      "loss: 2.149630  [  456/60000]\n",
      "loss: 1.949655  [  457/60000]\n",
      "loss: 2.245130  [  458/60000]\n",
      "loss: 1.953106  [  459/60000]\n",
      "loss: 2.382170  [  460/60000]\n",
      "loss: 2.090297  [  461/60000]\n",
      "loss: 2.238374  [  462/60000]\n",
      "loss: 1.910832  [  463/60000]\n",
      "loss: 2.346815  [  464/60000]\n",
      "loss: 2.008482  [  465/60000]\n",
      "loss: 2.270893  [  466/60000]\n",
      "loss: 2.197887  [  467/60000]\n",
      "loss: 1.984716  [  468/60000]\n",
      "loss: 2.003038  [  469/60000]\n",
      "loss: 1.982528  [  470/60000]\n",
      "loss: 2.332824  [  471/60000]\n",
      "loss: 2.143506  [  472/60000]\n",
      "loss: 2.272128  [  473/60000]\n",
      "loss: 2.057147  [  474/60000]\n",
      "loss: 2.593082  [  475/60000]\n",
      "loss: 2.235290  [  476/60000]\n",
      "loss: 2.318204  [  477/60000]\n",
      "loss: 2.134091  [  478/60000]\n",
      "loss: 1.934145  [  479/60000]\n",
      "loss: 2.236953  [  480/60000]\n",
      "loss: 2.581711  [  481/60000]\n",
      "loss: 2.196536  [  482/60000]\n",
      "loss: 2.224140  [  483/60000]\n",
      "loss: 1.966896  [  484/60000]\n",
      "loss: 2.222551  [  485/60000]\n",
      "loss: 2.064392  [  486/60000]\n",
      "loss: 2.284128  [  487/60000]\n",
      "loss: 2.186459  [  488/60000]\n",
      "loss: 2.441144  [  489/60000]\n",
      "loss: 2.132856  [  490/60000]\n",
      "loss: 2.148523  [  491/60000]\n",
      "loss: 2.332925  [  492/60000]\n",
      "loss: 2.084607  [  493/60000]\n",
      "loss: 2.181467  [  494/60000]\n",
      "loss: 2.436852  [  495/60000]\n",
      "loss: 2.188470  [  496/60000]\n",
      "loss: 2.561013  [  497/60000]\n",
      "loss: 2.012387  [  498/60000]\n",
      "loss: 2.281306  [  499/60000]\n",
      "loss: 1.905820  [  500/60000]\n",
      "loss: 2.491241  [  501/60000]\n",
      "loss: 2.213366  [  502/60000]\n",
      "loss: 2.481926  [  503/60000]\n",
      "loss: 2.062805  [  504/60000]\n",
      "loss: 2.632989  [  505/60000]\n",
      "loss: 2.003939  [  506/60000]\n",
      "loss: 2.263748  [  507/60000]\n",
      "loss: 2.192394  [  508/60000]\n",
      "loss: 2.151967  [  509/60000]\n",
      "loss: 2.375733  [  510/60000]\n",
      "loss: 2.135271  [  511/60000]\n",
      "loss: 2.029489  [  512/60000]\n",
      "loss: 2.510913  [  513/60000]\n",
      "loss: 2.399695  [  514/60000]\n",
      "loss: 2.500658  [  515/60000]\n",
      "loss: 2.142373  [  516/60000]\n",
      "loss: 2.418417  [  517/60000]\n",
      "loss: 2.065463  [  518/60000]\n",
      "loss: 2.001075  [  519/60000]\n",
      "loss: 1.972757  [  520/60000]\n",
      "loss: 2.191903  [  521/60000]\n",
      "loss: 2.030092  [  522/60000]\n",
      "loss: 1.958958  [  523/60000]\n",
      "loss: 2.371616  [  524/60000]\n",
      "loss: 2.005185  [  525/60000]\n",
      "loss: 2.139493  [  526/60000]\n",
      "loss: 1.972243  [  527/60000]\n",
      "loss: 2.265483  [  528/60000]\n",
      "loss: 2.189816  [  529/60000]\n",
      "loss: 2.190965  [  530/60000]\n",
      "loss: 2.178267  [  531/60000]\n",
      "loss: 2.207920  [  532/60000]\n",
      "loss: 2.084715  [  533/60000]\n",
      "loss: 2.303010  [  534/60000]\n",
      "loss: 2.291850  [  535/60000]\n",
      "loss: 2.227776  [  536/60000]\n",
      "loss: 2.273442  [  537/60000]\n",
      "loss: 2.392917  [  538/60000]\n",
      "loss: 2.129951  [  539/60000]\n",
      "loss: 1.947934  [  540/60000]\n",
      "loss: 2.394146  [  541/60000]\n",
      "loss: 2.219308  [  542/60000]\n",
      "loss: 2.238806  [  543/60000]\n",
      "loss: 2.375697  [  544/60000]\n",
      "loss: 2.616804  [  545/60000]\n",
      "loss: 1.903163  [  546/60000]\n",
      "loss: 2.236315  [  547/60000]\n",
      "loss: 2.489237  [  548/60000]\n",
      "loss: 2.195190  [  549/60000]\n",
      "loss: 2.152166  [  550/60000]\n",
      "loss: 2.109159  [  551/60000]\n",
      "loss: 2.080906  [  552/60000]\n",
      "loss: 2.181757  [  553/60000]\n",
      "loss: 2.137628  [  554/60000]\n",
      "loss: 2.542565  [  555/60000]\n",
      "loss: 2.166414  [  556/60000]\n",
      "loss: 2.113865  [  557/60000]\n",
      "loss: 2.111521  [  558/60000]\n",
      "loss: 2.336668  [  559/60000]\n",
      "loss: 1.987920  [  560/60000]\n",
      "loss: 2.071978  [  561/60000]\n",
      "loss: 2.273495  [  562/60000]\n",
      "loss: 1.998642  [  563/60000]\n",
      "loss: 2.018143  [  564/60000]\n",
      "loss: 2.259491  [  565/60000]\n",
      "loss: 2.385840  [  566/60000]\n",
      "loss: 2.186338  [  567/60000]\n",
      "loss: 1.996563  [  568/60000]\n",
      "loss: 2.288238  [  569/60000]\n",
      "loss: 2.269586  [  570/60000]\n",
      "loss: 2.180571  [  571/60000]\n",
      "loss: 1.909190  [  572/60000]\n",
      "loss: 2.106918  [  573/60000]\n",
      "loss: 2.091478  [  574/60000]\n",
      "loss: 2.290560  [  575/60000]\n",
      "loss: 1.928579  [  576/60000]\n",
      "loss: 2.325084  [  577/60000]\n",
      "loss: 1.903908  [  578/60000]\n",
      "loss: 2.315886  [  579/60000]\n",
      "loss: 2.440463  [  580/60000]\n",
      "loss: 2.099526  [  581/60000]\n",
      "loss: 2.173443  [  582/60000]\n",
      "loss: 2.085535  [  583/60000]\n",
      "loss: 2.228729  [  584/60000]\n",
      "loss: 2.269972  [  585/60000]\n",
      "loss: 2.199426  [  586/60000]\n",
      "loss: 1.981610  [  587/60000]\n",
      "loss: 2.165869  [  588/60000]\n",
      "loss: 2.327586  [  589/60000]\n",
      "loss: 2.204888  [  590/60000]\n",
      "loss: 2.249625  [  591/60000]\n",
      "loss: 2.133833  [  592/60000]\n",
      "loss: 2.245864  [  593/60000]\n",
      "loss: 2.121736  [  594/60000]\n",
      "loss: 1.958551  [  595/60000]\n",
      "loss: 2.141514  [  596/60000]\n",
      "loss: 1.959432  [  597/60000]\n",
      "loss: 2.444038  [  598/60000]\n",
      "loss: 2.307265  [  599/60000]\n",
      "loss: 2.047778  [  600/60000]\n",
      "loss: 2.182206  [  601/60000]\n",
      "loss: 2.228759  [  602/60000]\n",
      "loss: 2.098370  [  603/60000]\n",
      "loss: 2.154574  [  604/60000]\n",
      "loss: 2.207747  [  605/60000]\n",
      "loss: 2.267800  [  606/60000]\n",
      "loss: 2.233721  [  607/60000]\n",
      "loss: 2.336471  [  608/60000]\n",
      "loss: 2.121137  [  609/60000]\n",
      "loss: 2.066851  [  610/60000]\n",
      "loss: 2.584071  [  611/60000]\n",
      "loss: 2.079620  [  612/60000]\n",
      "loss: 1.929445  [  613/60000]\n",
      "loss: 2.254507  [  614/60000]\n",
      "loss: 2.432289  [  615/60000]\n",
      "loss: 2.082434  [  616/60000]\n",
      "loss: 2.157346  [  617/60000]\n",
      "loss: 2.006119  [  618/60000]\n",
      "loss: 2.180514  [  619/60000]\n",
      "loss: 2.132418  [  620/60000]\n",
      "loss: 2.545704  [  621/60000]\n",
      "loss: 2.142911  [  622/60000]\n",
      "loss: 2.491405  [  623/60000]\n",
      "loss: 2.545901  [  624/60000]\n",
      "loss: 2.046786  [  625/60000]\n",
      "loss: 2.526327  [  626/60000]\n",
      "loss: 2.148726  [  627/60000]\n",
      "loss: 2.062397  [  628/60000]\n",
      "loss: 2.133517  [  629/60000]\n",
      "loss: 2.192015  [  630/60000]\n",
      "loss: 2.396284  [  631/60000]\n",
      "loss: 2.196883  [  632/60000]\n",
      "loss: 2.023906  [  633/60000]\n",
      "loss: 2.032727  [  634/60000]\n",
      "loss: 2.012695  [  635/60000]\n",
      "loss: 2.341160  [  636/60000]\n",
      "loss: 2.073499  [  637/60000]\n",
      "loss: 2.123833  [  638/60000]\n",
      "loss: 2.160774  [  639/60000]\n",
      "loss: 1.898247  [  640/60000]\n",
      "loss: 1.985997  [  641/60000]\n",
      "loss: 2.218389  [  642/60000]\n",
      "loss: 2.099629  [  643/60000]\n",
      "loss: 2.198961  [  644/60000]\n",
      "loss: 2.041705  [  645/60000]\n",
      "loss: 2.149726  [  646/60000]\n",
      "loss: 2.254737  [  647/60000]\n",
      "loss: 2.279226  [  648/60000]\n",
      "loss: 2.107185  [  649/60000]\n",
      "loss: 2.338282  [  650/60000]\n",
      "loss: 2.276719  [  651/60000]\n",
      "loss: 1.946950  [  652/60000]\n",
      "loss: 2.510370  [  653/60000]\n",
      "loss: 2.408786  [  654/60000]\n",
      "loss: 1.985671  [  655/60000]\n",
      "loss: 2.172396  [  656/60000]\n",
      "loss: 2.027360  [  657/60000]\n",
      "loss: 2.220044  [  658/60000]\n",
      "loss: 2.293014  [  659/60000]\n",
      "loss: 2.246981  [  660/60000]\n",
      "loss: 2.353084  [  661/60000]\n",
      "loss: 2.350476  [  662/60000]\n",
      "loss: 2.008927  [  663/60000]\n",
      "loss: 2.236825  [  664/60000]\n",
      "loss: 2.357858  [  665/60000]\n",
      "loss: 2.555805  [  666/60000]\n",
      "loss: 2.064012  [  667/60000]\n",
      "loss: 2.163116  [  668/60000]\n",
      "loss: 1.984034  [  669/60000]\n",
      "loss: 1.784137  [  670/60000]\n",
      "loss: 2.416009  [  671/60000]\n",
      "loss: 2.041948  [  672/60000]\n",
      "loss: 2.330769  [  673/60000]\n",
      "loss: 2.151699  [  674/60000]\n",
      "loss: 2.293114  [  675/60000]\n",
      "loss: 2.012898  [  676/60000]\n",
      "loss: 2.105279  [  677/60000]\n",
      "loss: 2.261374  [  678/60000]\n",
      "loss: 2.127216  [  679/60000]\n",
      "loss: 2.463753  [  680/60000]\n",
      "loss: 2.228690  [  681/60000]\n",
      "loss: 2.216325  [  682/60000]\n",
      "loss: 2.289360  [  683/60000]\n",
      "loss: 1.939041  [  684/60000]\n",
      "loss: 2.209744  [  685/60000]\n",
      "loss: 1.917796  [  686/60000]\n",
      "loss: 1.976983  [  687/60000]\n",
      "loss: 2.116046  [  688/60000]\n",
      "loss: 2.209319  [  689/60000]\n",
      "loss: 1.732789  [  690/60000]\n",
      "loss: 2.503697  [  691/60000]\n",
      "loss: 2.025284  [  692/60000]\n",
      "loss: 2.276058  [  693/60000]\n",
      "loss: 2.188408  [  694/60000]\n",
      "loss: 2.068036  [  695/60000]\n",
      "loss: 2.091194  [  696/60000]\n",
      "loss: 2.181837  [  697/60000]\n",
      "loss: 2.294902  [  698/60000]\n",
      "loss: 2.005046  [  699/60000]\n",
      "loss: 2.514497  [  700/60000]\n",
      "loss: 2.204501  [  701/60000]\n",
      "loss: 2.234837  [  702/60000]\n",
      "loss: 1.973999  [  703/60000]\n",
      "loss: 2.029212  [  704/60000]\n",
      "loss: 2.332833  [  705/60000]\n",
      "loss: 1.980595  [  706/60000]\n",
      "loss: 2.207401  [  707/60000]\n",
      "loss: 2.137167  [  708/60000]\n",
      "loss: 2.111438  [  709/60000]\n",
      "loss: 1.736226  [  710/60000]\n",
      "loss: 2.110723  [  711/60000]\n",
      "loss: 2.075755  [  712/60000]\n",
      "loss: 2.203547  [  713/60000]\n",
      "loss: 2.023821  [  714/60000]\n",
      "loss: 2.142186  [  715/60000]\n",
      "loss: 2.054799  [  716/60000]\n",
      "loss: 2.098096  [  717/60000]\n",
      "loss: 2.067386  [  718/60000]\n",
      "loss: 2.209224  [  719/60000]\n",
      "loss: 2.571775  [  720/60000]\n",
      "loss: 2.102134  [  721/60000]\n",
      "loss: 2.288468  [  722/60000]\n",
      "loss: 2.463180  [  723/60000]\n",
      "loss: 1.957509  [  724/60000]\n",
      "loss: 1.912667  [  725/60000]\n",
      "loss: 1.896992  [  726/60000]\n",
      "loss: 2.122918  [  727/60000]\n",
      "loss: 2.141808  [  728/60000]\n",
      "loss: 2.027724  [  729/60000]\n",
      "loss: 2.144764  [  730/60000]\n",
      "loss: 2.023659  [  731/60000]\n",
      "loss: 2.082474  [  732/60000]\n",
      "loss: 2.037140  [  733/60000]\n",
      "loss: 1.817153  [  734/60000]\n",
      "loss: 2.009318  [  735/60000]\n",
      "loss: 2.272667  [  736/60000]\n",
      "loss: 2.119656  [  737/60000]\n",
      "loss: 2.352106  [  738/60000]\n",
      "loss: 2.225929  [  739/60000]\n",
      "loss: 2.325987  [  740/60000]\n",
      "loss: 2.036196  [  741/60000]\n",
      "loss: 2.149256  [  742/60000]\n",
      "loss: 2.201437  [  743/60000]\n",
      "loss: 1.693863  [  744/60000]\n",
      "loss: 2.206492  [  745/60000]\n",
      "loss: 1.817043  [  746/60000]\n",
      "loss: 2.375030  [  747/60000]\n",
      "loss: 2.095855  [  748/60000]\n",
      "loss: 2.531991  [  749/60000]\n",
      "loss: 2.269393  [  750/60000]\n",
      "loss: 2.002004  [  751/60000]\n",
      "loss: 2.448895  [  752/60000]\n",
      "loss: 2.068637  [  753/60000]\n",
      "loss: 1.982624  [  754/60000]\n",
      "loss: 2.325999  [  755/60000]\n",
      "loss: 2.112083  [  756/60000]\n",
      "loss: 1.957610  [  757/60000]\n",
      "loss: 2.466305  [  758/60000]\n",
      "loss: 1.837252  [  759/60000]\n",
      "loss: 2.346179  [  760/60000]\n",
      "loss: 2.290658  [  761/60000]\n",
      "loss: 1.847495  [  762/60000]\n",
      "loss: 2.243301  [  763/60000]\n",
      "loss: 2.144622  [  764/60000]\n",
      "loss: 2.206646  [  765/60000]\n",
      "loss: 2.030528  [  766/60000]\n",
      "loss: 1.728357  [  767/60000]\n",
      "loss: 2.126340  [  768/60000]\n",
      "loss: 2.448930  [  769/60000]\n",
      "loss: 1.746367  [  770/60000]\n",
      "loss: 2.347105  [  771/60000]\n",
      "loss: 2.424600  [  772/60000]\n",
      "loss: 2.162890  [  773/60000]\n",
      "loss: 2.007703  [  774/60000]\n",
      "loss: 2.020784  [  775/60000]\n",
      "loss: 2.354992  [  776/60000]\n",
      "loss: 2.065906  [  777/60000]\n",
      "loss: 1.852720  [  778/60000]\n",
      "loss: 2.418134  [  779/60000]\n",
      "loss: 2.061785  [  780/60000]\n",
      "loss: 2.107700  [  781/60000]\n",
      "loss: 1.681043  [  782/60000]\n",
      "loss: 2.212745  [  783/60000]\n",
      "loss: 2.023005  [  784/60000]\n",
      "loss: 1.862796  [  785/60000]\n",
      "loss: 2.250657  [  786/60000]\n",
      "loss: 2.016175  [  787/60000]\n",
      "loss: 1.707006  [  788/60000]\n",
      "loss: 2.159694  [  789/60000]\n",
      "loss: 2.160192  [  790/60000]\n",
      "loss: 2.014035  [  791/60000]\n",
      "loss: 1.680259  [  792/60000]\n",
      "loss: 1.948788  [  793/60000]\n",
      "loss: 2.129132  [  794/60000]\n",
      "loss: 2.390904  [  795/60000]\n",
      "loss: 2.030849  [  796/60000]\n",
      "loss: 1.875118  [  797/60000]\n",
      "loss: 2.049155  [  798/60000]\n",
      "loss: 1.934946  [  799/60000]\n",
      "loss: 2.470631  [  800/60000]\n",
      "loss: 2.122061  [  801/60000]\n",
      "loss: 2.319482  [  802/60000]\n",
      "loss: 2.298668  [  803/60000]\n",
      "loss: 2.010317  [  804/60000]\n",
      "loss: 2.105881  [  805/60000]\n",
      "loss: 2.451154  [  806/60000]\n",
      "loss: 2.066279  [  807/60000]\n",
      "loss: 2.294456  [  808/60000]\n",
      "loss: 2.347667  [  809/60000]\n",
      "loss: 2.023163  [  810/60000]\n",
      "loss: 2.319540  [  811/60000]\n",
      "loss: 2.109134  [  812/60000]\n",
      "loss: 2.002061  [  813/60000]\n",
      "loss: 2.278390  [  814/60000]\n",
      "loss: 2.028238  [  815/60000]\n",
      "loss: 2.538530  [  816/60000]\n",
      "loss: 2.208798  [  817/60000]\n",
      "loss: 1.742240  [  818/60000]\n",
      "loss: 1.921193  [  819/60000]\n",
      "loss: 1.962575  [  820/60000]\n",
      "loss: 1.948844  [  821/60000]\n",
      "loss: 2.061437  [  822/60000]\n",
      "loss: 2.142699  [  823/60000]\n",
      "loss: 1.755441  [  824/60000]\n",
      "loss: 1.976645  [  825/60000]\n",
      "loss: 1.799926  [  826/60000]\n",
      "loss: 2.166690  [  827/60000]\n",
      "loss: 2.009676  [  828/60000]\n",
      "loss: 2.264213  [  829/60000]\n",
      "loss: 2.275524  [  830/60000]\n",
      "loss: 2.123408  [  831/60000]\n",
      "loss: 1.714446  [  832/60000]\n",
      "loss: 2.445055  [  833/60000]\n",
      "loss: 1.974625  [  834/60000]\n",
      "loss: 2.179276  [  835/60000]\n",
      "loss: 2.006264  [  836/60000]\n",
      "loss: 2.548484  [  837/60000]\n",
      "loss: 2.355042  [  838/60000]\n",
      "loss: 2.087579  [  839/60000]\n",
      "loss: 1.960494  [  840/60000]\n",
      "loss: 2.127156  [  841/60000]\n",
      "loss: 2.215078  [  842/60000]\n",
      "loss: 2.229588  [  843/60000]\n",
      "loss: 2.323622  [  844/60000]\n",
      "loss: 2.196633  [  845/60000]\n",
      "loss: 2.335922  [  846/60000]\n",
      "loss: 2.422874  [  847/60000]\n",
      "loss: 1.925965  [  848/60000]\n",
      "loss: 2.063663  [  849/60000]\n",
      "loss: 1.819219  [  850/60000]\n",
      "loss: 2.151895  [  851/60000]\n",
      "loss: 1.971282  [  852/60000]\n",
      "loss: 2.358467  [  853/60000]\n",
      "loss: 1.981870  [  854/60000]\n",
      "loss: 2.530025  [  855/60000]\n",
      "loss: 1.981915  [  856/60000]\n",
      "loss: 2.143814  [  857/60000]\n",
      "loss: 2.135581  [  858/60000]\n",
      "loss: 1.970183  [  859/60000]\n",
      "loss: 1.632475  [  860/60000]\n",
      "loss: 1.803739  [  861/60000]\n",
      "loss: 2.197170  [  862/60000]\n",
      "loss: 2.188661  [  863/60000]\n",
      "loss: 2.358392  [  864/60000]\n",
      "loss: 2.172014  [  865/60000]\n",
      "loss: 1.824095  [  866/60000]\n",
      "loss: 2.200787  [  867/60000]\n",
      "loss: 2.092318  [  868/60000]\n",
      "loss: 2.051431  [  869/60000]\n",
      "loss: 1.939666  [  870/60000]\n",
      "loss: 2.262483  [  871/60000]\n",
      "loss: 1.992006  [  872/60000]\n",
      "loss: 1.757348  [  873/60000]\n",
      "loss: 2.157135  [  874/60000]\n",
      "loss: 1.887904  [  875/60000]\n",
      "loss: 2.226957  [  876/60000]\n",
      "loss: 1.978410  [  877/60000]\n",
      "loss: 2.026857  [  878/60000]\n",
      "loss: 2.244720  [  879/60000]\n",
      "loss: 2.334574  [  880/60000]\n",
      "loss: 1.968774  [  881/60000]\n",
      "loss: 2.306202  [  882/60000]\n",
      "loss: 2.124181  [  883/60000]\n",
      "loss: 1.992448  [  884/60000]\n",
      "loss: 2.007941  [  885/60000]\n",
      "loss: 1.719856  [  886/60000]\n",
      "loss: 2.470367  [  887/60000]\n",
      "loss: 2.098490  [  888/60000]\n",
      "loss: 1.998374  [  889/60000]\n",
      "loss: 1.910597  [  890/60000]\n",
      "loss: 1.906401  [  891/60000]\n",
      "loss: 1.983048  [  892/60000]\n",
      "loss: 2.427001  [  893/60000]\n",
      "loss: 2.245932  [  894/60000]\n",
      "loss: 2.256152  [  895/60000]\n",
      "loss: 2.004463  [  896/60000]\n",
      "loss: 2.036224  [  897/60000]\n",
      "loss: 2.050555  [  898/60000]\n",
      "loss: 1.951880  [  899/60000]\n",
      "loss: 2.333744  [  900/60000]\n",
      "loss: 2.184120  [  901/60000]\n",
      "loss: 2.211848  [  902/60000]\n",
      "loss: 2.324566  [  903/60000]\n",
      "loss: 1.955045  [  904/60000]\n",
      "loss: 1.913632  [  905/60000]\n",
      "loss: 2.049000  [  906/60000]\n",
      "loss: 2.111914  [  907/60000]\n",
      "loss: 2.039732  [  908/60000]\n",
      "loss: 1.946981  [  909/60000]\n",
      "loss: 1.903215  [  910/60000]\n",
      "loss: 2.338451  [  911/60000]\n",
      "loss: 1.857811  [  912/60000]\n",
      "loss: 2.470562  [  913/60000]\n",
      "loss: 1.948769  [  914/60000]\n",
      "loss: 2.025096  [  915/60000]\n",
      "loss: 2.357205  [  916/60000]\n",
      "loss: 2.497267  [  917/60000]\n",
      "loss: 2.157598  [  918/60000]\n",
      "loss: 1.934728  [  919/60000]\n",
      "loss: 1.987946  [  920/60000]\n",
      "loss: 1.963010  [  921/60000]\n",
      "loss: 2.358680  [  922/60000]\n",
      "loss: 2.336594  [  923/60000]\n",
      "loss: 1.802863  [  924/60000]\n",
      "loss: 2.410468  [  925/60000]\n",
      "loss: 1.685174  [  926/60000]\n",
      "loss: 2.033861  [  927/60000]\n",
      "loss: 1.773272  [  928/60000]\n",
      "loss: 1.922097  [  929/60000]\n",
      "loss: 2.154297  [  930/60000]\n",
      "loss: 2.233416  [  931/60000]\n",
      "loss: 2.167218  [  932/60000]\n",
      "loss: 2.072609  [  933/60000]\n",
      "loss: 2.021763  [  934/60000]\n",
      "loss: 2.022731  [  935/60000]\n",
      "loss: 2.028596  [  936/60000]\n",
      "loss: 1.981154  [  937/60000]\n",
      "loss: 2.006429  [  938/60000]\n",
      "loss: 2.210777  [  939/60000]\n",
      "loss: 2.459142  [  940/60000]\n",
      "loss: 2.095040  [  941/60000]\n",
      "loss: 2.078559  [  942/60000]\n",
      "loss: 1.938816  [  943/60000]\n",
      "loss: 2.211394  [  944/60000]\n",
      "loss: 2.386345  [  945/60000]\n",
      "loss: 2.101891  [  946/60000]\n",
      "loss: 2.268813  [  947/60000]\n",
      "loss: 2.348213  [  948/60000]\n",
      "loss: 1.951797  [  949/60000]\n",
      "loss: 1.646688  [  950/60000]\n",
      "loss: 1.871648  [  951/60000]\n",
      "loss: 1.996038  [  952/60000]\n",
      "loss: 1.931146  [  953/60000]\n",
      "loss: 2.035880  [  954/60000]\n",
      "loss: 1.835356  [  955/60000]\n",
      "loss: 2.380572  [  956/60000]\n",
      "loss: 2.368479  [  957/60000]\n",
      "loss: 2.086693  [  958/60000]\n",
      "loss: 2.415431  [  959/60000]\n",
      "loss: 1.913930  [  960/60000]\n",
      "loss: 2.457432  [  961/60000]\n",
      "loss: 2.018557  [  962/60000]\n",
      "loss: 1.838280  [  963/60000]\n",
      "loss: 1.756033  [  964/60000]\n",
      "loss: 2.336678  [  965/60000]\n",
      "loss: 1.884923  [  966/60000]\n",
      "loss: 2.338318  [  967/60000]\n",
      "loss: 2.001042  [  968/60000]\n",
      "loss: 1.903421  [  969/60000]\n",
      "loss: 1.864531  [  970/60000]\n",
      "loss: 2.476653  [  971/60000]\n",
      "loss: 1.902509  [  972/60000]\n",
      "loss: 1.939550  [  973/60000]\n",
      "loss: 2.284259  [  974/60000]\n",
      "loss: 2.351657  [  975/60000]\n",
      "loss: 1.873769  [  976/60000]\n",
      "loss: 1.774709  [  977/60000]\n",
      "loss: 2.302604  [  978/60000]\n",
      "loss: 2.184423  [  979/60000]\n",
      "loss: 1.961119  [  980/60000]\n",
      "loss: 2.264280  [  981/60000]\n",
      "loss: 1.707169  [  982/60000]\n",
      "loss: 2.142674  [  983/60000]\n",
      "loss: 2.004484  [  984/60000]\n",
      "loss: 2.064828  [  985/60000]\n",
      "loss: 2.309906  [  986/60000]\n",
      "loss: 2.348578  [  987/60000]\n",
      "loss: 1.914668  [  988/60000]\n",
      "loss: 2.082746  [  989/60000]\n",
      "loss: 2.029951  [  990/60000]\n",
      "loss: 1.934604  [  991/60000]\n",
      "loss: 2.036010  [  992/60000]\n",
      "loss: 2.107792  [  993/60000]\n",
      "loss: 2.044282  [  994/60000]\n",
      "loss: 1.815652  [  995/60000]\n",
      "loss: 1.885476  [  996/60000]\n",
      "loss: 2.284954  [  997/60000]\n",
      "loss: 1.837666  [  998/60000]\n",
      "loss: 2.111029  [  999/60000]\n",
      "loss: 2.344249  [ 1000/60000]\n",
      "loss: 1.960044  [ 1001/60000]\n",
      "loss: 1.795052  [ 1002/60000]\n",
      "loss: 2.178914  [ 1003/60000]\n",
      "loss: 2.023530  [ 1004/60000]\n",
      "loss: 2.396436  [ 1005/60000]\n",
      "loss: 2.117921  [ 1006/60000]\n",
      "loss: 2.124304  [ 1007/60000]\n",
      "loss: 2.158592  [ 1008/60000]\n",
      "loss: 1.948177  [ 1009/60000]\n",
      "loss: 1.984292  [ 1010/60000]\n",
      "loss: 1.910535  [ 1011/60000]\n",
      "loss: 2.186231  [ 1012/60000]\n",
      "loss: 1.961353  [ 1013/60000]\n",
      "loss: 1.978408  [ 1014/60000]\n",
      "loss: 1.913115  [ 1015/60000]\n",
      "loss: 1.958562  [ 1016/60000]\n",
      "loss: 1.918670  [ 1017/60000]\n",
      "loss: 2.047879  [ 1018/60000]\n",
      "loss: 1.718647  [ 1019/60000]\n",
      "loss: 2.035075  [ 1020/60000]\n",
      "loss: 2.299547  [ 1021/60000]\n",
      "loss: 2.089695  [ 1022/60000]\n",
      "loss: 1.904284  [ 1023/60000]\n",
      "loss: 1.959396  [ 1024/60000]\n",
      "loss: 2.514659  [ 1025/60000]\n",
      "loss: 1.814594  [ 1026/60000]\n",
      "loss: 1.857859  [ 1027/60000]\n",
      "loss: 2.165917  [ 1028/60000]\n",
      "loss: 2.116260  [ 1029/60000]\n",
      "loss: 1.987388  [ 1030/60000]\n",
      "loss: 2.032091  [ 1031/60000]\n",
      "loss: 2.113781  [ 1032/60000]\n",
      "loss: 2.437717  [ 1033/60000]\n",
      "loss: 2.137913  [ 1034/60000]\n",
      "loss: 2.160565  [ 1035/60000]\n",
      "loss: 2.309218  [ 1036/60000]\n",
      "loss: 2.238081  [ 1037/60000]\n",
      "loss: 2.124151  [ 1038/60000]\n",
      "loss: 2.248325  [ 1039/60000]\n",
      "loss: 2.371420  [ 1040/60000]\n",
      "loss: 1.563239  [ 1041/60000]\n",
      "loss: 2.273268  [ 1042/60000]\n",
      "loss: 1.883929  [ 1043/60000]\n",
      "loss: 1.906288  [ 1044/60000]\n",
      "loss: 1.974431  [ 1045/60000]\n",
      "loss: 1.859483  [ 1046/60000]\n",
      "loss: 2.061800  [ 1047/60000]\n",
      "loss: 2.268679  [ 1048/60000]\n",
      "loss: 2.145950  [ 1049/60000]\n",
      "loss: 1.887267  [ 1050/60000]\n",
      "loss: 1.988661  [ 1051/60000]\n",
      "loss: 1.944590  [ 1052/60000]\n",
      "loss: 2.013706  [ 1053/60000]\n",
      "loss: 2.232123  [ 1054/60000]\n",
      "loss: 1.801297  [ 1055/60000]\n",
      "loss: 2.162832  [ 1056/60000]\n",
      "loss: 2.126058  [ 1057/60000]\n",
      "loss: 1.869424  [ 1058/60000]\n",
      "loss: 2.279797  [ 1059/60000]\n",
      "loss: 1.854607  [ 1060/60000]\n",
      "loss: 2.041606  [ 1061/60000]\n",
      "loss: 2.111738  [ 1062/60000]\n",
      "loss: 2.468849  [ 1063/60000]\n",
      "loss: 1.916485  [ 1064/60000]\n",
      "loss: 2.028402  [ 1065/60000]\n",
      "loss: 2.006742  [ 1066/60000]\n",
      "loss: 2.221614  [ 1067/60000]\n",
      "loss: 1.802863  [ 1068/60000]\n",
      "loss: 2.036698  [ 1069/60000]\n",
      "loss: 2.048913  [ 1070/60000]\n",
      "loss: 2.497826  [ 1071/60000]\n",
      "loss: 2.247101  [ 1072/60000]\n",
      "loss: 1.790168  [ 1073/60000]\n",
      "loss: 1.983583  [ 1074/60000]\n",
      "loss: 2.233856  [ 1075/60000]\n",
      "loss: 2.115726  [ 1076/60000]\n",
      "loss: 1.750030  [ 1077/60000]\n",
      "loss: 2.113918  [ 1078/60000]\n",
      "loss: 1.640231  [ 1079/60000]\n",
      "loss: 2.630159  [ 1080/60000]\n",
      "loss: 2.214708  [ 1081/60000]\n",
      "loss: 2.024731  [ 1082/60000]\n",
      "loss: 1.794742  [ 1083/60000]\n",
      "loss: 2.330028  [ 1084/60000]\n",
      "loss: 1.979344  [ 1085/60000]\n",
      "loss: 1.950844  [ 1086/60000]\n",
      "loss: 1.990315  [ 1087/60000]\n",
      "loss: 2.351541  [ 1088/60000]\n",
      "loss: 2.065033  [ 1089/60000]\n",
      "loss: 2.442364  [ 1090/60000]\n",
      "loss: 1.662253  [ 1091/60000]\n",
      "loss: 2.324521  [ 1092/60000]\n",
      "loss: 2.323914  [ 1093/60000]\n",
      "loss: 1.637047  [ 1094/60000]\n",
      "loss: 2.114686  [ 1095/60000]\n",
      "loss: 1.987878  [ 1096/60000]\n",
      "loss: 2.149244  [ 1097/60000]\n",
      "loss: 2.053241  [ 1098/60000]\n",
      "loss: 2.100171  [ 1099/60000]\n",
      "loss: 1.957683  [ 1100/60000]\n",
      "loss: 2.345141  [ 1101/60000]\n",
      "loss: 1.848684  [ 1102/60000]\n",
      "loss: 2.009772  [ 1103/60000]\n",
      "loss: 2.197469  [ 1104/60000]\n",
      "loss: 2.449652  [ 1105/60000]\n",
      "loss: 1.746067  [ 1106/60000]\n",
      "loss: 1.954138  [ 1107/60000]\n",
      "loss: 1.555501  [ 1108/60000]\n",
      "loss: 2.133962  [ 1109/60000]\n",
      "loss: 2.380438  [ 1110/60000]\n",
      "loss: 2.207757  [ 1111/60000]\n",
      "loss: 2.318578  [ 1112/60000]\n",
      "loss: 2.190382  [ 1113/60000]\n",
      "loss: 2.383087  [ 1114/60000]\n",
      "loss: 1.915886  [ 1115/60000]\n",
      "loss: 1.940968  [ 1116/60000]\n",
      "loss: 2.121995  [ 1117/60000]\n",
      "loss: 2.388193  [ 1118/60000]\n",
      "loss: 1.957502  [ 1119/60000]\n",
      "loss: 2.185480  [ 1120/60000]\n",
      "loss: 2.141711  [ 1121/60000]\n",
      "loss: 2.403387  [ 1122/60000]\n",
      "loss: 2.140777  [ 1123/60000]\n",
      "loss: 2.027006  [ 1124/60000]\n",
      "loss: 2.227624  [ 1125/60000]\n",
      "loss: 1.863313  [ 1126/60000]\n",
      "loss: 1.897418  [ 1127/60000]\n",
      "loss: 2.062398  [ 1128/60000]\n",
      "loss: 1.758352  [ 1129/60000]\n",
      "loss: 2.273302  [ 1130/60000]\n",
      "loss: 1.900504  [ 1131/60000]\n",
      "loss: 2.031993  [ 1132/60000]\n",
      "loss: 1.970769  [ 1133/60000]\n",
      "loss: 2.190638  [ 1134/60000]\n",
      "loss: 2.142274  [ 1135/60000]\n",
      "loss: 2.292889  [ 1136/60000]\n",
      "loss: 2.467950  [ 1137/60000]\n",
      "loss: 2.384475  [ 1138/60000]\n",
      "loss: 2.172812  [ 1139/60000]\n",
      "loss: 2.331678  [ 1140/60000]\n",
      "loss: 2.439174  [ 1141/60000]\n",
      "loss: 2.019264  [ 1142/60000]\n",
      "loss: 2.099057  [ 1143/60000]\n",
      "loss: 2.228542  [ 1144/60000]\n",
      "loss: 2.052012  [ 1145/60000]\n",
      "loss: 1.923445  [ 1146/60000]\n",
      "loss: 2.277497  [ 1147/60000]\n",
      "loss: 2.143239  [ 1148/60000]\n",
      "loss: 2.052490  [ 1149/60000]\n",
      "loss: 2.324796  [ 1150/60000]\n",
      "loss: 2.186419  [ 1151/60000]\n",
      "loss: 1.977901  [ 1152/60000]\n",
      "loss: 1.663555  [ 1153/60000]\n",
      "loss: 1.860935  [ 1154/60000]\n",
      "loss: 1.942622  [ 1155/60000]\n",
      "loss: 1.993481  [ 1156/60000]\n",
      "loss: 1.866431  [ 1157/60000]\n",
      "loss: 1.807080  [ 1158/60000]\n",
      "loss: 2.227659  [ 1159/60000]\n",
      "loss: 2.215822  [ 1160/60000]\n",
      "loss: 2.134940  [ 1161/60000]\n",
      "loss: 2.018162  [ 1162/60000]\n",
      "loss: 2.500475  [ 1163/60000]\n",
      "loss: 2.009519  [ 1164/60000]\n",
      "loss: 2.170177  [ 1165/60000]\n",
      "loss: 2.273133  [ 1166/60000]\n",
      "loss: 1.925289  [ 1167/60000]\n",
      "loss: 2.114530  [ 1168/60000]\n",
      "loss: 2.134191  [ 1169/60000]\n",
      "loss: 2.101116  [ 1170/60000]\n",
      "loss: 2.085097  [ 1171/60000]\n",
      "loss: 2.093495  [ 1172/60000]\n",
      "loss: 1.965257  [ 1173/60000]\n",
      "loss: 2.072068  [ 1174/60000]\n",
      "loss: 1.879135  [ 1175/60000]\n",
      "loss: 2.154386  [ 1176/60000]\n",
      "loss: 1.777457  [ 1177/60000]\n",
      "loss: 2.157443  [ 1178/60000]\n",
      "loss: 2.191069  [ 1179/60000]\n",
      "loss: 1.384120  [ 1180/60000]\n",
      "loss: 1.952787  [ 1181/60000]\n",
      "loss: 1.774226  [ 1182/60000]\n",
      "loss: 2.325354  [ 1183/60000]\n",
      "loss: 2.051147  [ 1184/60000]\n",
      "loss: 2.316347  [ 1185/60000]\n",
      "loss: 1.858022  [ 1186/60000]\n",
      "loss: 1.811137  [ 1187/60000]\n",
      "loss: 1.724775  [ 1188/60000]\n",
      "loss: 2.192995  [ 1189/60000]\n",
      "loss: 2.306578  [ 1190/60000]\n",
      "loss: 2.392458  [ 1191/60000]\n",
      "loss: 1.808504  [ 1192/60000]\n",
      "loss: 2.034828  [ 1193/60000]\n",
      "loss: 1.826054  [ 1194/60000]\n",
      "loss: 2.222754  [ 1195/60000]\n",
      "loss: 1.685573  [ 1196/60000]\n",
      "loss: 2.045004  [ 1197/60000]\n",
      "loss: 1.794275  [ 1198/60000]\n",
      "loss: 1.981339  [ 1199/60000]\n",
      "loss: 1.994375  [ 1200/60000]\n",
      "loss: 1.907452  [ 1201/60000]\n",
      "loss: 1.911363  [ 1202/60000]\n",
      "loss: 2.206648  [ 1203/60000]\n",
      "loss: 1.922189  [ 1204/60000]\n",
      "loss: 2.221818  [ 1205/60000]\n",
      "loss: 1.772688  [ 1206/60000]\n",
      "loss: 2.498790  [ 1207/60000]\n",
      "loss: 1.841791  [ 1208/60000]\n",
      "loss: 1.719735  [ 1209/60000]\n",
      "loss: 1.333708  [ 1210/60000]\n",
      "loss: 2.443779  [ 1211/60000]\n",
      "loss: 1.771319  [ 1212/60000]\n",
      "loss: 1.915481  [ 1213/60000]\n",
      "loss: 2.204935  [ 1214/60000]\n",
      "loss: 1.875977  [ 1215/60000]\n",
      "loss: 1.547840  [ 1216/60000]\n",
      "loss: 1.889626  [ 1217/60000]\n",
      "loss: 1.725060  [ 1218/60000]\n",
      "loss: 1.989070  [ 1219/60000]\n",
      "loss: 1.904389  [ 1220/60000]\n",
      "loss: 1.821878  [ 1221/60000]\n",
      "loss: 2.151272  [ 1222/60000]\n",
      "loss: 2.484570  [ 1223/60000]\n",
      "loss: 1.871265  [ 1224/60000]\n",
      "loss: 2.177968  [ 1225/60000]\n",
      "loss: 1.744122  [ 1226/60000]\n",
      "loss: 1.848745  [ 1227/60000]\n",
      "loss: 2.255095  [ 1228/60000]\n",
      "loss: 2.361472  [ 1229/60000]\n",
      "loss: 1.761398  [ 1230/60000]\n",
      "loss: 1.894785  [ 1231/60000]\n",
      "loss: 1.664602  [ 1232/60000]\n",
      "loss: 1.973418  [ 1233/60000]\n",
      "loss: 1.877717  [ 1234/60000]\n",
      "loss: 2.022929  [ 1235/60000]\n",
      "loss: 2.304764  [ 1236/60000]\n",
      "loss: 1.969450  [ 1237/60000]\n",
      "loss: 1.616849  [ 1238/60000]\n",
      "loss: 2.000036  [ 1239/60000]\n",
      "loss: 2.006031  [ 1240/60000]\n",
      "loss: 2.366174  [ 1241/60000]\n",
      "loss: 1.749328  [ 1242/60000]\n",
      "loss: 2.042327  [ 1243/60000]\n",
      "loss: 1.935430  [ 1244/60000]\n",
      "loss: 2.507194  [ 1245/60000]\n",
      "loss: 2.594454  [ 1246/60000]\n",
      "loss: 2.192437  [ 1247/60000]\n",
      "loss: 1.597356  [ 1248/60000]\n",
      "loss: 2.002958  [ 1249/60000]\n",
      "loss: 2.090993  [ 1250/60000]\n",
      "loss: 2.159353  [ 1251/60000]\n",
      "loss: 1.723676  [ 1252/60000]\n",
      "loss: 1.914069  [ 1253/60000]\n",
      "loss: 1.834033  [ 1254/60000]\n",
      "loss: 1.814794  [ 1255/60000]\n",
      "loss: 2.076846  [ 1256/60000]\n",
      "loss: 2.086981  [ 1257/60000]\n",
      "loss: 2.369714  [ 1258/60000]\n",
      "loss: 1.778007  [ 1259/60000]\n",
      "loss: 1.913064  [ 1260/60000]\n",
      "loss: 2.415537  [ 1261/60000]\n",
      "loss: 1.812848  [ 1262/60000]\n",
      "loss: 1.989711  [ 1263/60000]\n",
      "loss: 2.563225  [ 1264/60000]\n",
      "loss: 2.153947  [ 1265/60000]\n",
      "loss: 2.548024  [ 1266/60000]\n",
      "loss: 2.615291  [ 1267/60000]\n",
      "loss: 1.741313  [ 1268/60000]\n",
      "loss: 1.841196  [ 1269/60000]\n",
      "loss: 2.627146  [ 1270/60000]\n",
      "loss: 2.279635  [ 1271/60000]\n",
      "loss: 1.756152  [ 1272/60000]\n",
      "loss: 2.214839  [ 1273/60000]\n",
      "loss: 1.605981  [ 1274/60000]\n",
      "loss: 2.235241  [ 1275/60000]\n",
      "loss: 1.718168  [ 1276/60000]\n",
      "loss: 2.565272  [ 1277/60000]\n",
      "loss: 1.736628  [ 1278/60000]\n",
      "loss: 2.397421  [ 1279/60000]\n",
      "loss: 1.907673  [ 1280/60000]\n",
      "loss: 1.860005  [ 1281/60000]\n",
      "loss: 1.852361  [ 1282/60000]\n",
      "loss: 2.320993  [ 1283/60000]\n",
      "loss: 2.340705  [ 1284/60000]\n",
      "loss: 2.078028  [ 1285/60000]\n",
      "loss: 2.306892  [ 1286/60000]\n",
      "loss: 2.063379  [ 1287/60000]\n",
      "loss: 1.886679  [ 1288/60000]\n",
      "loss: 2.437230  [ 1289/60000]\n",
      "loss: 1.719177  [ 1290/60000]\n",
      "loss: 1.988618  [ 1291/60000]\n",
      "loss: 2.097320  [ 1292/60000]\n",
      "loss: 2.056512  [ 1293/60000]\n",
      "loss: 1.797705  [ 1294/60000]\n",
      "loss: 1.892593  [ 1295/60000]\n",
      "loss: 1.866445  [ 1296/60000]\n",
      "loss: 1.619005  [ 1297/60000]\n",
      "loss: 2.261200  [ 1298/60000]\n",
      "loss: 2.558752  [ 1299/60000]\n",
      "loss: 2.246211  [ 1300/60000]\n",
      "loss: 1.807153  [ 1301/60000]\n",
      "loss: 2.308295  [ 1302/60000]\n",
      "loss: 1.864370  [ 1303/60000]\n",
      "loss: 2.169614  [ 1304/60000]\n",
      "loss: 1.806033  [ 1305/60000]\n",
      "loss: 1.995816  [ 1306/60000]\n",
      "loss: 1.857466  [ 1307/60000]\n",
      "loss: 2.033141  [ 1308/60000]\n",
      "loss: 1.902626  [ 1309/60000]\n",
      "loss: 2.293392  [ 1310/60000]\n",
      "loss: 1.798984  [ 1311/60000]\n",
      "loss: 2.265769  [ 1312/60000]\n",
      "loss: 1.970264  [ 1313/60000]\n",
      "loss: 2.462458  [ 1314/60000]\n",
      "loss: 1.864412  [ 1315/60000]\n",
      "loss: 1.663580  [ 1316/60000]\n",
      "loss: 1.992568  [ 1317/60000]\n",
      "loss: 1.772472  [ 1318/60000]\n",
      "loss: 2.271821  [ 1319/60000]\n",
      "loss: 1.961146  [ 1320/60000]\n",
      "loss: 1.831059  [ 1321/60000]\n",
      "loss: 1.750962  [ 1322/60000]\n",
      "loss: 1.726325  [ 1323/60000]\n",
      "loss: 2.064568  [ 1324/60000]\n",
      "loss: 2.379472  [ 1325/60000]\n",
      "loss: 2.248844  [ 1326/60000]\n",
      "loss: 1.792988  [ 1327/60000]\n",
      "loss: 2.100034  [ 1328/60000]\n",
      "loss: 2.492372  [ 1329/60000]\n",
      "loss: 1.606477  [ 1330/60000]\n",
      "loss: 2.116663  [ 1331/60000]\n",
      "loss: 1.677113  [ 1332/60000]\n",
      "loss: 1.713454  [ 1333/60000]\n",
      "loss: 2.086643  [ 1334/60000]\n",
      "loss: 2.207412  [ 1335/60000]\n",
      "loss: 1.764521  [ 1336/60000]\n",
      "loss: 2.130119  [ 1337/60000]\n",
      "loss: 2.240874  [ 1338/60000]\n",
      "loss: 1.721619  [ 1339/60000]\n",
      "loss: 1.678190  [ 1340/60000]\n",
      "loss: 2.526916  [ 1341/60000]\n",
      "loss: 1.967954  [ 1342/60000]\n",
      "loss: 1.943256  [ 1343/60000]\n",
      "loss: 1.751007  [ 1344/60000]\n",
      "loss: 2.014017  [ 1345/60000]\n",
      "loss: 1.656416  [ 1346/60000]\n",
      "loss: 1.686968  [ 1347/60000]\n",
      "loss: 1.738892  [ 1348/60000]\n",
      "loss: 2.195395  [ 1349/60000]\n",
      "loss: 1.984969  [ 1350/60000]\n",
      "loss: 2.323366  [ 1351/60000]\n",
      "loss: 1.773534  [ 1352/60000]\n",
      "loss: 2.581710  [ 1353/60000]\n",
      "loss: 1.688881  [ 1354/60000]\n",
      "loss: 1.965909  [ 1355/60000]\n",
      "loss: 2.028743  [ 1356/60000]\n",
      "loss: 2.621996  [ 1357/60000]\n",
      "loss: 1.841929  [ 1358/60000]\n",
      "loss: 2.211358  [ 1359/60000]\n",
      "loss: 1.761213  [ 1360/60000]\n",
      "loss: 2.068170  [ 1361/60000]\n",
      "loss: 2.379785  [ 1362/60000]\n",
      "loss: 1.819658  [ 1363/60000]\n",
      "loss: 1.443939  [ 1364/60000]\n",
      "loss: 2.310071  [ 1365/60000]\n",
      "loss: 1.738757  [ 1366/60000]\n",
      "loss: 2.282015  [ 1367/60000]\n",
      "loss: 1.242153  [ 1368/60000]\n",
      "loss: 1.502208  [ 1369/60000]\n",
      "loss: 1.696637  [ 1370/60000]\n",
      "loss: 2.162142  [ 1371/60000]\n",
      "loss: 1.244901  [ 1372/60000]\n",
      "loss: 1.226988  [ 1373/60000]\n",
      "loss: 1.841301  [ 1374/60000]\n",
      "loss: 2.275704  [ 1375/60000]\n",
      "loss: 1.902030  [ 1376/60000]\n",
      "loss: 2.134967  [ 1377/60000]\n",
      "loss: 2.014729  [ 1378/60000]\n",
      "loss: 2.101582  [ 1379/60000]\n",
      "loss: 1.686657  [ 1380/60000]\n",
      "loss: 2.039073  [ 1381/60000]\n",
      "loss: 1.699966  [ 1382/60000]\n",
      "loss: 2.676714  [ 1383/60000]\n",
      "loss: 1.986623  [ 1384/60000]\n",
      "loss: 1.783111  [ 1385/60000]\n",
      "loss: 2.227423  [ 1386/60000]\n",
      "loss: 1.488552  [ 1387/60000]\n",
      "loss: 2.094683  [ 1388/60000]\n",
      "loss: 1.668958  [ 1389/60000]\n",
      "loss: 2.316302  [ 1390/60000]\n",
      "loss: 2.279171  [ 1391/60000]\n",
      "loss: 2.077372  [ 1392/60000]\n",
      "loss: 1.756329  [ 1393/60000]\n",
      "loss: 2.191991  [ 1394/60000]\n",
      "loss: 1.866782  [ 1395/60000]\n",
      "loss: 2.054353  [ 1396/60000]\n",
      "loss: 2.012285  [ 1397/60000]\n",
      "loss: 1.953026  [ 1398/60000]\n",
      "loss: 2.329865  [ 1399/60000]\n",
      "loss: 2.011606  [ 1400/60000]\n",
      "loss: 1.795830  [ 1401/60000]\n",
      "loss: 2.064515  [ 1402/60000]\n",
      "loss: 2.607343  [ 1403/60000]\n",
      "loss: 1.308360  [ 1404/60000]\n",
      "loss: 2.069369  [ 1405/60000]\n",
      "loss: 1.757004  [ 1406/60000]\n",
      "loss: 2.515845  [ 1407/60000]\n",
      "loss: 2.100901  [ 1408/60000]\n",
      "loss: 1.861973  [ 1409/60000]\n",
      "loss: 1.789509  [ 1410/60000]\n",
      "loss: 1.993140  [ 1411/60000]\n",
      "loss: 2.155035  [ 1412/60000]\n",
      "loss: 1.991258  [ 1413/60000]\n",
      "loss: 2.401879  [ 1414/60000]\n",
      "loss: 2.449685  [ 1415/60000]\n",
      "loss: 2.141866  [ 1416/60000]\n",
      "loss: 1.853408  [ 1417/60000]\n",
      "loss: 1.679297  [ 1418/60000]\n",
      "loss: 2.083530  [ 1419/60000]\n",
      "loss: 1.833247  [ 1420/60000]\n",
      "loss: 2.472652  [ 1421/60000]\n",
      "loss: 2.110163  [ 1422/60000]\n",
      "loss: 1.823023  [ 1423/60000]\n",
      "loss: 1.725862  [ 1424/60000]\n",
      "loss: 2.230524  [ 1425/60000]\n",
      "loss: 1.724310  [ 1426/60000]\n",
      "loss: 2.093062  [ 1427/60000]\n",
      "loss: 2.056359  [ 1428/60000]\n",
      "loss: 2.092767  [ 1429/60000]\n",
      "loss: 1.832929  [ 1430/60000]\n",
      "loss: 2.637562  [ 1431/60000]\n",
      "loss: 1.691769  [ 1432/60000]\n",
      "loss: 1.831029  [ 1433/60000]\n",
      "loss: 2.397736  [ 1434/60000]\n",
      "loss: 1.950144  [ 1435/60000]\n",
      "loss: 2.257601  [ 1436/60000]\n",
      "loss: 1.882242  [ 1437/60000]\n",
      "loss: 1.673574  [ 1438/60000]\n",
      "loss: 2.069190  [ 1439/60000]\n",
      "loss: 1.980920  [ 1440/60000]\n",
      "loss: 2.055619  [ 1441/60000]\n",
      "loss: 2.166409  [ 1442/60000]\n",
      "loss: 2.351145  [ 1443/60000]\n",
      "loss: 1.396423  [ 1444/60000]\n",
      "loss: 2.214829  [ 1445/60000]\n",
      "loss: 1.702369  [ 1446/60000]\n",
      "loss: 1.960496  [ 1447/60000]\n",
      "loss: 2.246974  [ 1448/60000]\n",
      "loss: 1.649873  [ 1449/60000]\n",
      "loss: 1.924103  [ 1450/60000]\n",
      "loss: 1.818603  [ 1451/60000]\n",
      "loss: 1.719176  [ 1452/60000]\n",
      "loss: 2.079221  [ 1453/60000]\n",
      "loss: 1.952161  [ 1454/60000]\n",
      "loss: 1.911825  [ 1455/60000]\n",
      "loss: 2.251405  [ 1456/60000]\n",
      "loss: 2.031307  [ 1457/60000]\n",
      "loss: 1.870311  [ 1458/60000]\n",
      "loss: 1.851421  [ 1459/60000]\n",
      "loss: 2.011968  [ 1460/60000]\n",
      "loss: 1.982603  [ 1461/60000]\n",
      "loss: 2.117229  [ 1462/60000]\n",
      "loss: 1.980575  [ 1463/60000]\n",
      "loss: 1.914332  [ 1464/60000]\n",
      "loss: 1.761187  [ 1465/60000]\n",
      "loss: 2.130416  [ 1466/60000]\n",
      "loss: 2.437422  [ 1467/60000]\n",
      "loss: 2.286338  [ 1468/60000]\n",
      "loss: 2.458222  [ 1469/60000]\n",
      "loss: 1.972430  [ 1470/60000]\n",
      "loss: 2.026830  [ 1471/60000]\n",
      "loss: 1.262829  [ 1472/60000]\n",
      "loss: 1.892031  [ 1473/60000]\n",
      "loss: 2.434141  [ 1474/60000]\n",
      "loss: 1.942084  [ 1475/60000]\n",
      "loss: 1.889159  [ 1476/60000]\n",
      "loss: 1.931236  [ 1477/60000]\n",
      "loss: 2.322942  [ 1478/60000]\n",
      "loss: 2.190219  [ 1479/60000]\n",
      "loss: 1.161770  [ 1480/60000]\n",
      "loss: 1.754859  [ 1481/60000]\n",
      "loss: 2.310109  [ 1482/60000]\n",
      "loss: 2.517537  [ 1483/60000]\n",
      "loss: 1.804014  [ 1484/60000]\n",
      "loss: 1.731716  [ 1485/60000]\n",
      "loss: 2.400632  [ 1486/60000]\n",
      "loss: 1.964527  [ 1487/60000]\n",
      "loss: 1.844585  [ 1488/60000]\n",
      "loss: 2.154920  [ 1489/60000]\n",
      "loss: 1.108654  [ 1490/60000]\n",
      "loss: 2.401697  [ 1491/60000]\n",
      "loss: 1.980024  [ 1492/60000]\n",
      "loss: 2.381685  [ 1493/60000]\n",
      "loss: 2.073814  [ 1494/60000]\n",
      "loss: 2.234895  [ 1495/60000]\n",
      "loss: 1.251984  [ 1496/60000]\n",
      "loss: 1.773344  [ 1497/60000]\n",
      "loss: 1.704103  [ 1498/60000]\n",
      "loss: 2.113661  [ 1499/60000]\n",
      "loss: 1.785170  [ 1500/60000]\n",
      "loss: 2.075871  [ 1501/60000]\n",
      "loss: 1.388062  [ 1502/60000]\n",
      "loss: 1.305314  [ 1503/60000]\n",
      "loss: 2.235868  [ 1504/60000]\n",
      "loss: 2.165640  [ 1505/60000]\n",
      "loss: 1.838934  [ 1506/60000]\n",
      "loss: 2.282405  [ 1507/60000]\n",
      "loss: 1.966748  [ 1508/60000]\n",
      "loss: 1.785948  [ 1509/60000]\n",
      "loss: 2.579191  [ 1510/60000]\n",
      "loss: 2.252577  [ 1511/60000]\n",
      "loss: 1.784853  [ 1512/60000]\n",
      "loss: 2.839929  [ 1513/60000]\n",
      "loss: 1.753290  [ 1514/60000]\n",
      "loss: 2.482003  [ 1515/60000]\n",
      "loss: 2.236440  [ 1516/60000]\n",
      "loss: 2.501493  [ 1517/60000]\n",
      "loss: 0.988330  [ 1518/60000]\n",
      "loss: 2.119819  [ 1519/60000]\n",
      "loss: 1.790859  [ 1520/60000]\n",
      "loss: 2.217889  [ 1521/60000]\n",
      "loss: 1.837724  [ 1522/60000]\n",
      "loss: 2.054542  [ 1523/60000]\n",
      "loss: 1.779833  [ 1524/60000]\n",
      "loss: 2.496617  [ 1525/60000]\n",
      "loss: 1.801178  [ 1526/60000]\n",
      "loss: 2.570714  [ 1527/60000]\n",
      "loss: 1.799315  [ 1528/60000]\n",
      "loss: 1.850065  [ 1529/60000]\n",
      "loss: 2.007389  [ 1530/60000]\n",
      "loss: 1.645158  [ 1531/60000]\n",
      "loss: 2.327793  [ 1532/60000]\n",
      "loss: 1.456675  [ 1533/60000]\n",
      "loss: 1.844741  [ 1534/60000]\n",
      "loss: 1.941960  [ 1535/60000]\n",
      "loss: 2.015211  [ 1536/60000]\n",
      "loss: 2.279650  [ 1537/60000]\n",
      "loss: 1.751560  [ 1538/60000]\n",
      "loss: 2.639530  [ 1539/60000]\n",
      "loss: 2.314925  [ 1540/60000]\n",
      "loss: 2.163173  [ 1541/60000]\n",
      "loss: 1.909700  [ 1542/60000]\n",
      "loss: 1.778712  [ 1543/60000]\n",
      "loss: 1.690442  [ 1544/60000]\n",
      "loss: 2.006847  [ 1545/60000]\n",
      "loss: 2.271026  [ 1546/60000]\n",
      "loss: 1.668126  [ 1547/60000]\n",
      "loss: 1.867998  [ 1548/60000]\n",
      "loss: 1.968228  [ 1549/60000]\n",
      "loss: 2.120789  [ 1550/60000]\n",
      "loss: 1.783512  [ 1551/60000]\n",
      "loss: 1.769380  [ 1552/60000]\n",
      "loss: 1.997824  [ 1553/60000]\n",
      "loss: 2.127383  [ 1554/60000]\n",
      "loss: 2.572056  [ 1555/60000]\n",
      "loss: 1.987174  [ 1556/60000]\n",
      "loss: 2.508224  [ 1557/60000]\n",
      "loss: 2.342099  [ 1558/60000]\n",
      "loss: 1.749363  [ 1559/60000]\n",
      "loss: 2.099591  [ 1560/60000]\n",
      "loss: 2.109889  [ 1561/60000]\n",
      "loss: 2.276643  [ 1562/60000]\n",
      "loss: 2.200442  [ 1563/60000]\n",
      "loss: 2.064053  [ 1564/60000]\n",
      "loss: 1.742831  [ 1565/60000]\n",
      "loss: 1.807132  [ 1566/60000]\n",
      "loss: 1.682484  [ 1567/60000]\n",
      "loss: 2.054539  [ 1568/60000]\n",
      "loss: 1.935182  [ 1569/60000]\n",
      "loss: 2.065332  [ 1570/60000]\n",
      "loss: 1.812380  [ 1571/60000]\n",
      "loss: 1.251052  [ 1572/60000]\n",
      "loss: 2.227949  [ 1573/60000]\n",
      "loss: 2.453236  [ 1574/60000]\n",
      "loss: 2.076056  [ 1575/60000]\n",
      "loss: 2.249628  [ 1576/60000]\n",
      "loss: 2.085432  [ 1577/60000]\n",
      "loss: 2.374703  [ 1578/60000]\n",
      "loss: 1.384971  [ 1579/60000]\n",
      "loss: 1.692205  [ 1580/60000]\n",
      "loss: 1.685839  [ 1581/60000]\n",
      "loss: 1.741492  [ 1582/60000]\n",
      "loss: 2.180683  [ 1583/60000]\n",
      "loss: 1.559783  [ 1584/60000]\n",
      "loss: 2.139871  [ 1585/60000]\n",
      "loss: 2.103962  [ 1586/60000]\n",
      "loss: 1.904242  [ 1587/60000]\n",
      "loss: 2.330224  [ 1588/60000]\n",
      "loss: 1.896435  [ 1589/60000]\n",
      "loss: 1.812524  [ 1590/60000]\n",
      "loss: 1.675700  [ 1591/60000]\n",
      "loss: 2.113775  [ 1592/60000]\n",
      "loss: 2.403060  [ 1593/60000]\n",
      "loss: 1.940933  [ 1594/60000]\n",
      "loss: 1.828024  [ 1595/60000]\n",
      "loss: 2.392883  [ 1596/60000]\n",
      "loss: 1.494176  [ 1597/60000]\n",
      "loss: 1.840260  [ 1598/60000]\n",
      "loss: 1.805993  [ 1599/60000]\n",
      "loss: 1.714321  [ 1600/60000]\n",
      "loss: 2.231993  [ 1601/60000]\n",
      "loss: 1.787700  [ 1602/60000]\n",
      "loss: 2.031589  [ 1603/60000]\n",
      "loss: 1.517737  [ 1604/60000]\n",
      "loss: 1.828194  [ 1605/60000]\n",
      "loss: 1.197567  [ 1606/60000]\n",
      "loss: 1.389518  [ 1607/60000]\n",
      "loss: 1.833485  [ 1608/60000]\n",
      "loss: 2.072316  [ 1609/60000]\n",
      "loss: 1.906535  [ 1610/60000]\n",
      "loss: 2.217285  [ 1611/60000]\n",
      "loss: 1.777509  [ 1612/60000]\n",
      "loss: 2.046467  [ 1613/60000]\n",
      "loss: 1.823450  [ 1614/60000]\n",
      "loss: 2.291054  [ 1615/60000]\n",
      "loss: 2.344694  [ 1616/60000]\n",
      "loss: 1.984170  [ 1617/60000]\n",
      "loss: 2.101962  [ 1618/60000]\n",
      "loss: 1.966222  [ 1619/60000]\n",
      "loss: 1.827554  [ 1620/60000]\n",
      "loss: 2.212915  [ 1621/60000]\n",
      "loss: 1.704241  [ 1622/60000]\n",
      "loss: 2.140989  [ 1623/60000]\n",
      "loss: 2.090113  [ 1624/60000]\n",
      "loss: 1.858524  [ 1625/60000]\n",
      "loss: 1.289345  [ 1626/60000]\n",
      "loss: 1.384887  [ 1627/60000]\n",
      "loss: 1.777536  [ 1628/60000]\n",
      "loss: 1.696046  [ 1629/60000]\n",
      "loss: 1.862464  [ 1630/60000]\n",
      "loss: 1.839286  [ 1631/60000]\n",
      "loss: 1.592853  [ 1632/60000]\n",
      "loss: 2.002531  [ 1633/60000]\n",
      "loss: 1.844549  [ 1634/60000]\n",
      "loss: 1.585381  [ 1635/60000]\n",
      "loss: 2.396659  [ 1636/60000]\n",
      "loss: 1.662221  [ 1637/60000]\n",
      "loss: 2.057836  [ 1638/60000]\n",
      "loss: 2.582091  [ 1639/60000]\n",
      "loss: 1.768703  [ 1640/60000]\n",
      "loss: 1.831981  [ 1641/60000]\n",
      "loss: 1.660376  [ 1642/60000]\n",
      "loss: 2.371005  [ 1643/60000]\n",
      "loss: 1.899808  [ 1644/60000]\n",
      "loss: 2.063819  [ 1645/60000]\n",
      "loss: 1.251744  [ 1646/60000]\n",
      "loss: 1.588845  [ 1647/60000]\n",
      "loss: 1.777862  [ 1648/60000]\n",
      "loss: 2.116826  [ 1649/60000]\n",
      "loss: 1.897563  [ 1650/60000]\n",
      "loss: 2.468125  [ 1651/60000]\n",
      "loss: 1.742150  [ 1652/60000]\n",
      "loss: 1.981040  [ 1653/60000]\n",
      "loss: 1.637246  [ 1654/60000]\n",
      "loss: 1.961352  [ 1655/60000]\n",
      "loss: 1.788510  [ 1656/60000]\n",
      "loss: 1.999305  [ 1657/60000]\n",
      "loss: 1.570984  [ 1658/60000]\n",
      "loss: 1.768757  [ 1659/60000]\n",
      "loss: 1.908314  [ 1660/60000]\n",
      "loss: 2.156683  [ 1661/60000]\n",
      "loss: 1.771901  [ 1662/60000]\n",
      "loss: 2.222209  [ 1663/60000]\n",
      "loss: 1.881323  [ 1664/60000]\n",
      "loss: 1.026406  [ 1665/60000]\n",
      "loss: 1.861691  [ 1666/60000]\n",
      "loss: 1.906910  [ 1667/60000]\n",
      "loss: 1.954223  [ 1668/60000]\n",
      "loss: 1.778269  [ 1669/60000]\n",
      "loss: 2.051640  [ 1670/60000]\n",
      "loss: 2.467412  [ 1671/60000]\n",
      "loss: 2.085018  [ 1672/60000]\n",
      "loss: 1.787781  [ 1673/60000]\n",
      "loss: 1.705202  [ 1674/60000]\n",
      "loss: 2.623591  [ 1675/60000]\n",
      "loss: 1.538261  [ 1676/60000]\n",
      "loss: 2.082331  [ 1677/60000]\n",
      "loss: 1.630516  [ 1678/60000]\n",
      "loss: 0.946988  [ 1679/60000]\n",
      "loss: 2.082636  [ 1680/60000]\n",
      "loss: 1.727433  [ 1681/60000]\n",
      "loss: 1.711567  [ 1682/60000]\n",
      "loss: 1.481806  [ 1683/60000]\n",
      "loss: 2.140571  [ 1684/60000]\n",
      "loss: 1.898665  [ 1685/60000]\n",
      "loss: 1.720044  [ 1686/60000]\n",
      "loss: 2.196878  [ 1687/60000]\n",
      "loss: 1.717052  [ 1688/60000]\n",
      "loss: 2.422590  [ 1689/60000]\n",
      "loss: 1.706796  [ 1690/60000]\n",
      "loss: 2.541014  [ 1691/60000]\n",
      "loss: 2.353925  [ 1692/60000]\n",
      "loss: 1.491483  [ 1693/60000]\n",
      "loss: 1.672806  [ 1694/60000]\n",
      "loss: 2.178237  [ 1695/60000]\n",
      "loss: 1.948996  [ 1696/60000]\n",
      "loss: 1.927629  [ 1697/60000]\n",
      "loss: 1.797961  [ 1698/60000]\n",
      "loss: 1.478344  [ 1699/60000]\n",
      "loss: 1.782517  [ 1700/60000]\n",
      "loss: 1.818370  [ 1701/60000]\n",
      "loss: 1.015674  [ 1702/60000]\n",
      "loss: 2.607641  [ 1703/60000]\n",
      "loss: 1.738381  [ 1704/60000]\n",
      "loss: 2.408273  [ 1705/60000]\n",
      "loss: 1.877434  [ 1706/60000]\n",
      "loss: 1.988580  [ 1707/60000]\n",
      "loss: 1.990514  [ 1708/60000]\n",
      "loss: 2.089165  [ 1709/60000]\n",
      "loss: 0.931493  [ 1710/60000]\n",
      "loss: 1.826357  [ 1711/60000]\n",
      "loss: 1.920507  [ 1712/60000]\n",
      "loss: 1.265801  [ 1713/60000]\n",
      "loss: 1.726249  [ 1714/60000]\n",
      "loss: 2.192769  [ 1715/60000]\n",
      "loss: 1.912930  [ 1716/60000]\n",
      "loss: 2.049284  [ 1717/60000]\n",
      "loss: 1.694374  [ 1718/60000]\n",
      "loss: 1.997063  [ 1719/60000]\n",
      "loss: 1.792274  [ 1720/60000]\n",
      "loss: 1.944063  [ 1721/60000]\n",
      "loss: 2.454452  [ 1722/60000]\n",
      "loss: 1.610804  [ 1723/60000]\n",
      "loss: 1.049734  [ 1724/60000]\n",
      "loss: 2.003662  [ 1725/60000]\n",
      "loss: 1.179528  [ 1726/60000]\n",
      "loss: 2.093503  [ 1727/60000]\n",
      "loss: 1.825474  [ 1728/60000]\n",
      "loss: 1.943155  [ 1729/60000]\n",
      "loss: 1.228227  [ 1730/60000]\n",
      "loss: 1.768372  [ 1731/60000]\n",
      "loss: 1.972973  [ 1732/60000]\n",
      "loss: 2.071507  [ 1733/60000]\n",
      "loss: 1.905589  [ 1734/60000]\n",
      "loss: 1.643887  [ 1735/60000]\n",
      "loss: 1.931167  [ 1736/60000]\n",
      "loss: 1.979528  [ 1737/60000]\n",
      "loss: 1.702965  [ 1738/60000]\n",
      "loss: 1.696188  [ 1739/60000]\n",
      "loss: 1.697507  [ 1740/60000]\n",
      "loss: 2.104725  [ 1741/60000]\n",
      "loss: 2.142906  [ 1742/60000]\n",
      "loss: 1.136844  [ 1743/60000]\n",
      "loss: 1.890400  [ 1744/60000]\n",
      "loss: 2.248342  [ 1745/60000]\n",
      "loss: 1.671212  [ 1746/60000]\n",
      "loss: 1.641307  [ 1747/60000]\n",
      "loss: 1.771972  [ 1748/60000]\n",
      "loss: 1.958619  [ 1749/60000]\n",
      "loss: 1.697515  [ 1750/60000]\n",
      "loss: 2.481322  [ 1751/60000]\n",
      "loss: 1.874285  [ 1752/60000]\n",
      "loss: 2.551873  [ 1753/60000]\n",
      "loss: 1.914392  [ 1754/60000]\n",
      "loss: 2.052089  [ 1755/60000]\n",
      "loss: 1.897353  [ 1756/60000]\n",
      "loss: 1.622438  [ 1757/60000]\n",
      "loss: 1.902485  [ 1758/60000]\n",
      "loss: 1.879106  [ 1759/60000]\n",
      "loss: 1.864726  [ 1760/60000]\n",
      "loss: 1.865418  [ 1761/60000]\n",
      "loss: 2.061534  [ 1762/60000]\n",
      "loss: 1.804751  [ 1763/60000]\n",
      "loss: 1.866294  [ 1764/60000]\n",
      "loss: 1.644373  [ 1765/60000]\n",
      "loss: 1.898272  [ 1766/60000]\n",
      "loss: 2.028658  [ 1767/60000]\n",
      "loss: 1.854442  [ 1768/60000]\n",
      "loss: 2.283053  [ 1769/60000]\n",
      "loss: 1.242579  [ 1770/60000]\n",
      "loss: 1.825836  [ 1771/60000]\n",
      "loss: 1.077537  [ 1772/60000]\n",
      "loss: 1.776192  [ 1773/60000]\n",
      "loss: 1.703345  [ 1774/60000]\n",
      "loss: 1.944903  [ 1775/60000]\n",
      "loss: 0.936999  [ 1776/60000]\n",
      "loss: 1.939351  [ 1777/60000]\n",
      "loss: 1.622418  [ 1778/60000]\n",
      "loss: 1.905670  [ 1779/60000]\n",
      "loss: 2.028034  [ 1780/60000]\n",
      "loss: 1.550417  [ 1781/60000]\n",
      "loss: 1.989131  [ 1782/60000]\n",
      "loss: 2.277823  [ 1783/60000]\n",
      "loss: 1.541116  [ 1784/60000]\n",
      "loss: 2.508018  [ 1785/60000]\n",
      "loss: 2.358391  [ 1786/60000]\n",
      "loss: 2.069026  [ 1787/60000]\n",
      "loss: 1.704510  [ 1788/60000]\n",
      "loss: 1.675226  [ 1789/60000]\n",
      "loss: 1.796802  [ 1790/60000]\n",
      "loss: 1.745250  [ 1791/60000]\n",
      "loss: 1.589202  [ 1792/60000]\n",
      "loss: 2.205664  [ 1793/60000]\n",
      "loss: 1.608207  [ 1794/60000]\n",
      "loss: 1.829095  [ 1795/60000]\n",
      "loss: 2.576272  [ 1796/60000]\n",
      "loss: 1.569482  [ 1797/60000]\n",
      "loss: 1.255120  [ 1798/60000]\n",
      "loss: 1.402932  [ 1799/60000]\n",
      "loss: 1.669696  [ 1800/60000]\n",
      "loss: 2.145934  [ 1801/60000]\n",
      "loss: 2.078963  [ 1802/60000]\n",
      "loss: 2.053203  [ 1803/60000]\n",
      "loss: 1.787387  [ 1804/60000]\n",
      "loss: 1.878011  [ 1805/60000]\n",
      "loss: 1.624955  [ 1806/60000]\n",
      "loss: 2.621810  [ 1807/60000]\n",
      "loss: 1.679194  [ 1808/60000]\n",
      "loss: 1.805718  [ 1809/60000]\n",
      "loss: 1.386299  [ 1810/60000]\n",
      "loss: 2.464427  [ 1811/60000]\n",
      "loss: 1.814574  [ 1812/60000]\n",
      "loss: 2.407480  [ 1813/60000]\n",
      "loss: 2.333473  [ 1814/60000]\n",
      "loss: 2.192383  [ 1815/60000]\n",
      "loss: 1.830320  [ 1816/60000]\n",
      "loss: 1.804750  [ 1817/60000]\n",
      "loss: 2.417502  [ 1818/60000]\n",
      "loss: 2.060129  [ 1819/60000]\n",
      "loss: 1.044785  [ 1820/60000]\n",
      "loss: 2.105244  [ 1821/60000]\n",
      "loss: 1.912226  [ 1822/60000]\n",
      "loss: 1.817234  [ 1823/60000]\n",
      "loss: 2.024467  [ 1824/60000]\n",
      "loss: 2.502416  [ 1825/60000]\n",
      "loss: 1.794997  [ 1826/60000]\n",
      "loss: 1.933254  [ 1827/60000]\n",
      "loss: 1.687368  [ 1828/60000]\n",
      "loss: 0.844443  [ 1829/60000]\n",
      "loss: 1.683287  [ 1830/60000]\n",
      "loss: 1.932329  [ 1831/60000]\n",
      "loss: 1.670766  [ 1832/60000]\n",
      "loss: 1.930948  [ 1833/60000]\n",
      "loss: 1.631553  [ 1834/60000]\n",
      "loss: 2.669508  [ 1835/60000]\n",
      "loss: 2.121286  [ 1836/60000]\n",
      "loss: 1.784783  [ 1837/60000]\n",
      "loss: 0.782723  [ 1838/60000]\n",
      "loss: 1.779220  [ 1839/60000]\n",
      "loss: 1.597793  [ 1840/60000]\n",
      "loss: 1.990068  [ 1841/60000]\n",
      "loss: 1.989848  [ 1842/60000]\n",
      "loss: 1.831782  [ 1843/60000]\n",
      "loss: 1.199898  [ 1844/60000]\n",
      "loss: 1.969661  [ 1845/60000]\n",
      "loss: 1.394526  [ 1846/60000]\n",
      "loss: 2.095340  [ 1847/60000]\n",
      "loss: 2.560385  [ 1848/60000]\n",
      "loss: 1.629139  [ 1849/60000]\n",
      "loss: 2.025805  [ 1850/60000]\n",
      "loss: 2.045574  [ 1851/60000]\n",
      "loss: 1.832712  [ 1852/60000]\n",
      "loss: 2.483070  [ 1853/60000]\n",
      "loss: 1.533065  [ 1854/60000]\n",
      "loss: 1.944029  [ 1855/60000]\n",
      "loss: 1.939075  [ 1856/60000]\n",
      "loss: 1.780052  [ 1857/60000]\n",
      "loss: 0.825632  [ 1858/60000]\n",
      "loss: 1.774183  [ 1859/60000]\n",
      "loss: 1.604790  [ 1860/60000]\n",
      "loss: 1.719116  [ 1861/60000]\n",
      "loss: 2.130238  [ 1862/60000]\n",
      "loss: 1.563303  [ 1863/60000]\n",
      "loss: 2.025816  [ 1864/60000]\n",
      "loss: 2.415414  [ 1865/60000]\n",
      "loss: 1.648798  [ 1866/60000]\n",
      "loss: 2.118500  [ 1867/60000]\n",
      "loss: 2.583442  [ 1868/60000]\n",
      "loss: 0.984303  [ 1869/60000]\n",
      "loss: 2.038661  [ 1870/60000]\n",
      "loss: 1.889724  [ 1871/60000]\n",
      "loss: 1.694439  [ 1872/60000]\n",
      "loss: 1.669177  [ 1873/60000]\n",
      "loss: 2.001940  [ 1874/60000]\n",
      "loss: 1.839129  [ 1875/60000]\n",
      "loss: 2.258970  [ 1876/60000]\n",
      "loss: 1.773131  [ 1877/60000]\n",
      "loss: 1.100005  [ 1878/60000]\n",
      "loss: 1.909716  [ 1879/60000]\n",
      "loss: 1.583194  [ 1880/60000]\n",
      "loss: 1.993947  [ 1881/60000]\n",
      "loss: 2.395978  [ 1882/60000]\n",
      "loss: 1.912242  [ 1883/60000]\n",
      "loss: 1.133087  [ 1884/60000]\n",
      "loss: 1.873501  [ 1885/60000]\n",
      "loss: 1.399068  [ 1886/60000]\n",
      "loss: 2.172606  [ 1887/60000]\n",
      "loss: 2.709834  [ 1888/60000]\n",
      "loss: 2.276270  [ 1889/60000]\n",
      "loss: 2.134003  [ 1890/60000]\n",
      "loss: 2.098546  [ 1891/60000]\n",
      "loss: 1.519802  [ 1892/60000]\n",
      "loss: 2.088803  [ 1893/60000]\n",
      "loss: 1.594053  [ 1894/60000]\n",
      "loss: 1.824428  [ 1895/60000]\n",
      "loss: 1.926963  [ 1896/60000]\n",
      "loss: 2.060028  [ 1897/60000]\n",
      "loss: 0.829643  [ 1898/60000]\n",
      "loss: 2.039562  [ 1899/60000]\n",
      "loss: 1.754187  [ 1900/60000]\n",
      "loss: 2.020865  [ 1901/60000]\n",
      "loss: 1.634431  [ 1902/60000]\n",
      "loss: 2.119986  [ 1903/60000]\n",
      "loss: 1.893284  [ 1904/60000]\n",
      "loss: 1.972813  [ 1905/60000]\n",
      "loss: 1.931291  [ 1906/60000]\n",
      "loss: 2.330231  [ 1907/60000]\n",
      "loss: 0.814761  [ 1908/60000]\n",
      "loss: 2.030526  [ 1909/60000]\n",
      "loss: 1.503618  [ 1910/60000]\n",
      "loss: 1.675467  [ 1911/60000]\n",
      "loss: 1.460588  [ 1912/60000]\n",
      "loss: 1.964604  [ 1913/60000]\n",
      "loss: 2.588392  [ 1914/60000]\n",
      "loss: 1.900002  [ 1915/60000]\n",
      "loss: 1.482199  [ 1916/60000]\n",
      "loss: 1.693366  [ 1917/60000]\n",
      "loss: 2.022756  [ 1918/60000]\n",
      "loss: 2.003932  [ 1919/60000]\n",
      "loss: 1.692898  [ 1920/60000]\n",
      "loss: 2.255584  [ 1921/60000]\n",
      "loss: 1.417578  [ 1922/60000]\n",
      "loss: 1.763801  [ 1923/60000]\n",
      "loss: 1.986786  [ 1924/60000]\n",
      "loss: 1.754675  [ 1925/60000]\n",
      "loss: 1.897140  [ 1926/60000]\n",
      "loss: 0.877257  [ 1927/60000]\n",
      "loss: 0.734859  [ 1928/60000]\n",
      "loss: 2.413050  [ 1929/60000]\n",
      "loss: 2.628211  [ 1930/60000]\n",
      "loss: 1.141084  [ 1931/60000]\n",
      "loss: 2.389823  [ 1932/60000]\n",
      "loss: 1.550041  [ 1933/60000]\n",
      "loss: 1.494906  [ 1934/60000]\n",
      "loss: 1.746691  [ 1935/60000]\n",
      "loss: 1.840967  [ 1936/60000]\n",
      "loss: 1.782118  [ 1937/60000]\n",
      "loss: 2.057120  [ 1938/60000]\n",
      "loss: 1.902351  [ 1939/60000]\n",
      "loss: 1.884261  [ 1940/60000]\n",
      "loss: 2.149275  [ 1941/60000]\n",
      "loss: 1.498024  [ 1942/60000]\n",
      "loss: 1.720022  [ 1943/60000]\n",
      "loss: 1.625108  [ 1944/60000]\n",
      "loss: 1.654274  [ 1945/60000]\n",
      "loss: 1.608781  [ 1946/60000]\n",
      "loss: 1.824449  [ 1947/60000]\n",
      "loss: 1.958344  [ 1948/60000]\n",
      "loss: 2.036392  [ 1949/60000]\n",
      "loss: 1.833008  [ 1950/60000]\n",
      "loss: 1.912810  [ 1951/60000]\n",
      "loss: 1.985823  [ 1952/60000]\n",
      "loss: 1.945251  [ 1953/60000]\n",
      "loss: 1.533549  [ 1954/60000]\n",
      "loss: 2.115639  [ 1955/60000]\n",
      "loss: 2.620691  [ 1956/60000]\n",
      "loss: 0.945199  [ 1957/60000]\n",
      "loss: 1.875851  [ 1958/60000]\n",
      "loss: 1.612859  [ 1959/60000]\n",
      "loss: 1.616172  [ 1960/60000]\n",
      "loss: 1.792120  [ 1961/60000]\n",
      "loss: 2.619254  [ 1962/60000]\n",
      "loss: 1.894578  [ 1963/60000]\n",
      "loss: 0.776680  [ 1964/60000]\n",
      "loss: 2.038089  [ 1965/60000]\n",
      "loss: 2.508238  [ 1966/60000]\n",
      "loss: 1.832499  [ 1967/60000]\n",
      "loss: 1.445793  [ 1968/60000]\n",
      "loss: 2.022618  [ 1969/60000]\n",
      "loss: 0.829697  [ 1970/60000]\n",
      "loss: 2.439929  [ 1971/60000]\n",
      "loss: 1.303751  [ 1972/60000]\n",
      "loss: 2.113034  [ 1973/60000]\n",
      "loss: 1.750686  [ 1974/60000]\n",
      "loss: 2.003017  [ 1975/60000]\n",
      "loss: 1.769246  [ 1976/60000]\n",
      "loss: 1.670827  [ 1977/60000]\n",
      "loss: 1.618106  [ 1978/60000]\n",
      "loss: 2.515609  [ 1979/60000]\n",
      "loss: 1.212668  [ 1980/60000]\n",
      "loss: 2.258171  [ 1981/60000]\n",
      "loss: 1.861730  [ 1982/60000]\n",
      "loss: 1.911601  [ 1983/60000]\n",
      "loss: 1.513679  [ 1984/60000]\n",
      "loss: 1.890445  [ 1985/60000]\n",
      "loss: 1.950606  [ 1986/60000]\n",
      "loss: 1.743465  [ 1987/60000]\n",
      "loss: 2.015487  [ 1988/60000]\n",
      "loss: 2.268954  [ 1989/60000]\n",
      "loss: 1.506402  [ 1990/60000]\n",
      "loss: 2.066831  [ 1991/60000]\n",
      "loss: 2.315530  [ 1992/60000]\n",
      "loss: 2.246651  [ 1993/60000]\n",
      "loss: 2.370694  [ 1994/60000]\n",
      "loss: 2.034160  [ 1995/60000]\n",
      "loss: 0.681735  [ 1996/60000]\n",
      "loss: 2.454651  [ 1997/60000]\n",
      "loss: 2.492186  [ 1998/60000]\n",
      "loss: 2.269256  [ 1999/60000]\n",
      "loss: 0.683282  [ 2000/60000]\n",
      "loss: 2.630142  [ 2001/60000]\n",
      "loss: 1.635909  [ 2002/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.813157 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.573"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataloader, model, loss_fn, optimizer)\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696197eb-d4c5-4a1a-870f-e9d7e644b404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4d0f7e0b-1a36-4bd9-86dd-6605ab6bd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 19.9%\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 47.5%\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n",
      "loss: 1.609467  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 58.6%\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.557949  [   64/60000]\n",
      "loss: 1.491758  [ 6464/60000]\n",
      "loss: 1.660951  [12864/60000]\n",
      "loss: 1.377765  [19264/60000]\n",
      "loss: 1.444292  [25664/60000]\n",
      "loss: 1.447927  [32064/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[186], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      6\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf90d4-a792-4e0e-a068-8254fd746dcc",
   "metadata": {},
   "source": [
    "## Overfitting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7ad4c2e2-4a86-49d1-8048-1a12440ac1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 19.9%\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 47.5%\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n",
      "loss: 1.609467  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 58.6%\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.557949  [   64/60000]\n",
      "loss: 1.491758  [ 6464/60000]\n",
      "loss: 1.660951  [12864/60000]\n",
      "loss: 1.377765  [19264/60000]\n",
      "loss: 1.444292  [25664/60000]\n",
      "loss: 1.447927  [32064/60000]\n",
      "loss: 1.356688  [38464/60000]\n",
      "loss: 1.537228  [44864/60000]\n",
      "loss: 1.358394  [51264/60000]\n",
      "loss: 1.372089  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 66.5%\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.318120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.321148  [   64/60000]\n",
      "loss: 1.245712  [ 6464/60000]\n",
      "loss: 1.394385  [12864/60000]\n",
      "loss: 1.151553  [19264/60000]\n",
      "loss: 1.197075  [25664/60000]\n",
      "loss: 1.215134  [32064/60000]\n",
      "loss: 1.118639  [38464/60000]\n",
      "loss: 1.316757  [44864/60000]\n",
      "loss: 1.153122  [51264/60000]\n",
      "loss: 1.170352  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 72.7%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 1.109394 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.132133  [   64/60000]\n",
      "loss: 1.050473  [ 6464/60000]\n",
      "loss: 1.166535  [12864/60000]\n",
      "loss: 0.973874  [19264/60000]\n",
      "loss: 1.005295  [25664/60000]\n",
      "loss: 1.031641  [32064/60000]\n",
      "loss: 0.938574  [38464/60000]\n",
      "loss: 1.139525  [44864/60000]\n",
      "loss: 0.997379  [51264/60000]\n",
      "loss: 1.014367  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 76.8%\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.952216 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.989064  [   64/60000]\n",
      "loss: 0.902320  [ 6464/60000]\n",
      "loss: 0.988564  [12864/60000]\n",
      "loss: 0.843259  [19264/60000]\n",
      "loss: 0.862754  [25664/60000]\n",
      "loss: 0.892941  [32064/60000]\n",
      "loss: 0.806622  [38464/60000]\n",
      "loss: 1.004429  [44864/60000]\n",
      "loss: 0.880611  [51264/60000]\n",
      "loss: 0.898888  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 79.5%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.836298 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.881625  [   64/60000]\n",
      "loss: 0.789953  [ 6464/60000]\n",
      "loss: 0.853873  [12864/60000]\n",
      "loss: 0.748737  [19264/60000]\n",
      "loss: 0.757954  [25664/60000]\n",
      "loss: 0.789745  [32064/60000]\n",
      "loss: 0.709818  [38464/60000]\n",
      "loss: 0.902217  [44864/60000]\n",
      "loss: 0.792498  [51264/60000]\n",
      "loss: 0.814121  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 81.4%\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.749816 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.799808  [   64/60000]\n",
      "loss: 0.703597  [ 6464/60000]\n",
      "loss: 0.751970  [12864/60000]\n",
      "loss: 0.679617  [19264/60000]\n",
      "loss: 0.678745  [25664/60000]\n",
      "loss: 0.712081  [32064/60000]\n",
      "loss: 0.636931  [38464/60000]\n",
      "loss: 0.824047  [44864/60000]\n",
      "loss: 0.724426  [51264/60000]\n",
      "loss: 0.750376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 82.7%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.683793 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.736062  [   64/60000]\n",
      "loss: 0.636310  [ 6464/60000]\n",
      "loss: 0.674065  [12864/60000]\n",
      "loss: 0.627934  [19264/60000]\n",
      "loss: 0.616891  [25664/60000]\n",
      "loss: 0.652553  [32064/60000]\n",
      "loss: 0.580294  [38464/60000]\n",
      "loss: 0.763228  [44864/60000]\n",
      "loss: 0.670565  [51264/60000]\n",
      "loss: 0.701475  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 83.9%\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.632073 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.685212  [   64/60000]\n",
      "loss: 0.582800  [ 6464/60000]\n",
      "loss: 0.613257  [12864/60000]\n",
      "loss: 0.588443  [19264/60000]\n",
      "loss: 0.567234  [25664/60000]\n",
      "loss: 0.606197  [32064/60000]\n",
      "loss: 0.535110  [38464/60000]\n",
      "loss: 0.714983  [44864/60000]\n",
      "loss: 0.627312  [51264/60000]\n",
      "loss: 0.663166  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 84.7%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.590623 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.643741  [   64/60000]\n",
      "loss: 0.539546  [ 6464/60000]\n",
      "loss: 0.564672  [12864/60000]\n",
      "loss: 0.557566  [19264/60000]\n",
      "loss: 0.526341  [25664/60000]\n",
      "loss: 0.569478  [32064/60000]\n",
      "loss: 0.498467  [38464/60000]\n",
      "loss: 0.676008  [44864/60000]\n",
      "loss: 0.591738  [51264/60000]\n",
      "loss: 0.632310  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 85.4%\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.556767 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.609224  [   64/60000]\n",
      "loss: 0.504055  [ 6464/60000]\n",
      "loss: 0.525274  [12864/60000]\n",
      "loss: 0.532958  [19264/60000]\n",
      "loss: 0.492195  [25664/60000]\n",
      "loss: 0.539940  [32064/60000]\n",
      "loss: 0.468215  [38464/60000]\n",
      "loss: 0.643991  [44864/60000]\n",
      "loss: 0.562275  [51264/60000]\n",
      "loss: 0.607107  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.0%\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.528659 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.580129  [   64/60000]\n",
      "loss: 0.474578  [ 6464/60000]\n",
      "loss: 0.492751  [12864/60000]\n",
      "loss: 0.512964  [19264/60000]\n",
      "loss: 0.463309  [25664/60000]\n",
      "loss: 0.515848  [32064/60000]\n",
      "loss: 0.442773  [38464/60000]\n",
      "loss: 0.617319  [44864/60000]\n",
      "loss: 0.537686  [51264/60000]\n",
      "loss: 0.586342  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.5%\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.505008 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.555234  [   64/60000]\n",
      "loss: 0.449872  [ 6464/60000]\n",
      "loss: 0.465381  [12864/60000]\n",
      "loss: 0.496506  [19264/60000]\n",
      "loss: 0.438663  [25664/60000]\n",
      "loss: 0.495941  [32064/60000]\n",
      "loss: 0.421250  [38464/60000]\n",
      "loss: 0.594771  [44864/60000]\n",
      "loss: 0.517012  [51264/60000]\n",
      "loss: 0.569075  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.0%\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.484894 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.533636  [   64/60000]\n",
      "loss: 0.428864  [ 6464/60000]\n",
      "loss: 0.442080  [12864/60000]\n",
      "loss: 0.482801  [19264/60000]\n",
      "loss: 0.417484  [25664/60000]\n",
      "loss: 0.479340  [32064/60000]\n",
      "loss: 0.402846  [38464/60000]\n",
      "loss: 0.575419  [44864/60000]\n",
      "loss: 0.499393  [51264/60000]\n",
      "loss: 0.554512  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.4%\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.467629 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.514752  [   64/60000]\n",
      "loss: 0.410934  [ 6464/60000]\n",
      "loss: 0.422006  [12864/60000]\n",
      "loss: 0.471228  [19264/60000]\n",
      "loss: 0.399241  [25664/60000]\n",
      "loss: 0.465358  [32064/60000]\n",
      "loss: 0.386991  [38464/60000]\n",
      "loss: 0.558591  [44864/60000]\n",
      "loss: 0.484281  [51264/60000]\n",
      "loss: 0.542164  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.8%\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.452679 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.498072  [   64/60000]\n",
      "loss: 0.395538  [ 6464/60000]\n",
      "loss: 0.404541  [12864/60000]\n",
      "loss: 0.461305  [19264/60000]\n",
      "loss: 0.383379  [25664/60000]\n",
      "loss: 0.453440  [32064/60000]\n",
      "loss: 0.373245  [38464/60000]\n",
      "loss: 0.543806  [44864/60000]\n",
      "loss: 0.471273  [51264/60000]\n",
      "loss: 0.531639  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.439641 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.483209  [   64/60000]\n",
      "loss: 0.382195  [ 6464/60000]\n",
      "loss: 0.389225  [12864/60000]\n",
      "loss: 0.452666  [19264/60000]\n",
      "loss: 0.369548  [25664/60000]\n",
      "loss: 0.443190  [32064/60000]\n",
      "loss: 0.361243  [38464/60000]\n",
      "loss: 0.530696  [44864/60000]\n",
      "loss: 0.459930  [51264/60000]\n",
      "loss: 0.522511  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.3%\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.428185 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.469908  [   64/60000]\n",
      "loss: 0.370535  [ 6464/60000]\n",
      "loss: 0.375691  [12864/60000]\n",
      "loss: 0.445108  [19264/60000]\n",
      "loss: 0.357461  [25664/60000]\n",
      "loss: 0.434370  [32064/60000]\n",
      "loss: 0.350595  [38464/60000]\n",
      "loss: 0.519020  [44864/60000]\n",
      "loss: 0.449971  [51264/60000]\n",
      "loss: 0.514630  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.5%\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.418057 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.457895  [   64/60000]\n",
      "loss: 0.360293  [ 6464/60000]\n",
      "loss: 0.363641  [12864/60000]\n",
      "loss: 0.438435  [19264/60000]\n",
      "loss: 0.346842  [25664/60000]\n",
      "loss: 0.426683  [32064/60000]\n",
      "loss: 0.341132  [38464/60000]\n",
      "loss: 0.508522  [44864/60000]\n",
      "loss: 0.441188  [51264/60000]\n",
      "loss: 0.507772  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.7%\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.409051 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.446948  [   64/60000]\n",
      "loss: 0.351254  [ 6464/60000]\n",
      "loss: 0.352856  [12864/60000]\n",
      "loss: 0.432484  [19264/60000]\n",
      "loss: 0.337381  [25664/60000]\n",
      "loss: 0.419940  [32064/60000]\n",
      "loss: 0.332631  [38464/60000]\n",
      "loss: 0.499001  [44864/60000]\n",
      "loss: 0.433327  [51264/60000]\n",
      "loss: 0.501830  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.8%\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.400999 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.436893  [   64/60000]\n",
      "loss: 0.343207  [ 6464/60000]\n",
      "loss: 0.343160  [12864/60000]\n",
      "loss: 0.427129  [19264/60000]\n",
      "loss: 0.328976  [25664/60000]\n",
      "loss: 0.414029  [32064/60000]\n",
      "loss: 0.324936  [38464/60000]\n",
      "loss: 0.490381  [44864/60000]\n",
      "loss: 0.426376  [51264/60000]\n",
      "loss: 0.496547  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.0%\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.393751 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.427625  [   64/60000]\n",
      "loss: 0.336079  [ 6464/60000]\n",
      "loss: 0.334352  [12864/60000]\n",
      "loss: 0.422218  [19264/60000]\n",
      "loss: 0.321472  [25664/60000]\n",
      "loss: 0.408761  [32064/60000]\n",
      "loss: 0.317975  [38464/60000]\n",
      "loss: 0.482516  [44864/60000]\n",
      "loss: 0.420126  [51264/60000]\n",
      "loss: 0.491938  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.1%\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.387207 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.419028  [   64/60000]\n",
      "loss: 0.329684  [ 6464/60000]\n",
      "loss: 0.326338  [12864/60000]\n",
      "loss: 0.417774  [19264/60000]\n",
      "loss: 0.314757  [25664/60000]\n",
      "loss: 0.404038  [32064/60000]\n",
      "loss: 0.311657  [38464/60000]\n",
      "loss: 0.475334  [44864/60000]\n",
      "loss: 0.414519  [51264/60000]\n",
      "loss: 0.487707  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.2%\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.381264 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.411041  [   64/60000]\n",
      "loss: 0.323946  [ 6464/60000]\n",
      "loss: 0.319070  [12864/60000]\n",
      "loss: 0.413676  [19264/60000]\n",
      "loss: 0.308720  [25664/60000]\n",
      "loss: 0.399793  [32064/60000]\n",
      "loss: 0.305919  [38464/60000]\n",
      "loss: 0.468747  [44864/60000]\n",
      "loss: 0.409420  [51264/60000]\n",
      "loss: 0.483958  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.3%\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.375842 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.403606  [   64/60000]\n",
      "loss: 0.318770  [ 6464/60000]\n",
      "loss: 0.312419  [12864/60000]\n",
      "loss: 0.409868  [19264/60000]\n",
      "loss: 0.303256  [25664/60000]\n",
      "loss: 0.395937  [32064/60000]\n",
      "loss: 0.300664  [38464/60000]\n",
      "loss: 0.462695  [44864/60000]\n",
      "loss: 0.404762  [51264/60000]\n",
      "loss: 0.480620  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.4%\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.370872 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.396618  [   64/60000]\n",
      "loss: 0.314116  [ 6464/60000]\n",
      "loss: 0.306286  [12864/60000]\n",
      "loss: 0.406302  [19264/60000]\n",
      "loss: 0.298277  [25664/60000]\n",
      "loss: 0.392432  [32064/60000]\n",
      "loss: 0.295827  [38464/60000]\n",
      "loss: 0.457135  [44864/60000]\n",
      "loss: 0.400491  [51264/60000]\n",
      "loss: 0.477629  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.5%\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.366304 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.390042  [   64/60000]\n",
      "loss: 0.309918  [ 6464/60000]\n",
      "loss: 0.300610  [12864/60000]\n",
      "loss: 0.402982  [19264/60000]\n",
      "loss: 0.293758  [25664/60000]\n",
      "loss: 0.389216  [32064/60000]\n",
      "loss: 0.291381  [38464/60000]\n",
      "loss: 0.452016  [44864/60000]\n",
      "loss: 0.396515  [51264/60000]\n",
      "loss: 0.474933  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.6%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.362095 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.383802  [   64/60000]\n",
      "loss: 0.306093  [ 6464/60000]\n",
      "loss: 0.295327  [12864/60000]\n",
      "loss: 0.399895  [19264/60000]\n",
      "loss: 0.289599  [25664/60000]\n",
      "loss: 0.386275  [32064/60000]\n",
      "loss: 0.287284  [38464/60000]\n",
      "loss: 0.447272  [44864/60000]\n",
      "loss: 0.392873  [51264/60000]\n",
      "loss: 0.472503  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.358196 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.377893  [   64/60000]\n",
      "loss: 0.302624  [ 6464/60000]\n",
      "loss: 0.290400  [12864/60000]\n",
      "loss: 0.396970  [19264/60000]\n",
      "loss: 0.285810  [25664/60000]\n",
      "loss: 0.383609  [32064/60000]\n",
      "loss: 0.283493  [38464/60000]\n",
      "loss: 0.442835  [44864/60000]\n",
      "loss: 0.389476  [51264/60000]\n",
      "loss: 0.470337  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.354576 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.372278  [   64/60000]\n",
      "loss: 0.299469  [ 6464/60000]\n",
      "loss: 0.285788  [12864/60000]\n",
      "loss: 0.394212  [19264/60000]\n",
      "loss: 0.282320  [25664/60000]\n",
      "loss: 0.381166  [32064/60000]\n",
      "loss: 0.279971  [38464/60000]\n",
      "loss: 0.438735  [44864/60000]\n",
      "loss: 0.386328  [51264/60000]\n",
      "loss: 0.468376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.351200 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.366960  [   64/60000]\n",
      "loss: 0.296603  [ 6464/60000]\n",
      "loss: 0.281467  [12864/60000]\n",
      "loss: 0.391569  [19264/60000]\n",
      "loss: 0.279109  [25664/60000]\n",
      "loss: 0.378893  [32064/60000]\n",
      "loss: 0.276695  [38464/60000]\n",
      "loss: 0.434914  [44864/60000]\n",
      "loss: 0.383394  [51264/60000]\n",
      "loss: 0.466559  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.348050 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.361929  [   64/60000]\n",
      "loss: 0.293956  [ 6464/60000]\n",
      "loss: 0.277419  [12864/60000]\n",
      "loss: 0.389016  [19264/60000]\n",
      "loss: 0.276153  [25664/60000]\n",
      "loss: 0.376759  [32064/60000]\n",
      "loss: 0.273651  [38464/60000]\n",
      "loss: 0.431349  [44864/60000]\n",
      "loss: 0.380643  [51264/60000]\n",
      "loss: 0.464907  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.9%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.345101 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.357106  [   64/60000]\n",
      "loss: 0.291519  [ 6464/60000]\n",
      "loss: 0.273610  [12864/60000]\n",
      "loss: 0.386581  [19264/60000]\n",
      "loss: 0.273380  [25664/60000]\n",
      "loss: 0.374776  [32064/60000]\n",
      "loss: 0.270817  [38464/60000]\n",
      "loss: 0.428014  [44864/60000]\n",
      "loss: 0.378020  [51264/60000]\n",
      "loss: 0.463414  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.342331 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.352486  [   64/60000]\n",
      "loss: 0.289283  [ 6464/60000]\n",
      "loss: 0.270017  [12864/60000]\n",
      "loss: 0.384253  [19264/60000]\n",
      "loss: 0.270773  [25664/60000]\n",
      "loss: 0.372907  [32064/60000]\n",
      "loss: 0.268161  [38464/60000]\n",
      "loss: 0.424887  [44864/60000]\n",
      "loss: 0.375553  [51264/60000]\n",
      "loss: 0.462046  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.339719 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.348055  [   64/60000]\n",
      "loss: 0.287234  [ 6464/60000]\n",
      "loss: 0.266590  [12864/60000]\n",
      "loss: 0.381983  [19264/60000]\n",
      "loss: 0.268346  [25664/60000]\n",
      "loss: 0.371140  [32064/60000]\n",
      "loss: 0.265664  [38464/60000]\n",
      "loss: 0.421982  [44864/60000]\n",
      "loss: 0.373270  [51264/60000]\n",
      "loss: 0.460792  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.337255 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.343811  [   64/60000]\n",
      "loss: 0.285360  [ 6464/60000]\n",
      "loss: 0.263343  [12864/60000]\n",
      "loss: 0.379786  [19264/60000]\n",
      "loss: 0.266061  [25664/60000]\n",
      "loss: 0.369476  [32064/60000]\n",
      "loss: 0.263358  [38464/60000]\n",
      "loss: 0.419190  [44864/60000]\n",
      "loss: 0.371083  [51264/60000]\n",
      "loss: 0.459650  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.334935 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.339731  [   64/60000]\n",
      "loss: 0.283560  [ 6464/60000]\n",
      "loss: 0.260268  [12864/60000]\n",
      "loss: 0.377673  [19264/60000]\n",
      "loss: 0.263876  [25664/60000]\n",
      "loss: 0.368027  [32064/60000]\n",
      "loss: 0.261131  [38464/60000]\n",
      "loss: 0.416569  [44864/60000]\n",
      "loss: 0.369035  [51264/60000]\n",
      "loss: 0.458614  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.332736 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.335784  [   64/60000]\n",
      "loss: 0.281895  [ 6464/60000]\n",
      "loss: 0.257351  [12864/60000]\n",
      "loss: 0.375694  [19264/60000]\n",
      "loss: 0.261864  [25664/60000]\n",
      "loss: 0.366651  [32064/60000]\n",
      "loss: 0.259001  [38464/60000]\n",
      "loss: 0.414089  [44864/60000]\n",
      "loss: 0.367049  [51264/60000]\n",
      "loss: 0.457645  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.330648 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.331939  [   64/60000]\n",
      "loss: 0.280379  [ 6464/60000]\n",
      "loss: 0.254579  [12864/60000]\n",
      "loss: 0.373776  [19264/60000]\n",
      "loss: 0.259961  [25664/60000]\n",
      "loss: 0.365373  [32064/60000]\n",
      "loss: 0.257010  [38464/60000]\n",
      "loss: 0.411737  [44864/60000]\n",
      "loss: 0.365146  [51264/60000]\n",
      "loss: 0.456740  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.328663 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.328240  [   64/60000]\n",
      "loss: 0.278977  [ 6464/60000]\n",
      "loss: 0.251932  [12864/60000]\n",
      "loss: 0.371922  [19264/60000]\n",
      "loss: 0.258162  [25664/60000]\n",
      "loss: 0.364164  [32064/60000]\n",
      "loss: 0.255108  [38464/60000]\n",
      "loss: 0.409504  [44864/60000]\n",
      "loss: 0.363297  [51264/60000]\n",
      "loss: 0.455878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.326776 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.324579  [   64/60000]\n",
      "loss: 0.277670  [ 6464/60000]\n",
      "loss: 0.249412  [12864/60000]\n",
      "loss: 0.370111  [19264/60000]\n",
      "loss: 0.256447  [25664/60000]\n",
      "loss: 0.363039  [32064/60000]\n",
      "loss: 0.253300  [38464/60000]\n",
      "loss: 0.407395  [44864/60000]\n",
      "loss: 0.361510  [51264/60000]\n",
      "loss: 0.454985  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.324981 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.321034  [   64/60000]\n",
      "loss: 0.276453  [ 6464/60000]\n",
      "loss: 0.247065  [12864/60000]\n",
      "loss: 0.368344  [19264/60000]\n",
      "loss: 0.254840  [25664/60000]\n",
      "loss: 0.361960  [32064/60000]\n",
      "loss: 0.251577  [38464/60000]\n",
      "loss: 0.405412  [44864/60000]\n",
      "loss: 0.359763  [51264/60000]\n",
      "loss: 0.454153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.323264 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.317589  [   64/60000]\n",
      "loss: 0.275312  [ 6464/60000]\n",
      "loss: 0.244839  [12864/60000]\n",
      "loss: 0.366690  [19264/60000]\n",
      "loss: 0.253297  [25664/60000]\n",
      "loss: 0.360966  [32064/60000]\n",
      "loss: 0.249923  [38464/60000]\n",
      "loss: 0.403501  [44864/60000]\n",
      "loss: 0.358088  [51264/60000]\n",
      "loss: 0.453323  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.321621 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.314243  [   64/60000]\n",
      "loss: 0.274265  [ 6464/60000]\n",
      "loss: 0.242711  [12864/60000]\n",
      "loss: 0.365063  [19264/60000]\n",
      "loss: 0.251779  [25664/60000]\n",
      "loss: 0.359882  [32064/60000]\n",
      "loss: 0.248368  [38464/60000]\n",
      "loss: 0.401665  [44864/60000]\n",
      "loss: 0.356466  [51264/60000]\n",
      "loss: 0.452569  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.320046 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.310971  [   64/60000]\n",
      "loss: 0.273295  [ 6464/60000]\n",
      "loss: 0.240691  [12864/60000]\n",
      "loss: 0.363443  [19264/60000]\n",
      "loss: 0.250335  [25664/60000]\n",
      "loss: 0.358840  [32064/60000]\n",
      "loss: 0.246891  [38464/60000]\n",
      "loss: 0.399927  [44864/60000]\n",
      "loss: 0.354897  [51264/60000]\n",
      "loss: 0.451816  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.318538 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.307802  [   64/60000]\n",
      "loss: 0.272382  [ 6464/60000]\n",
      "loss: 0.238761  [12864/60000]\n",
      "loss: 0.361861  [19264/60000]\n",
      "loss: 0.248977  [25664/60000]\n",
      "loss: 0.357790  [32064/60000]\n",
      "loss: 0.245520  [38464/60000]\n",
      "loss: 0.398207  [44864/60000]\n",
      "loss: 0.353408  [51264/60000]\n",
      "loss: 0.451085  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.317088 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.304733  [   64/60000]\n",
      "loss: 0.271497  [ 6464/60000]\n",
      "loss: 0.236913  [12864/60000]\n",
      "loss: 0.360322  [19264/60000]\n",
      "loss: 0.247720  [25664/60000]\n",
      "loss: 0.356810  [32064/60000]\n",
      "loss: 0.244182  [38464/60000]\n",
      "loss: 0.396583  [44864/60000]\n",
      "loss: 0.351987  [51264/60000]\n",
      "loss: 0.450442  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.315697 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.301729  [   64/60000]\n",
      "loss: 0.270657  [ 6464/60000]\n",
      "loss: 0.235146  [12864/60000]\n",
      "loss: 0.358837  [19264/60000]\n",
      "loss: 0.246505  [25664/60000]\n",
      "loss: 0.355904  [32064/60000]\n",
      "loss: 0.242935  [38464/60000]\n",
      "loss: 0.395000  [44864/60000]\n",
      "loss: 0.350593  [51264/60000]\n",
      "loss: 0.449746  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.314361 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.298826  [   64/60000]\n",
      "loss: 0.269848  [ 6464/60000]\n",
      "loss: 0.233430  [12864/60000]\n",
      "loss: 0.357380  [19264/60000]\n",
      "loss: 0.245385  [25664/60000]\n",
      "loss: 0.355014  [32064/60000]\n",
      "loss: 0.241703  [38464/60000]\n",
      "loss: 0.393487  [44864/60000]\n",
      "loss: 0.349199  [51264/60000]\n",
      "loss: 0.449051  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.313068 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.295966  [   64/60000]\n",
      "loss: 0.269110  [ 6464/60000]\n",
      "loss: 0.231742  [12864/60000]\n",
      "loss: 0.355931  [19264/60000]\n",
      "loss: 0.244334  [25664/60000]\n",
      "loss: 0.354139  [32064/60000]\n",
      "loss: 0.240559  [38464/60000]\n",
      "loss: 0.392049  [44864/60000]\n",
      "loss: 0.347825  [51264/60000]\n",
      "loss: 0.448461  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.311820 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.293166  [   64/60000]\n",
      "loss: 0.268428  [ 6464/60000]\n",
      "loss: 0.230137  [12864/60000]\n",
      "loss: 0.354495  [19264/60000]\n",
      "loss: 0.243302  [25664/60000]\n",
      "loss: 0.353278  [32064/60000]\n",
      "loss: 0.239469  [38464/60000]\n",
      "loss: 0.390675  [44864/60000]\n",
      "loss: 0.346472  [51264/60000]\n",
      "loss: 0.447874  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.310615 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.290458  [   64/60000]\n",
      "loss: 0.267775  [ 6464/60000]\n",
      "loss: 0.228563  [12864/60000]\n",
      "loss: 0.353103  [19264/60000]\n",
      "loss: 0.242335  [25664/60000]\n",
      "loss: 0.352485  [32064/60000]\n",
      "loss: 0.238412  [38464/60000]\n",
      "loss: 0.389451  [44864/60000]\n",
      "loss: 0.345149  [51264/60000]\n",
      "loss: 0.447307  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.309445 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.287826  [   64/60000]\n",
      "loss: 0.267161  [ 6464/60000]\n",
      "loss: 0.227018  [12864/60000]\n",
      "loss: 0.351709  [19264/60000]\n",
      "loss: 0.241392  [25664/60000]\n",
      "loss: 0.351708  [32064/60000]\n",
      "loss: 0.237416  [38464/60000]\n",
      "loss: 0.388272  [44864/60000]\n",
      "loss: 0.343855  [51264/60000]\n",
      "loss: 0.446767  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.308315 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.285238  [   64/60000]\n",
      "loss: 0.266577  [ 6464/60000]\n",
      "loss: 0.225517  [12864/60000]\n",
      "loss: 0.350297  [19264/60000]\n",
      "loss: 0.240477  [25664/60000]\n",
      "loss: 0.350938  [32064/60000]\n",
      "loss: 0.236480  [38464/60000]\n",
      "loss: 0.387129  [44864/60000]\n",
      "loss: 0.342626  [51264/60000]\n",
      "loss: 0.446221  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.307221 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.282732  [   64/60000]\n",
      "loss: 0.266016  [ 6464/60000]\n",
      "loss: 0.224058  [12864/60000]\n",
      "loss: 0.348947  [19264/60000]\n",
      "loss: 0.239578  [25664/60000]\n",
      "loss: 0.350212  [32064/60000]\n",
      "loss: 0.235580  [38464/60000]\n",
      "loss: 0.386025  [44864/60000]\n",
      "loss: 0.341384  [51264/60000]\n",
      "loss: 0.445606  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.306167 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.280269  [   64/60000]\n",
      "loss: 0.265480  [ 6464/60000]\n",
      "loss: 0.222645  [12864/60000]\n",
      "loss: 0.347616  [19264/60000]\n",
      "loss: 0.238733  [25664/60000]\n",
      "loss: 0.349494  [32064/60000]\n",
      "loss: 0.234751  [38464/60000]\n",
      "loss: 0.384915  [44864/60000]\n",
      "loss: 0.340146  [51264/60000]\n",
      "loss: 0.445001  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.305146 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.277882  [   64/60000]\n",
      "loss: 0.264972  [ 6464/60000]\n",
      "loss: 0.221269  [12864/60000]\n",
      "loss: 0.346307  [19264/60000]\n",
      "loss: 0.237827  [25664/60000]\n",
      "loss: 0.348813  [32064/60000]\n",
      "loss: 0.233939  [38464/60000]\n",
      "loss: 0.383880  [44864/60000]\n",
      "loss: 0.338978  [51264/60000]\n",
      "loss: 0.444390  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.304157 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.275552  [   64/60000]\n",
      "loss: 0.264638  [ 6464/60000]\n",
      "loss: 0.219939  [12864/60000]\n",
      "loss: 0.345044  [19264/60000]\n",
      "loss: 0.236835  [25664/60000]\n",
      "loss: 0.348143  [32064/60000]\n",
      "loss: 0.233142  [38464/60000]\n",
      "loss: 0.382895  [44864/60000]\n",
      "loss: 0.337906  [51264/60000]\n",
      "loss: 0.443790  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.303189 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.273331  [   64/60000]\n",
      "loss: 0.264376  [ 6464/60000]\n",
      "loss: 0.218615  [12864/60000]\n",
      "loss: 0.343765  [19264/60000]\n",
      "loss: 0.235863  [25664/60000]\n",
      "loss: 0.347487  [32064/60000]\n",
      "loss: 0.232413  [38464/60000]\n",
      "loss: 0.381935  [44864/60000]\n",
      "loss: 0.336823  [51264/60000]\n",
      "loss: 0.443205  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.302251 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.271162  [   64/60000]\n",
      "loss: 0.264144  [ 6464/60000]\n",
      "loss: 0.217355  [12864/60000]\n",
      "loss: 0.342524  [19264/60000]\n",
      "loss: 0.234907  [25664/60000]\n",
      "loss: 0.346829  [32064/60000]\n",
      "loss: 0.231697  [38464/60000]\n",
      "loss: 0.381001  [44864/60000]\n",
      "loss: 0.335749  [51264/60000]\n",
      "loss: 0.442647  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.301339 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.269055  [   64/60000]\n",
      "loss: 0.263907  [ 6464/60000]\n",
      "loss: 0.216140  [12864/60000]\n",
      "loss: 0.341370  [19264/60000]\n",
      "loss: 0.233978  [25664/60000]\n",
      "loss: 0.346199  [32064/60000]\n",
      "loss: 0.231012  [38464/60000]\n",
      "loss: 0.380151  [44864/60000]\n",
      "loss: 0.334724  [51264/60000]\n",
      "loss: 0.442129  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.300454 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.266978  [   64/60000]\n",
      "loss: 0.263679  [ 6464/60000]\n",
      "loss: 0.214988  [12864/60000]\n",
      "loss: 0.340233  [19264/60000]\n",
      "loss: 0.233095  [25664/60000]\n",
      "loss: 0.345587  [32064/60000]\n",
      "loss: 0.230390  [38464/60000]\n",
      "loss: 0.379328  [44864/60000]\n",
      "loss: 0.333704  [51264/60000]\n",
      "loss: 0.441612  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.299589 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.264926  [   64/60000]\n",
      "loss: 0.263483  [ 6464/60000]\n",
      "loss: 0.213876  [12864/60000]\n",
      "loss: 0.339137  [19264/60000]\n",
      "loss: 0.232241  [25664/60000]\n",
      "loss: 0.345002  [32064/60000]\n",
      "loss: 0.229791  [38464/60000]\n",
      "loss: 0.378541  [44864/60000]\n",
      "loss: 0.332726  [51264/60000]\n",
      "loss: 0.441093  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.298742 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.262905  [   64/60000]\n",
      "loss: 0.263294  [ 6464/60000]\n",
      "loss: 0.212792  [12864/60000]\n",
      "loss: 0.338017  [19264/60000]\n",
      "loss: 0.231363  [25664/60000]\n",
      "loss: 0.344402  [32064/60000]\n",
      "loss: 0.229191  [38464/60000]\n",
      "loss: 0.377770  [44864/60000]\n",
      "loss: 0.331777  [51264/60000]\n",
      "loss: 0.440520  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.297916 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.260919  [   64/60000]\n",
      "loss: 0.263119  [ 6464/60000]\n",
      "loss: 0.211727  [12864/60000]\n",
      "loss: 0.336916  [19264/60000]\n",
      "loss: 0.230520  [25664/60000]\n",
      "loss: 0.343846  [32064/60000]\n",
      "loss: 0.228613  [38464/60000]\n",
      "loss: 0.377023  [44864/60000]\n",
      "loss: 0.330816  [51264/60000]\n",
      "loss: 0.439982  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.297113 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.258944  [   64/60000]\n",
      "loss: 0.262949  [ 6464/60000]\n",
      "loss: 0.210702  [12864/60000]\n",
      "loss: 0.335893  [19264/60000]\n",
      "loss: 0.229731  [25664/60000]\n",
      "loss: 0.343300  [32064/60000]\n",
      "loss: 0.228051  [38464/60000]\n",
      "loss: 0.376340  [44864/60000]\n",
      "loss: 0.329899  [51264/60000]\n",
      "loss: 0.439411  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.296323 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.257028  [   64/60000]\n",
      "loss: 0.262809  [ 6464/60000]\n",
      "loss: 0.209655  [12864/60000]\n",
      "loss: 0.334821  [19264/60000]\n",
      "loss: 0.228915  [25664/60000]\n",
      "loss: 0.342755  [32064/60000]\n",
      "loss: 0.227524  [38464/60000]\n",
      "loss: 0.375705  [44864/60000]\n",
      "loss: 0.328947  [51264/60000]\n",
      "loss: 0.438878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.295557 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.255128  [   64/60000]\n",
      "loss: 0.262701  [ 6464/60000]\n",
      "loss: 0.208637  [12864/60000]\n",
      "loss: 0.333826  [19264/60000]\n",
      "loss: 0.228120  [25664/60000]\n",
      "loss: 0.342272  [32064/60000]\n",
      "loss: 0.226980  [38464/60000]\n",
      "loss: 0.375117  [44864/60000]\n",
      "loss: 0.328021  [51264/60000]\n",
      "loss: 0.438305  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294802 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.253281  [   64/60000]\n",
      "loss: 0.262628  [ 6464/60000]\n",
      "loss: 0.207632  [12864/60000]\n",
      "loss: 0.332828  [19264/60000]\n",
      "loss: 0.227327  [25664/60000]\n",
      "loss: 0.341767  [32064/60000]\n",
      "loss: 0.226461  [38464/60000]\n",
      "loss: 0.374522  [44864/60000]\n",
      "loss: 0.327119  [51264/60000]\n",
      "loss: 0.437756  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294062 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.251477  [   64/60000]\n",
      "loss: 0.262552  [ 6464/60000]\n",
      "loss: 0.206644  [12864/60000]\n",
      "loss: 0.331845  [19264/60000]\n",
      "loss: 0.226513  [25664/60000]\n",
      "loss: 0.341234  [32064/60000]\n",
      "loss: 0.225962  [38464/60000]\n",
      "loss: 0.373926  [44864/60000]\n",
      "loss: 0.326273  [51264/60000]\n",
      "loss: 0.437202  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.293337 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.249717  [   64/60000]\n",
      "loss: 0.262479  [ 6464/60000]\n",
      "loss: 0.205671  [12864/60000]\n",
      "loss: 0.330861  [19264/60000]\n",
      "loss: 0.225713  [25664/60000]\n",
      "loss: 0.340694  [32064/60000]\n",
      "loss: 0.225486  [38464/60000]\n",
      "loss: 0.373268  [44864/60000]\n",
      "loss: 0.325382  [51264/60000]\n",
      "loss: 0.436692  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.292631 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.247951  [   64/60000]\n",
      "loss: 0.262412  [ 6464/60000]\n",
      "loss: 0.204745  [12864/60000]\n",
      "loss: 0.329921  [19264/60000]\n",
      "loss: 0.224945  [25664/60000]\n",
      "loss: 0.340214  [32064/60000]\n",
      "loss: 0.225020  [38464/60000]\n",
      "loss: 0.372637  [44864/60000]\n",
      "loss: 0.324445  [51264/60000]\n",
      "loss: 0.436153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291939 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.246218  [   64/60000]\n",
      "loss: 0.262327  [ 6464/60000]\n",
      "loss: 0.203799  [12864/60000]\n",
      "loss: 0.329014  [19264/60000]\n",
      "loss: 0.224217  [25664/60000]\n",
      "loss: 0.339700  [32064/60000]\n",
      "loss: 0.224583  [38464/60000]\n",
      "loss: 0.372048  [44864/60000]\n",
      "loss: 0.323575  [51264/60000]\n",
      "loss: 0.435601  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291256 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.244550  [   64/60000]\n",
      "loss: 0.262262  [ 6464/60000]\n",
      "loss: 0.202847  [12864/60000]\n",
      "loss: 0.328070  [19264/60000]\n",
      "loss: 0.223528  [25664/60000]\n",
      "loss: 0.339159  [32064/60000]\n",
      "loss: 0.224138  [38464/60000]\n",
      "loss: 0.371467  [44864/60000]\n",
      "loss: 0.322684  [51264/60000]\n",
      "loss: 0.435070  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.290580 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.242908  [   64/60000]\n",
      "loss: 0.262179  [ 6464/60000]\n",
      "loss: 0.201914  [12864/60000]\n",
      "loss: 0.327168  [19264/60000]\n",
      "loss: 0.222848  [25664/60000]\n",
      "loss: 0.338631  [32064/60000]\n",
      "loss: 0.223747  [38464/60000]\n",
      "loss: 0.370874  [44864/60000]\n",
      "loss: 0.321823  [51264/60000]\n",
      "loss: 0.434495  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289920 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.241296  [   64/60000]\n",
      "loss: 0.262064  [ 6464/60000]\n",
      "loss: 0.200993  [12864/60000]\n",
      "loss: 0.326226  [19264/60000]\n",
      "loss: 0.222183  [25664/60000]\n",
      "loss: 0.338156  [32064/60000]\n",
      "loss: 0.223360  [38464/60000]\n",
      "loss: 0.370214  [44864/60000]\n",
      "loss: 0.320909  [51264/60000]\n",
      "loss: 0.433892  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289270 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.239696  [   64/60000]\n",
      "loss: 0.261996  [ 6464/60000]\n",
      "loss: 0.200119  [12864/60000]\n",
      "loss: 0.325246  [19264/60000]\n",
      "loss: 0.221546  [25664/60000]\n",
      "loss: 0.337743  [32064/60000]\n",
      "loss: 0.223004  [38464/60000]\n",
      "loss: 0.369534  [44864/60000]\n",
      "loss: 0.320051  [51264/60000]\n",
      "loss: 0.433291  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.288631 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.238151  [   64/60000]\n",
      "loss: 0.261889  [ 6464/60000]\n",
      "loss: 0.199259  [12864/60000]\n",
      "loss: 0.324294  [19264/60000]\n",
      "loss: 0.220953  [25664/60000]\n",
      "loss: 0.337353  [32064/60000]\n",
      "loss: 0.222659  [38464/60000]\n",
      "loss: 0.368885  [44864/60000]\n",
      "loss: 0.319209  [51264/60000]\n",
      "loss: 0.432661  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.287999 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.236660  [   64/60000]\n",
      "loss: 0.261770  [ 6464/60000]\n",
      "loss: 0.198410  [12864/60000]\n",
      "loss: 0.323337  [19264/60000]\n",
      "loss: 0.220353  [25664/60000]\n",
      "loss: 0.336925  [32064/60000]\n",
      "loss: 0.222322  [38464/60000]\n",
      "loss: 0.368301  [44864/60000]\n",
      "loss: 0.318417  [51264/60000]\n",
      "loss: 0.432048  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.287371 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.235225  [   64/60000]\n",
      "loss: 0.261391  [ 6464/60000]\n",
      "loss: 0.197565  [12864/60000]\n",
      "loss: 0.322295  [19264/60000]\n",
      "loss: 0.219759  [25664/60000]\n",
      "loss: 0.336425  [32064/60000]\n",
      "loss: 0.222034  [38464/60000]\n",
      "loss: 0.367705  [44864/60000]\n",
      "loss: 0.317636  [51264/60000]\n",
      "loss: 0.431499  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286755 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.233788  [   64/60000]\n",
      "loss: 0.260947  [ 6464/60000]\n",
      "loss: 0.196761  [12864/60000]\n",
      "loss: 0.321329  [19264/60000]\n",
      "loss: 0.219211  [25664/60000]\n",
      "loss: 0.336022  [32064/60000]\n",
      "loss: 0.221756  [38464/60000]\n",
      "loss: 0.367067  [44864/60000]\n",
      "loss: 0.316805  [51264/60000]\n",
      "loss: 0.430895  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286148 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.232377  [   64/60000]\n",
      "loss: 0.260517  [ 6464/60000]\n",
      "loss: 0.195978  [12864/60000]\n",
      "loss: 0.320401  [19264/60000]\n",
      "loss: 0.218679  [25664/60000]\n",
      "loss: 0.335649  [32064/60000]\n",
      "loss: 0.221502  [38464/60000]\n",
      "loss: 0.366469  [44864/60000]\n",
      "loss: 0.316112  [51264/60000]\n",
      "loss: 0.430327  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.285545 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.230961  [   64/60000]\n",
      "loss: 0.260129  [ 6464/60000]\n",
      "loss: 0.195204  [12864/60000]\n",
      "loss: 0.319471  [19264/60000]\n",
      "loss: 0.218125  [25664/60000]\n",
      "loss: 0.335283  [32064/60000]\n",
      "loss: 0.221215  [38464/60000]\n",
      "loss: 0.365885  [44864/60000]\n",
      "loss: 0.315432  [51264/60000]\n",
      "loss: 0.429731  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284949 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.229576  [   64/60000]\n",
      "loss: 0.259770  [ 6464/60000]\n",
      "loss: 0.194432  [12864/60000]\n",
      "loss: 0.318548  [19264/60000]\n",
      "loss: 0.217614  [25664/60000]\n",
      "loss: 0.334922  [32064/60000]\n",
      "loss: 0.220915  [38464/60000]\n",
      "loss: 0.365316  [44864/60000]\n",
      "loss: 0.314743  [51264/60000]\n",
      "loss: 0.429119  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284369 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.228234  [   64/60000]\n",
      "loss: 0.259448  [ 6464/60000]\n",
      "loss: 0.193597  [12864/60000]\n",
      "loss: 0.317639  [19264/60000]\n",
      "loss: 0.217113  [25664/60000]\n",
      "loss: 0.334547  [32064/60000]\n",
      "loss: 0.220682  [38464/60000]\n",
      "loss: 0.364663  [44864/60000]\n",
      "loss: 0.314041  [51264/60000]\n",
      "loss: 0.428491  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283801 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.226927  [   64/60000]\n",
      "loss: 0.259090  [ 6464/60000]\n",
      "loss: 0.192772  [12864/60000]\n",
      "loss: 0.316695  [19264/60000]\n",
      "loss: 0.216646  [25664/60000]\n",
      "loss: 0.334203  [32064/60000]\n",
      "loss: 0.220462  [38464/60000]\n",
      "loss: 0.364028  [44864/60000]\n",
      "loss: 0.313383  [51264/60000]\n",
      "loss: 0.427864  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283241 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.225642  [   64/60000]\n",
      "loss: 0.258741  [ 6464/60000]\n",
      "loss: 0.191990  [12864/60000]\n",
      "loss: 0.315770  [19264/60000]\n",
      "loss: 0.216188  [25664/60000]\n",
      "loss: 0.333843  [32064/60000]\n",
      "loss: 0.220266  [38464/60000]\n",
      "loss: 0.363403  [44864/60000]\n",
      "loss: 0.312758  [51264/60000]\n",
      "loss: 0.427306  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282689 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.224346  [   64/60000]\n",
      "loss: 0.258375  [ 6464/60000]\n",
      "loss: 0.191216  [12864/60000]\n",
      "loss: 0.314831  [19264/60000]\n",
      "loss: 0.215689  [25664/60000]\n",
      "loss: 0.333478  [32064/60000]\n",
      "loss: 0.220074  [38464/60000]\n",
      "loss: 0.362793  [44864/60000]\n",
      "loss: 0.312145  [51264/60000]\n",
      "loss: 0.426694  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282145 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.223062  [   64/60000]\n",
      "loss: 0.258040  [ 6464/60000]\n",
      "loss: 0.190455  [12864/60000]\n",
      "loss: 0.313886  [19264/60000]\n",
      "loss: 0.215225  [25664/60000]\n",
      "loss: 0.333125  [32064/60000]\n",
      "loss: 0.219909  [38464/60000]\n",
      "loss: 0.362203  [44864/60000]\n",
      "loss: 0.311505  [51264/60000]\n",
      "loss: 0.426125  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281614 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.221798  [   64/60000]\n",
      "loss: 0.257677  [ 6464/60000]\n",
      "loss: 0.189701  [12864/60000]\n",
      "loss: 0.313027  [19264/60000]\n",
      "loss: 0.214802  [25664/60000]\n",
      "loss: 0.332849  [32064/60000]\n",
      "loss: 0.219737  [38464/60000]\n",
      "loss: 0.361619  [44864/60000]\n",
      "loss: 0.310867  [51264/60000]\n",
      "loss: 0.425563  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281085 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.220542  [   64/60000]\n",
      "loss: 0.257339  [ 6464/60000]\n",
      "loss: 0.188971  [12864/60000]\n",
      "loss: 0.312171  [19264/60000]\n",
      "loss: 0.214481  [25664/60000]\n",
      "loss: 0.332522  [32064/60000]\n",
      "loss: 0.219591  [38464/60000]\n",
      "loss: 0.361058  [44864/60000]\n",
      "loss: 0.310256  [51264/60000]\n",
      "loss: 0.425009  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280559 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.219335  [   64/60000]\n",
      "loss: 0.256987  [ 6464/60000]\n",
      "loss: 0.188248  [12864/60000]\n",
      "loss: 0.311340  [19264/60000]\n",
      "loss: 0.214234  [25664/60000]\n",
      "loss: 0.332202  [32064/60000]\n",
      "loss: 0.219325  [38464/60000]\n",
      "loss: 0.360568  [44864/60000]\n",
      "loss: 0.309516  [51264/60000]\n",
      "loss: 0.424427  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280039 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.218158  [   64/60000]\n",
      "loss: 0.256574  [ 6464/60000]\n",
      "loss: 0.187533  [12864/60000]\n",
      "loss: 0.310527  [19264/60000]\n",
      "loss: 0.214021  [25664/60000]\n",
      "loss: 0.331889  [32064/60000]\n",
      "loss: 0.219024  [38464/60000]\n",
      "loss: 0.360076  [44864/60000]\n",
      "loss: 0.308773  [51264/60000]\n",
      "loss: 0.423909  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279524 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.216977  [   64/60000]\n",
      "loss: 0.256241  [ 6464/60000]\n",
      "loss: 0.186820  [12864/60000]\n",
      "loss: 0.309717  [19264/60000]\n",
      "loss: 0.213729  [25664/60000]\n",
      "loss: 0.331617  [32064/60000]\n",
      "loss: 0.218760  [38464/60000]\n",
      "loss: 0.359597  [44864/60000]\n",
      "loss: 0.308057  [51264/60000]\n",
      "loss: 0.423351  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279021 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.215834  [   64/60000]\n",
      "loss: 0.255895  [ 6464/60000]\n",
      "loss: 0.186115  [12864/60000]\n",
      "loss: 0.308951  [19264/60000]\n",
      "loss: 0.213431  [25664/60000]\n",
      "loss: 0.331347  [32064/60000]\n",
      "loss: 0.218492  [38464/60000]\n",
      "loss: 0.359052  [44864/60000]\n",
      "loss: 0.307341  [51264/60000]\n",
      "loss: 0.422815  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278522 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.214695  [   64/60000]\n",
      "loss: 0.255576  [ 6464/60000]\n",
      "loss: 0.185400  [12864/60000]\n",
      "loss: 0.308152  [19264/60000]\n",
      "loss: 0.213127  [25664/60000]\n",
      "loss: 0.331066  [32064/60000]\n",
      "loss: 0.218237  [38464/60000]\n",
      "loss: 0.358557  [44864/60000]\n",
      "loss: 0.306624  [51264/60000]\n",
      "loss: 0.422289  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278026 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.213554  [   64/60000]\n",
      "loss: 0.255255  [ 6464/60000]\n",
      "loss: 0.184675  [12864/60000]\n",
      "loss: 0.307374  [19264/60000]\n",
      "loss: 0.212848  [25664/60000]\n",
      "loss: 0.330785  [32064/60000]\n",
      "loss: 0.217965  [38464/60000]\n",
      "loss: 0.358060  [44864/60000]\n",
      "loss: 0.305837  [51264/60000]\n",
      "loss: 0.421818  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277535 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.212421  [   64/60000]\n",
      "loss: 0.254953  [ 6464/60000]\n",
      "loss: 0.183950  [12864/60000]\n",
      "loss: 0.306626  [19264/60000]\n",
      "loss: 0.212568  [25664/60000]\n",
      "loss: 0.330524  [32064/60000]\n",
      "loss: 0.217660  [38464/60000]\n",
      "loss: 0.357608  [44864/60000]\n",
      "loss: 0.305109  [51264/60000]\n",
      "loss: 0.421449  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277049 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I want to make a graph of the training error and the test error after X number of epochs!. But wait, I can just record the test error after each epoch! that will save some time...\n",
    "'''\n",
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_acc = np.array([])\n",
    "test_acc = np.array([])\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_acc = np.hstack((train_acc, train(train_dataloader, model, loss_fn, optimizer)))\n",
    "    test_acc = np.hstack((test_acc, test(test_dataloader, model, loss_fn)))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c3512788-0b3f-47ae-8064-d5b3a8ae22a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtrain_acc, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtest_acc))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:645\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m color \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    643\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _default_color(ax\u001b[38;5;241m.\u001b[39mplot, hue, color, kwargs)\n\u001b[1;32m--> 645\u001b[0m p\u001b[38;5;241m.\u001b[39mplot(ax, kwargs)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:423\u001b[0m, in \u001b[0;36m_LinePlotter.plot\u001b[1;34m(self, ax, kws)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# TODO How to handle NA? We don't want NA to propagate through to the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# estimate/CI when some values are present, but we would also like\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# matplotlib to show \"gaps\" in the line when all values are missing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Loop over the semantic subsets and add to the plot\u001b[39;00m\n\u001b[0;32m    422\u001b[0m grouping_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_vars, sub_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_data(grouping_vars, from_comp_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort:\n\u001b[0;32m    426\u001b[0m         sort_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient, other]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\_oldcore.py:1065\u001b[0m, in \u001b[0;36mVectorPlotter.iter_data\u001b[1;34m(self, grouping_vars, reverse, from_comp_data, by_facet, allow_empty, dropna)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m grouping_vars:\n\u001b[0;32m   1063\u001b[0m     grouping_keys\u001b[38;5;241m.\u001b[39mappend(levels\u001b[38;5;241m.\u001b[39mget(var, []))\n\u001b[1;32m-> 1065\u001b[0m iter_keys \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mgrouping_keys)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reverse:\n\u001b[0;32m   1067\u001b[0m     iter_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(iter_keys))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot((1-train_acc, 1-test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
