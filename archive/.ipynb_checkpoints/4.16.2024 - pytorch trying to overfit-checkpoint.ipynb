{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ccde100-704b-443e-90b3-100855cd94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f91008-c4e3-4355-b11e-bfae14fa3aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1747e3843b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4cc1f9-00ed-43de-aba1-dbd6540496bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c4cc97a3-ff6b-488b-83cb-4b86ca414324",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4a125aec-b1d8-482a-8198-645ad6657abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ace84846-9e28-4fe3-bd71-8bcef49864c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1, 1, 28, 28])\n",
      "Shape of y: torch.Size([1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "a3ad7e7c-aa0f-421d-8625-55ed516e6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "7591b7bb-a598-48fb-82ed-44ae0b6d74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    correct /= size\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1bad07ea-741c-49a7-9bfe-9d1497287da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_one(dataloader, model, loss_fn, optimizer):\n",
    "    X, y1 = next(iter(dataloader))\n",
    "    X =  X[0][None,:]\n",
    "    y1 = y1[None,:][:,0]\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "853730be-4cd7-4ac3-8045-ccbae7c1391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    x1, y1 = next(iter(dataloader))\n",
    "    x1 =  x1[0][None,:]\n",
    "    y1 = y1[None,:][:,0]\n",
    "    pred = model(x1)\n",
    "    loss = loss_fn(pred, y1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "2f044e20-dd72-404d-97cc-2657695623c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(train_dataloader))\n",
    "x1 =  x1[0][None,:]\n",
    "y1 = y1[None,:][:,0]\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ea2e893f-871b-4c18-89f5-479029789b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_one(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "b9037264-c6cb-42e5-8151-8bf7ad2e8727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn_image as isns\n",
    "isns.imgplot(x1.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "3e476d3d-7986-4687-b5a3-9110299344ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "402cd606-4ed8-4ea8-b61d-4be15af43749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "660553e4-d864-4d13-863a-c02e9e998fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4574, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "471a1013-f7bb-4ed9-bd1c-f8348ce16e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0112,  0.0015,  0.0017, -0.0255, -0.0140, -0.0105, -0.0166, -0.0134,\n",
       "         0.0040,  0.0343, -0.0148, -0.0309,  0.0098,  0.0320, -0.0349,  0.0313,\n",
       "         0.0259,  0.0109])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "d173ab29-d37b-4ca7-85a6-5a40ebb213ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1423, -0.0627, -0.0509, -0.2057, -0.0989, -0.1230,  0.0110,  0.2337,\n",
       "         0.1429,  0.1193])"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "67972596-e89f-490a-9b7e-435938def73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
       "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
       "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
       "        ...,\n",
       "        [-0.0215,  0.0106,  0.0308,  ..., -0.0199,  0.0161, -0.0342],\n",
       "        [ 0.0350, -0.0297, -0.0037,  ...,  0.0171,  0.0238, -0.0001],\n",
       "        [ 0.0085,  0.0223, -0.0324,  ..., -0.0296,  0.0182, -0.0296]])"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "e3f5553c-0fe8-4607-91e7-6ab4febd60d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1390, -0.1494, -0.2147,  0.2142,  0.1165, -0.1546, -0.1654,  0.0631,\n",
       "          0.1544,  0.2064,  0.0297,  0.1380,  0.0920,  0.0833, -0.1180, -0.1707,\n",
       "          0.2289,  0.0489],\n",
       "        [-0.2119, -0.0872,  0.0234,  0.0395, -0.1763,  0.2059, -0.2174,  0.1155,\n",
       "         -0.1042, -0.1669,  0.1042, -0.0565, -0.0937, -0.0247, -0.0842,  0.2194,\n",
       "         -0.2220, -0.0808],\n",
       "        [-0.0286, -0.0655, -0.1446,  0.0312,  0.0047,  0.1498,  0.0774,  0.1614,\n",
       "         -0.1665,  0.1966, -0.0986, -0.0030,  0.0868, -0.0487, -0.0625,  0.1071,\n",
       "          0.1698, -0.1533],\n",
       "        [ 0.1358,  0.1561,  0.0980,  0.0758,  0.1265,  0.2233,  0.1601,  0.0075,\n",
       "         -0.1609, -0.0058, -0.1355,  0.2079, -0.0700,  0.0170,  0.0997,  0.0820,\n",
       "         -0.1933, -0.1521],\n",
       "        [ 0.0403, -0.1731,  0.0918,  0.0625, -0.0918, -0.1864,  0.0361, -0.1316,\n",
       "          0.2262,  0.1522,  0.1508,  0.1103,  0.0919, -0.1769,  0.2244, -0.0482,\n",
       "          0.1366,  0.2164],\n",
       "        [ 0.0454, -0.1707, -0.0112, -0.1373, -0.1919,  0.1684, -0.0890,  0.1894,\n",
       "         -0.1368,  0.0977, -0.0158, -0.0262,  0.1361, -0.2134,  0.1971, -0.0481,\n",
       "          0.1065, -0.1399],\n",
       "        [-0.1230, -0.0055, -0.0564, -0.0958,  0.1275, -0.0257, -0.0967, -0.0269,\n",
       "          0.2111, -0.1932, -0.0392, -0.0369,  0.1148,  0.1943, -0.1480, -0.0437,\n",
       "         -0.2194, -0.1743],\n",
       "        [ 0.0884, -0.0257,  0.1745, -0.0224, -0.1076, -0.0778,  0.0250,  0.1510,\n",
       "          0.0193,  0.0538, -0.0828,  0.0895, -0.1591,  0.1464, -0.0102,  0.1980,\n",
       "         -0.1200,  0.2311],\n",
       "        [ 0.1497, -0.1904, -0.0700,  0.2058,  0.0800,  0.1960, -0.0494, -0.1007,\n",
       "          0.1563, -0.0707,  0.2137,  0.1376,  0.2347,  0.1778, -0.0700,  0.1988,\n",
       "         -0.1829,  0.2306],\n",
       "        [-0.0006, -0.1074,  0.0688,  0.1265,  0.0111,  0.1675, -0.0168, -0.2316,\n",
       "          0.1520,  0.0021,  0.1323, -0.1164,  0.1540,  0.0885, -0.1370, -0.2191,\n",
       "         -0.0079,  0.0439]])"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "c776ea7d-454a-4bed-8a5e-267f98e3ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1448, -0.0842, -0.0713, -0.1799, -0.0463, -0.1540,  0.0068,  0.2833,\n",
       "          0.2197,  0.0625]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "10b4f616-7310-4676-88c3-d769295865c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0453, -0.1708, -0.0112, -0.1373, -0.1919,  0.1684, -0.0890,  0.1894,\n",
       "         -0.1370,  0.0977, -0.0159, -0.0262,  0.1361, -0.2134,  0.1970, -0.0483,\n",
       "          0.1064, -0.1399]])"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['linear_relu_stack.2.weight'][y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e96a9e85-3749-4b63-b873-3ca22e3cd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "c25197a6-4b17-47a8-bcd0-5c86331f55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f3d5f233-e32a-44a5-bac3-16f85ee389d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "bc893a84-cffa-443f-b260-62215e7cdcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.311471  [  128/60000]\n",
      "loss: 2.300783  [12928/60000]\n",
      "loss: 2.265585  [25728/60000]\n",
      "loss: 2.248653  [38528/60000]\n",
      "loss: 2.184186  [51328/60000]\n",
      "Training Error: \n",
      " Accuracy: 14.1%\n",
      "Test Error: \n",
      " Accuracy: 18.6%, Avg loss: 2.201106 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.186"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataloader, model, loss_fn, optimizer)\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696197eb-d4c5-4a1a-870f-e9d7e644b404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4d0f7e0b-1a36-4bd9-86dd-6605ab6bd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 19.9%\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 47.5%\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n",
      "loss: 1.609467  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 58.6%\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.557949  [   64/60000]\n",
      "loss: 1.491758  [ 6464/60000]\n",
      "loss: 1.660951  [12864/60000]\n",
      "loss: 1.377765  [19264/60000]\n",
      "loss: 1.444292  [25664/60000]\n",
      "loss: 1.447927  [32064/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[186], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      6\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf90d4-a792-4e0e-a068-8254fd746dcc",
   "metadata": {},
   "source": [
    "## Overfitting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7ad4c2e2-4a86-49d1-8048-1a12440ac1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 19.9%\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 47.5%\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n",
      "loss: 1.609467  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 58.6%\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.557949  [   64/60000]\n",
      "loss: 1.491758  [ 6464/60000]\n",
      "loss: 1.660951  [12864/60000]\n",
      "loss: 1.377765  [19264/60000]\n",
      "loss: 1.444292  [25664/60000]\n",
      "loss: 1.447927  [32064/60000]\n",
      "loss: 1.356688  [38464/60000]\n",
      "loss: 1.537228  [44864/60000]\n",
      "loss: 1.358394  [51264/60000]\n",
      "loss: 1.372089  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 66.5%\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.318120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.321148  [   64/60000]\n",
      "loss: 1.245712  [ 6464/60000]\n",
      "loss: 1.394385  [12864/60000]\n",
      "loss: 1.151553  [19264/60000]\n",
      "loss: 1.197075  [25664/60000]\n",
      "loss: 1.215134  [32064/60000]\n",
      "loss: 1.118639  [38464/60000]\n",
      "loss: 1.316757  [44864/60000]\n",
      "loss: 1.153122  [51264/60000]\n",
      "loss: 1.170352  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 72.7%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 1.109394 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.132133  [   64/60000]\n",
      "loss: 1.050473  [ 6464/60000]\n",
      "loss: 1.166535  [12864/60000]\n",
      "loss: 0.973874  [19264/60000]\n",
      "loss: 1.005295  [25664/60000]\n",
      "loss: 1.031641  [32064/60000]\n",
      "loss: 0.938574  [38464/60000]\n",
      "loss: 1.139525  [44864/60000]\n",
      "loss: 0.997379  [51264/60000]\n",
      "loss: 1.014367  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 76.8%\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.952216 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.989064  [   64/60000]\n",
      "loss: 0.902320  [ 6464/60000]\n",
      "loss: 0.988564  [12864/60000]\n",
      "loss: 0.843259  [19264/60000]\n",
      "loss: 0.862754  [25664/60000]\n",
      "loss: 0.892941  [32064/60000]\n",
      "loss: 0.806622  [38464/60000]\n",
      "loss: 1.004429  [44864/60000]\n",
      "loss: 0.880611  [51264/60000]\n",
      "loss: 0.898888  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 79.5%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.836298 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.881625  [   64/60000]\n",
      "loss: 0.789953  [ 6464/60000]\n",
      "loss: 0.853873  [12864/60000]\n",
      "loss: 0.748737  [19264/60000]\n",
      "loss: 0.757954  [25664/60000]\n",
      "loss: 0.789745  [32064/60000]\n",
      "loss: 0.709818  [38464/60000]\n",
      "loss: 0.902217  [44864/60000]\n",
      "loss: 0.792498  [51264/60000]\n",
      "loss: 0.814121  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 81.4%\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.749816 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.799808  [   64/60000]\n",
      "loss: 0.703597  [ 6464/60000]\n",
      "loss: 0.751970  [12864/60000]\n",
      "loss: 0.679617  [19264/60000]\n",
      "loss: 0.678745  [25664/60000]\n",
      "loss: 0.712081  [32064/60000]\n",
      "loss: 0.636931  [38464/60000]\n",
      "loss: 0.824047  [44864/60000]\n",
      "loss: 0.724426  [51264/60000]\n",
      "loss: 0.750376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 82.7%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.683793 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.736062  [   64/60000]\n",
      "loss: 0.636310  [ 6464/60000]\n",
      "loss: 0.674065  [12864/60000]\n",
      "loss: 0.627934  [19264/60000]\n",
      "loss: 0.616891  [25664/60000]\n",
      "loss: 0.652553  [32064/60000]\n",
      "loss: 0.580294  [38464/60000]\n",
      "loss: 0.763228  [44864/60000]\n",
      "loss: 0.670565  [51264/60000]\n",
      "loss: 0.701475  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 83.9%\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.632073 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.685212  [   64/60000]\n",
      "loss: 0.582800  [ 6464/60000]\n",
      "loss: 0.613257  [12864/60000]\n",
      "loss: 0.588443  [19264/60000]\n",
      "loss: 0.567234  [25664/60000]\n",
      "loss: 0.606197  [32064/60000]\n",
      "loss: 0.535110  [38464/60000]\n",
      "loss: 0.714983  [44864/60000]\n",
      "loss: 0.627312  [51264/60000]\n",
      "loss: 0.663166  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 84.7%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.590623 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.643741  [   64/60000]\n",
      "loss: 0.539546  [ 6464/60000]\n",
      "loss: 0.564672  [12864/60000]\n",
      "loss: 0.557566  [19264/60000]\n",
      "loss: 0.526341  [25664/60000]\n",
      "loss: 0.569478  [32064/60000]\n",
      "loss: 0.498467  [38464/60000]\n",
      "loss: 0.676008  [44864/60000]\n",
      "loss: 0.591738  [51264/60000]\n",
      "loss: 0.632310  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 85.4%\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.556767 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.609224  [   64/60000]\n",
      "loss: 0.504055  [ 6464/60000]\n",
      "loss: 0.525274  [12864/60000]\n",
      "loss: 0.532958  [19264/60000]\n",
      "loss: 0.492195  [25664/60000]\n",
      "loss: 0.539940  [32064/60000]\n",
      "loss: 0.468215  [38464/60000]\n",
      "loss: 0.643991  [44864/60000]\n",
      "loss: 0.562275  [51264/60000]\n",
      "loss: 0.607107  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.0%\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.528659 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.580129  [   64/60000]\n",
      "loss: 0.474578  [ 6464/60000]\n",
      "loss: 0.492751  [12864/60000]\n",
      "loss: 0.512964  [19264/60000]\n",
      "loss: 0.463309  [25664/60000]\n",
      "loss: 0.515848  [32064/60000]\n",
      "loss: 0.442773  [38464/60000]\n",
      "loss: 0.617319  [44864/60000]\n",
      "loss: 0.537686  [51264/60000]\n",
      "loss: 0.586342  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.5%\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.505008 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.555234  [   64/60000]\n",
      "loss: 0.449872  [ 6464/60000]\n",
      "loss: 0.465381  [12864/60000]\n",
      "loss: 0.496506  [19264/60000]\n",
      "loss: 0.438663  [25664/60000]\n",
      "loss: 0.495941  [32064/60000]\n",
      "loss: 0.421250  [38464/60000]\n",
      "loss: 0.594771  [44864/60000]\n",
      "loss: 0.517012  [51264/60000]\n",
      "loss: 0.569075  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.0%\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.484894 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.533636  [   64/60000]\n",
      "loss: 0.428864  [ 6464/60000]\n",
      "loss: 0.442080  [12864/60000]\n",
      "loss: 0.482801  [19264/60000]\n",
      "loss: 0.417484  [25664/60000]\n",
      "loss: 0.479340  [32064/60000]\n",
      "loss: 0.402846  [38464/60000]\n",
      "loss: 0.575419  [44864/60000]\n",
      "loss: 0.499393  [51264/60000]\n",
      "loss: 0.554512  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.4%\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.467629 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.514752  [   64/60000]\n",
      "loss: 0.410934  [ 6464/60000]\n",
      "loss: 0.422006  [12864/60000]\n",
      "loss: 0.471228  [19264/60000]\n",
      "loss: 0.399241  [25664/60000]\n",
      "loss: 0.465358  [32064/60000]\n",
      "loss: 0.386991  [38464/60000]\n",
      "loss: 0.558591  [44864/60000]\n",
      "loss: 0.484281  [51264/60000]\n",
      "loss: 0.542164  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.8%\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.452679 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.498072  [   64/60000]\n",
      "loss: 0.395538  [ 6464/60000]\n",
      "loss: 0.404541  [12864/60000]\n",
      "loss: 0.461305  [19264/60000]\n",
      "loss: 0.383379  [25664/60000]\n",
      "loss: 0.453440  [32064/60000]\n",
      "loss: 0.373245  [38464/60000]\n",
      "loss: 0.543806  [44864/60000]\n",
      "loss: 0.471273  [51264/60000]\n",
      "loss: 0.531639  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.439641 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.483209  [   64/60000]\n",
      "loss: 0.382195  [ 6464/60000]\n",
      "loss: 0.389225  [12864/60000]\n",
      "loss: 0.452666  [19264/60000]\n",
      "loss: 0.369548  [25664/60000]\n",
      "loss: 0.443190  [32064/60000]\n",
      "loss: 0.361243  [38464/60000]\n",
      "loss: 0.530696  [44864/60000]\n",
      "loss: 0.459930  [51264/60000]\n",
      "loss: 0.522511  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.3%\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.428185 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.469908  [   64/60000]\n",
      "loss: 0.370535  [ 6464/60000]\n",
      "loss: 0.375691  [12864/60000]\n",
      "loss: 0.445108  [19264/60000]\n",
      "loss: 0.357461  [25664/60000]\n",
      "loss: 0.434370  [32064/60000]\n",
      "loss: 0.350595  [38464/60000]\n",
      "loss: 0.519020  [44864/60000]\n",
      "loss: 0.449971  [51264/60000]\n",
      "loss: 0.514630  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.5%\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.418057 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.457895  [   64/60000]\n",
      "loss: 0.360293  [ 6464/60000]\n",
      "loss: 0.363641  [12864/60000]\n",
      "loss: 0.438435  [19264/60000]\n",
      "loss: 0.346842  [25664/60000]\n",
      "loss: 0.426683  [32064/60000]\n",
      "loss: 0.341132  [38464/60000]\n",
      "loss: 0.508522  [44864/60000]\n",
      "loss: 0.441188  [51264/60000]\n",
      "loss: 0.507772  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.7%\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.409051 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.446948  [   64/60000]\n",
      "loss: 0.351254  [ 6464/60000]\n",
      "loss: 0.352856  [12864/60000]\n",
      "loss: 0.432484  [19264/60000]\n",
      "loss: 0.337381  [25664/60000]\n",
      "loss: 0.419940  [32064/60000]\n",
      "loss: 0.332631  [38464/60000]\n",
      "loss: 0.499001  [44864/60000]\n",
      "loss: 0.433327  [51264/60000]\n",
      "loss: 0.501830  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.8%\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.400999 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.436893  [   64/60000]\n",
      "loss: 0.343207  [ 6464/60000]\n",
      "loss: 0.343160  [12864/60000]\n",
      "loss: 0.427129  [19264/60000]\n",
      "loss: 0.328976  [25664/60000]\n",
      "loss: 0.414029  [32064/60000]\n",
      "loss: 0.324936  [38464/60000]\n",
      "loss: 0.490381  [44864/60000]\n",
      "loss: 0.426376  [51264/60000]\n",
      "loss: 0.496547  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.0%\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.393751 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.427625  [   64/60000]\n",
      "loss: 0.336079  [ 6464/60000]\n",
      "loss: 0.334352  [12864/60000]\n",
      "loss: 0.422218  [19264/60000]\n",
      "loss: 0.321472  [25664/60000]\n",
      "loss: 0.408761  [32064/60000]\n",
      "loss: 0.317975  [38464/60000]\n",
      "loss: 0.482516  [44864/60000]\n",
      "loss: 0.420126  [51264/60000]\n",
      "loss: 0.491938  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.1%\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.387207 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.419028  [   64/60000]\n",
      "loss: 0.329684  [ 6464/60000]\n",
      "loss: 0.326338  [12864/60000]\n",
      "loss: 0.417774  [19264/60000]\n",
      "loss: 0.314757  [25664/60000]\n",
      "loss: 0.404038  [32064/60000]\n",
      "loss: 0.311657  [38464/60000]\n",
      "loss: 0.475334  [44864/60000]\n",
      "loss: 0.414519  [51264/60000]\n",
      "loss: 0.487707  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.2%\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.381264 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.411041  [   64/60000]\n",
      "loss: 0.323946  [ 6464/60000]\n",
      "loss: 0.319070  [12864/60000]\n",
      "loss: 0.413676  [19264/60000]\n",
      "loss: 0.308720  [25664/60000]\n",
      "loss: 0.399793  [32064/60000]\n",
      "loss: 0.305919  [38464/60000]\n",
      "loss: 0.468747  [44864/60000]\n",
      "loss: 0.409420  [51264/60000]\n",
      "loss: 0.483958  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.3%\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.375842 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.403606  [   64/60000]\n",
      "loss: 0.318770  [ 6464/60000]\n",
      "loss: 0.312419  [12864/60000]\n",
      "loss: 0.409868  [19264/60000]\n",
      "loss: 0.303256  [25664/60000]\n",
      "loss: 0.395937  [32064/60000]\n",
      "loss: 0.300664  [38464/60000]\n",
      "loss: 0.462695  [44864/60000]\n",
      "loss: 0.404762  [51264/60000]\n",
      "loss: 0.480620  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.4%\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.370872 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.396618  [   64/60000]\n",
      "loss: 0.314116  [ 6464/60000]\n",
      "loss: 0.306286  [12864/60000]\n",
      "loss: 0.406302  [19264/60000]\n",
      "loss: 0.298277  [25664/60000]\n",
      "loss: 0.392432  [32064/60000]\n",
      "loss: 0.295827  [38464/60000]\n",
      "loss: 0.457135  [44864/60000]\n",
      "loss: 0.400491  [51264/60000]\n",
      "loss: 0.477629  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.5%\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.366304 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.390042  [   64/60000]\n",
      "loss: 0.309918  [ 6464/60000]\n",
      "loss: 0.300610  [12864/60000]\n",
      "loss: 0.402982  [19264/60000]\n",
      "loss: 0.293758  [25664/60000]\n",
      "loss: 0.389216  [32064/60000]\n",
      "loss: 0.291381  [38464/60000]\n",
      "loss: 0.452016  [44864/60000]\n",
      "loss: 0.396515  [51264/60000]\n",
      "loss: 0.474933  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.6%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.362095 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.383802  [   64/60000]\n",
      "loss: 0.306093  [ 6464/60000]\n",
      "loss: 0.295327  [12864/60000]\n",
      "loss: 0.399895  [19264/60000]\n",
      "loss: 0.289599  [25664/60000]\n",
      "loss: 0.386275  [32064/60000]\n",
      "loss: 0.287284  [38464/60000]\n",
      "loss: 0.447272  [44864/60000]\n",
      "loss: 0.392873  [51264/60000]\n",
      "loss: 0.472503  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.358196 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.377893  [   64/60000]\n",
      "loss: 0.302624  [ 6464/60000]\n",
      "loss: 0.290400  [12864/60000]\n",
      "loss: 0.396970  [19264/60000]\n",
      "loss: 0.285810  [25664/60000]\n",
      "loss: 0.383609  [32064/60000]\n",
      "loss: 0.283493  [38464/60000]\n",
      "loss: 0.442835  [44864/60000]\n",
      "loss: 0.389476  [51264/60000]\n",
      "loss: 0.470337  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.354576 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.372278  [   64/60000]\n",
      "loss: 0.299469  [ 6464/60000]\n",
      "loss: 0.285788  [12864/60000]\n",
      "loss: 0.394212  [19264/60000]\n",
      "loss: 0.282320  [25664/60000]\n",
      "loss: 0.381166  [32064/60000]\n",
      "loss: 0.279971  [38464/60000]\n",
      "loss: 0.438735  [44864/60000]\n",
      "loss: 0.386328  [51264/60000]\n",
      "loss: 0.468376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.351200 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.366960  [   64/60000]\n",
      "loss: 0.296603  [ 6464/60000]\n",
      "loss: 0.281467  [12864/60000]\n",
      "loss: 0.391569  [19264/60000]\n",
      "loss: 0.279109  [25664/60000]\n",
      "loss: 0.378893  [32064/60000]\n",
      "loss: 0.276695  [38464/60000]\n",
      "loss: 0.434914  [44864/60000]\n",
      "loss: 0.383394  [51264/60000]\n",
      "loss: 0.466559  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.348050 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.361929  [   64/60000]\n",
      "loss: 0.293956  [ 6464/60000]\n",
      "loss: 0.277419  [12864/60000]\n",
      "loss: 0.389016  [19264/60000]\n",
      "loss: 0.276153  [25664/60000]\n",
      "loss: 0.376759  [32064/60000]\n",
      "loss: 0.273651  [38464/60000]\n",
      "loss: 0.431349  [44864/60000]\n",
      "loss: 0.380643  [51264/60000]\n",
      "loss: 0.464907  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.9%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.345101 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.357106  [   64/60000]\n",
      "loss: 0.291519  [ 6464/60000]\n",
      "loss: 0.273610  [12864/60000]\n",
      "loss: 0.386581  [19264/60000]\n",
      "loss: 0.273380  [25664/60000]\n",
      "loss: 0.374776  [32064/60000]\n",
      "loss: 0.270817  [38464/60000]\n",
      "loss: 0.428014  [44864/60000]\n",
      "loss: 0.378020  [51264/60000]\n",
      "loss: 0.463414  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.342331 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.352486  [   64/60000]\n",
      "loss: 0.289283  [ 6464/60000]\n",
      "loss: 0.270017  [12864/60000]\n",
      "loss: 0.384253  [19264/60000]\n",
      "loss: 0.270773  [25664/60000]\n",
      "loss: 0.372907  [32064/60000]\n",
      "loss: 0.268161  [38464/60000]\n",
      "loss: 0.424887  [44864/60000]\n",
      "loss: 0.375553  [51264/60000]\n",
      "loss: 0.462046  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.339719 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.348055  [   64/60000]\n",
      "loss: 0.287234  [ 6464/60000]\n",
      "loss: 0.266590  [12864/60000]\n",
      "loss: 0.381983  [19264/60000]\n",
      "loss: 0.268346  [25664/60000]\n",
      "loss: 0.371140  [32064/60000]\n",
      "loss: 0.265664  [38464/60000]\n",
      "loss: 0.421982  [44864/60000]\n",
      "loss: 0.373270  [51264/60000]\n",
      "loss: 0.460792  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.337255 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.343811  [   64/60000]\n",
      "loss: 0.285360  [ 6464/60000]\n",
      "loss: 0.263343  [12864/60000]\n",
      "loss: 0.379786  [19264/60000]\n",
      "loss: 0.266061  [25664/60000]\n",
      "loss: 0.369476  [32064/60000]\n",
      "loss: 0.263358  [38464/60000]\n",
      "loss: 0.419190  [44864/60000]\n",
      "loss: 0.371083  [51264/60000]\n",
      "loss: 0.459650  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.334935 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.339731  [   64/60000]\n",
      "loss: 0.283560  [ 6464/60000]\n",
      "loss: 0.260268  [12864/60000]\n",
      "loss: 0.377673  [19264/60000]\n",
      "loss: 0.263876  [25664/60000]\n",
      "loss: 0.368027  [32064/60000]\n",
      "loss: 0.261131  [38464/60000]\n",
      "loss: 0.416569  [44864/60000]\n",
      "loss: 0.369035  [51264/60000]\n",
      "loss: 0.458614  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.332736 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.335784  [   64/60000]\n",
      "loss: 0.281895  [ 6464/60000]\n",
      "loss: 0.257351  [12864/60000]\n",
      "loss: 0.375694  [19264/60000]\n",
      "loss: 0.261864  [25664/60000]\n",
      "loss: 0.366651  [32064/60000]\n",
      "loss: 0.259001  [38464/60000]\n",
      "loss: 0.414089  [44864/60000]\n",
      "loss: 0.367049  [51264/60000]\n",
      "loss: 0.457645  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.330648 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.331939  [   64/60000]\n",
      "loss: 0.280379  [ 6464/60000]\n",
      "loss: 0.254579  [12864/60000]\n",
      "loss: 0.373776  [19264/60000]\n",
      "loss: 0.259961  [25664/60000]\n",
      "loss: 0.365373  [32064/60000]\n",
      "loss: 0.257010  [38464/60000]\n",
      "loss: 0.411737  [44864/60000]\n",
      "loss: 0.365146  [51264/60000]\n",
      "loss: 0.456740  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.328663 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.328240  [   64/60000]\n",
      "loss: 0.278977  [ 6464/60000]\n",
      "loss: 0.251932  [12864/60000]\n",
      "loss: 0.371922  [19264/60000]\n",
      "loss: 0.258162  [25664/60000]\n",
      "loss: 0.364164  [32064/60000]\n",
      "loss: 0.255108  [38464/60000]\n",
      "loss: 0.409504  [44864/60000]\n",
      "loss: 0.363297  [51264/60000]\n",
      "loss: 0.455878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.326776 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.324579  [   64/60000]\n",
      "loss: 0.277670  [ 6464/60000]\n",
      "loss: 0.249412  [12864/60000]\n",
      "loss: 0.370111  [19264/60000]\n",
      "loss: 0.256447  [25664/60000]\n",
      "loss: 0.363039  [32064/60000]\n",
      "loss: 0.253300  [38464/60000]\n",
      "loss: 0.407395  [44864/60000]\n",
      "loss: 0.361510  [51264/60000]\n",
      "loss: 0.454985  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.324981 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.321034  [   64/60000]\n",
      "loss: 0.276453  [ 6464/60000]\n",
      "loss: 0.247065  [12864/60000]\n",
      "loss: 0.368344  [19264/60000]\n",
      "loss: 0.254840  [25664/60000]\n",
      "loss: 0.361960  [32064/60000]\n",
      "loss: 0.251577  [38464/60000]\n",
      "loss: 0.405412  [44864/60000]\n",
      "loss: 0.359763  [51264/60000]\n",
      "loss: 0.454153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.323264 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.317589  [   64/60000]\n",
      "loss: 0.275312  [ 6464/60000]\n",
      "loss: 0.244839  [12864/60000]\n",
      "loss: 0.366690  [19264/60000]\n",
      "loss: 0.253297  [25664/60000]\n",
      "loss: 0.360966  [32064/60000]\n",
      "loss: 0.249923  [38464/60000]\n",
      "loss: 0.403501  [44864/60000]\n",
      "loss: 0.358088  [51264/60000]\n",
      "loss: 0.453323  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.321621 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.314243  [   64/60000]\n",
      "loss: 0.274265  [ 6464/60000]\n",
      "loss: 0.242711  [12864/60000]\n",
      "loss: 0.365063  [19264/60000]\n",
      "loss: 0.251779  [25664/60000]\n",
      "loss: 0.359882  [32064/60000]\n",
      "loss: 0.248368  [38464/60000]\n",
      "loss: 0.401665  [44864/60000]\n",
      "loss: 0.356466  [51264/60000]\n",
      "loss: 0.452569  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.320046 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.310971  [   64/60000]\n",
      "loss: 0.273295  [ 6464/60000]\n",
      "loss: 0.240691  [12864/60000]\n",
      "loss: 0.363443  [19264/60000]\n",
      "loss: 0.250335  [25664/60000]\n",
      "loss: 0.358840  [32064/60000]\n",
      "loss: 0.246891  [38464/60000]\n",
      "loss: 0.399927  [44864/60000]\n",
      "loss: 0.354897  [51264/60000]\n",
      "loss: 0.451816  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.318538 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.307802  [   64/60000]\n",
      "loss: 0.272382  [ 6464/60000]\n",
      "loss: 0.238761  [12864/60000]\n",
      "loss: 0.361861  [19264/60000]\n",
      "loss: 0.248977  [25664/60000]\n",
      "loss: 0.357790  [32064/60000]\n",
      "loss: 0.245520  [38464/60000]\n",
      "loss: 0.398207  [44864/60000]\n",
      "loss: 0.353408  [51264/60000]\n",
      "loss: 0.451085  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.317088 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.304733  [   64/60000]\n",
      "loss: 0.271497  [ 6464/60000]\n",
      "loss: 0.236913  [12864/60000]\n",
      "loss: 0.360322  [19264/60000]\n",
      "loss: 0.247720  [25664/60000]\n",
      "loss: 0.356810  [32064/60000]\n",
      "loss: 0.244182  [38464/60000]\n",
      "loss: 0.396583  [44864/60000]\n",
      "loss: 0.351987  [51264/60000]\n",
      "loss: 0.450442  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.315697 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.301729  [   64/60000]\n",
      "loss: 0.270657  [ 6464/60000]\n",
      "loss: 0.235146  [12864/60000]\n",
      "loss: 0.358837  [19264/60000]\n",
      "loss: 0.246505  [25664/60000]\n",
      "loss: 0.355904  [32064/60000]\n",
      "loss: 0.242935  [38464/60000]\n",
      "loss: 0.395000  [44864/60000]\n",
      "loss: 0.350593  [51264/60000]\n",
      "loss: 0.449746  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.314361 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.298826  [   64/60000]\n",
      "loss: 0.269848  [ 6464/60000]\n",
      "loss: 0.233430  [12864/60000]\n",
      "loss: 0.357380  [19264/60000]\n",
      "loss: 0.245385  [25664/60000]\n",
      "loss: 0.355014  [32064/60000]\n",
      "loss: 0.241703  [38464/60000]\n",
      "loss: 0.393487  [44864/60000]\n",
      "loss: 0.349199  [51264/60000]\n",
      "loss: 0.449051  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.313068 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.295966  [   64/60000]\n",
      "loss: 0.269110  [ 6464/60000]\n",
      "loss: 0.231742  [12864/60000]\n",
      "loss: 0.355931  [19264/60000]\n",
      "loss: 0.244334  [25664/60000]\n",
      "loss: 0.354139  [32064/60000]\n",
      "loss: 0.240559  [38464/60000]\n",
      "loss: 0.392049  [44864/60000]\n",
      "loss: 0.347825  [51264/60000]\n",
      "loss: 0.448461  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.311820 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.293166  [   64/60000]\n",
      "loss: 0.268428  [ 6464/60000]\n",
      "loss: 0.230137  [12864/60000]\n",
      "loss: 0.354495  [19264/60000]\n",
      "loss: 0.243302  [25664/60000]\n",
      "loss: 0.353278  [32064/60000]\n",
      "loss: 0.239469  [38464/60000]\n",
      "loss: 0.390675  [44864/60000]\n",
      "loss: 0.346472  [51264/60000]\n",
      "loss: 0.447874  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.310615 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.290458  [   64/60000]\n",
      "loss: 0.267775  [ 6464/60000]\n",
      "loss: 0.228563  [12864/60000]\n",
      "loss: 0.353103  [19264/60000]\n",
      "loss: 0.242335  [25664/60000]\n",
      "loss: 0.352485  [32064/60000]\n",
      "loss: 0.238412  [38464/60000]\n",
      "loss: 0.389451  [44864/60000]\n",
      "loss: 0.345149  [51264/60000]\n",
      "loss: 0.447307  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.309445 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.287826  [   64/60000]\n",
      "loss: 0.267161  [ 6464/60000]\n",
      "loss: 0.227018  [12864/60000]\n",
      "loss: 0.351709  [19264/60000]\n",
      "loss: 0.241392  [25664/60000]\n",
      "loss: 0.351708  [32064/60000]\n",
      "loss: 0.237416  [38464/60000]\n",
      "loss: 0.388272  [44864/60000]\n",
      "loss: 0.343855  [51264/60000]\n",
      "loss: 0.446767  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.308315 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.285238  [   64/60000]\n",
      "loss: 0.266577  [ 6464/60000]\n",
      "loss: 0.225517  [12864/60000]\n",
      "loss: 0.350297  [19264/60000]\n",
      "loss: 0.240477  [25664/60000]\n",
      "loss: 0.350938  [32064/60000]\n",
      "loss: 0.236480  [38464/60000]\n",
      "loss: 0.387129  [44864/60000]\n",
      "loss: 0.342626  [51264/60000]\n",
      "loss: 0.446221  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.307221 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.282732  [   64/60000]\n",
      "loss: 0.266016  [ 6464/60000]\n",
      "loss: 0.224058  [12864/60000]\n",
      "loss: 0.348947  [19264/60000]\n",
      "loss: 0.239578  [25664/60000]\n",
      "loss: 0.350212  [32064/60000]\n",
      "loss: 0.235580  [38464/60000]\n",
      "loss: 0.386025  [44864/60000]\n",
      "loss: 0.341384  [51264/60000]\n",
      "loss: 0.445606  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.306167 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.280269  [   64/60000]\n",
      "loss: 0.265480  [ 6464/60000]\n",
      "loss: 0.222645  [12864/60000]\n",
      "loss: 0.347616  [19264/60000]\n",
      "loss: 0.238733  [25664/60000]\n",
      "loss: 0.349494  [32064/60000]\n",
      "loss: 0.234751  [38464/60000]\n",
      "loss: 0.384915  [44864/60000]\n",
      "loss: 0.340146  [51264/60000]\n",
      "loss: 0.445001  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.305146 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.277882  [   64/60000]\n",
      "loss: 0.264972  [ 6464/60000]\n",
      "loss: 0.221269  [12864/60000]\n",
      "loss: 0.346307  [19264/60000]\n",
      "loss: 0.237827  [25664/60000]\n",
      "loss: 0.348813  [32064/60000]\n",
      "loss: 0.233939  [38464/60000]\n",
      "loss: 0.383880  [44864/60000]\n",
      "loss: 0.338978  [51264/60000]\n",
      "loss: 0.444390  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.304157 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.275552  [   64/60000]\n",
      "loss: 0.264638  [ 6464/60000]\n",
      "loss: 0.219939  [12864/60000]\n",
      "loss: 0.345044  [19264/60000]\n",
      "loss: 0.236835  [25664/60000]\n",
      "loss: 0.348143  [32064/60000]\n",
      "loss: 0.233142  [38464/60000]\n",
      "loss: 0.382895  [44864/60000]\n",
      "loss: 0.337906  [51264/60000]\n",
      "loss: 0.443790  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.303189 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.273331  [   64/60000]\n",
      "loss: 0.264376  [ 6464/60000]\n",
      "loss: 0.218615  [12864/60000]\n",
      "loss: 0.343765  [19264/60000]\n",
      "loss: 0.235863  [25664/60000]\n",
      "loss: 0.347487  [32064/60000]\n",
      "loss: 0.232413  [38464/60000]\n",
      "loss: 0.381935  [44864/60000]\n",
      "loss: 0.336823  [51264/60000]\n",
      "loss: 0.443205  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.302251 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.271162  [   64/60000]\n",
      "loss: 0.264144  [ 6464/60000]\n",
      "loss: 0.217355  [12864/60000]\n",
      "loss: 0.342524  [19264/60000]\n",
      "loss: 0.234907  [25664/60000]\n",
      "loss: 0.346829  [32064/60000]\n",
      "loss: 0.231697  [38464/60000]\n",
      "loss: 0.381001  [44864/60000]\n",
      "loss: 0.335749  [51264/60000]\n",
      "loss: 0.442647  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.301339 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.269055  [   64/60000]\n",
      "loss: 0.263907  [ 6464/60000]\n",
      "loss: 0.216140  [12864/60000]\n",
      "loss: 0.341370  [19264/60000]\n",
      "loss: 0.233978  [25664/60000]\n",
      "loss: 0.346199  [32064/60000]\n",
      "loss: 0.231012  [38464/60000]\n",
      "loss: 0.380151  [44864/60000]\n",
      "loss: 0.334724  [51264/60000]\n",
      "loss: 0.442129  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.300454 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.266978  [   64/60000]\n",
      "loss: 0.263679  [ 6464/60000]\n",
      "loss: 0.214988  [12864/60000]\n",
      "loss: 0.340233  [19264/60000]\n",
      "loss: 0.233095  [25664/60000]\n",
      "loss: 0.345587  [32064/60000]\n",
      "loss: 0.230390  [38464/60000]\n",
      "loss: 0.379328  [44864/60000]\n",
      "loss: 0.333704  [51264/60000]\n",
      "loss: 0.441612  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.299589 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.264926  [   64/60000]\n",
      "loss: 0.263483  [ 6464/60000]\n",
      "loss: 0.213876  [12864/60000]\n",
      "loss: 0.339137  [19264/60000]\n",
      "loss: 0.232241  [25664/60000]\n",
      "loss: 0.345002  [32064/60000]\n",
      "loss: 0.229791  [38464/60000]\n",
      "loss: 0.378541  [44864/60000]\n",
      "loss: 0.332726  [51264/60000]\n",
      "loss: 0.441093  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.298742 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.262905  [   64/60000]\n",
      "loss: 0.263294  [ 6464/60000]\n",
      "loss: 0.212792  [12864/60000]\n",
      "loss: 0.338017  [19264/60000]\n",
      "loss: 0.231363  [25664/60000]\n",
      "loss: 0.344402  [32064/60000]\n",
      "loss: 0.229191  [38464/60000]\n",
      "loss: 0.377770  [44864/60000]\n",
      "loss: 0.331777  [51264/60000]\n",
      "loss: 0.440520  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.297916 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.260919  [   64/60000]\n",
      "loss: 0.263119  [ 6464/60000]\n",
      "loss: 0.211727  [12864/60000]\n",
      "loss: 0.336916  [19264/60000]\n",
      "loss: 0.230520  [25664/60000]\n",
      "loss: 0.343846  [32064/60000]\n",
      "loss: 0.228613  [38464/60000]\n",
      "loss: 0.377023  [44864/60000]\n",
      "loss: 0.330816  [51264/60000]\n",
      "loss: 0.439982  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.297113 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.258944  [   64/60000]\n",
      "loss: 0.262949  [ 6464/60000]\n",
      "loss: 0.210702  [12864/60000]\n",
      "loss: 0.335893  [19264/60000]\n",
      "loss: 0.229731  [25664/60000]\n",
      "loss: 0.343300  [32064/60000]\n",
      "loss: 0.228051  [38464/60000]\n",
      "loss: 0.376340  [44864/60000]\n",
      "loss: 0.329899  [51264/60000]\n",
      "loss: 0.439411  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.296323 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.257028  [   64/60000]\n",
      "loss: 0.262809  [ 6464/60000]\n",
      "loss: 0.209655  [12864/60000]\n",
      "loss: 0.334821  [19264/60000]\n",
      "loss: 0.228915  [25664/60000]\n",
      "loss: 0.342755  [32064/60000]\n",
      "loss: 0.227524  [38464/60000]\n",
      "loss: 0.375705  [44864/60000]\n",
      "loss: 0.328947  [51264/60000]\n",
      "loss: 0.438878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.295557 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.255128  [   64/60000]\n",
      "loss: 0.262701  [ 6464/60000]\n",
      "loss: 0.208637  [12864/60000]\n",
      "loss: 0.333826  [19264/60000]\n",
      "loss: 0.228120  [25664/60000]\n",
      "loss: 0.342272  [32064/60000]\n",
      "loss: 0.226980  [38464/60000]\n",
      "loss: 0.375117  [44864/60000]\n",
      "loss: 0.328021  [51264/60000]\n",
      "loss: 0.438305  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294802 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.253281  [   64/60000]\n",
      "loss: 0.262628  [ 6464/60000]\n",
      "loss: 0.207632  [12864/60000]\n",
      "loss: 0.332828  [19264/60000]\n",
      "loss: 0.227327  [25664/60000]\n",
      "loss: 0.341767  [32064/60000]\n",
      "loss: 0.226461  [38464/60000]\n",
      "loss: 0.374522  [44864/60000]\n",
      "loss: 0.327119  [51264/60000]\n",
      "loss: 0.437756  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294062 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.251477  [   64/60000]\n",
      "loss: 0.262552  [ 6464/60000]\n",
      "loss: 0.206644  [12864/60000]\n",
      "loss: 0.331845  [19264/60000]\n",
      "loss: 0.226513  [25664/60000]\n",
      "loss: 0.341234  [32064/60000]\n",
      "loss: 0.225962  [38464/60000]\n",
      "loss: 0.373926  [44864/60000]\n",
      "loss: 0.326273  [51264/60000]\n",
      "loss: 0.437202  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.293337 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.249717  [   64/60000]\n",
      "loss: 0.262479  [ 6464/60000]\n",
      "loss: 0.205671  [12864/60000]\n",
      "loss: 0.330861  [19264/60000]\n",
      "loss: 0.225713  [25664/60000]\n",
      "loss: 0.340694  [32064/60000]\n",
      "loss: 0.225486  [38464/60000]\n",
      "loss: 0.373268  [44864/60000]\n",
      "loss: 0.325382  [51264/60000]\n",
      "loss: 0.436692  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.292631 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.247951  [   64/60000]\n",
      "loss: 0.262412  [ 6464/60000]\n",
      "loss: 0.204745  [12864/60000]\n",
      "loss: 0.329921  [19264/60000]\n",
      "loss: 0.224945  [25664/60000]\n",
      "loss: 0.340214  [32064/60000]\n",
      "loss: 0.225020  [38464/60000]\n",
      "loss: 0.372637  [44864/60000]\n",
      "loss: 0.324445  [51264/60000]\n",
      "loss: 0.436153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291939 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.246218  [   64/60000]\n",
      "loss: 0.262327  [ 6464/60000]\n",
      "loss: 0.203799  [12864/60000]\n",
      "loss: 0.329014  [19264/60000]\n",
      "loss: 0.224217  [25664/60000]\n",
      "loss: 0.339700  [32064/60000]\n",
      "loss: 0.224583  [38464/60000]\n",
      "loss: 0.372048  [44864/60000]\n",
      "loss: 0.323575  [51264/60000]\n",
      "loss: 0.435601  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291256 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.244550  [   64/60000]\n",
      "loss: 0.262262  [ 6464/60000]\n",
      "loss: 0.202847  [12864/60000]\n",
      "loss: 0.328070  [19264/60000]\n",
      "loss: 0.223528  [25664/60000]\n",
      "loss: 0.339159  [32064/60000]\n",
      "loss: 0.224138  [38464/60000]\n",
      "loss: 0.371467  [44864/60000]\n",
      "loss: 0.322684  [51264/60000]\n",
      "loss: 0.435070  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.290580 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.242908  [   64/60000]\n",
      "loss: 0.262179  [ 6464/60000]\n",
      "loss: 0.201914  [12864/60000]\n",
      "loss: 0.327168  [19264/60000]\n",
      "loss: 0.222848  [25664/60000]\n",
      "loss: 0.338631  [32064/60000]\n",
      "loss: 0.223747  [38464/60000]\n",
      "loss: 0.370874  [44864/60000]\n",
      "loss: 0.321823  [51264/60000]\n",
      "loss: 0.434495  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289920 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.241296  [   64/60000]\n",
      "loss: 0.262064  [ 6464/60000]\n",
      "loss: 0.200993  [12864/60000]\n",
      "loss: 0.326226  [19264/60000]\n",
      "loss: 0.222183  [25664/60000]\n",
      "loss: 0.338156  [32064/60000]\n",
      "loss: 0.223360  [38464/60000]\n",
      "loss: 0.370214  [44864/60000]\n",
      "loss: 0.320909  [51264/60000]\n",
      "loss: 0.433892  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289270 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.239696  [   64/60000]\n",
      "loss: 0.261996  [ 6464/60000]\n",
      "loss: 0.200119  [12864/60000]\n",
      "loss: 0.325246  [19264/60000]\n",
      "loss: 0.221546  [25664/60000]\n",
      "loss: 0.337743  [32064/60000]\n",
      "loss: 0.223004  [38464/60000]\n",
      "loss: 0.369534  [44864/60000]\n",
      "loss: 0.320051  [51264/60000]\n",
      "loss: 0.433291  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.288631 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.238151  [   64/60000]\n",
      "loss: 0.261889  [ 6464/60000]\n",
      "loss: 0.199259  [12864/60000]\n",
      "loss: 0.324294  [19264/60000]\n",
      "loss: 0.220953  [25664/60000]\n",
      "loss: 0.337353  [32064/60000]\n",
      "loss: 0.222659  [38464/60000]\n",
      "loss: 0.368885  [44864/60000]\n",
      "loss: 0.319209  [51264/60000]\n",
      "loss: 0.432661  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.287999 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.236660  [   64/60000]\n",
      "loss: 0.261770  [ 6464/60000]\n",
      "loss: 0.198410  [12864/60000]\n",
      "loss: 0.323337  [19264/60000]\n",
      "loss: 0.220353  [25664/60000]\n",
      "loss: 0.336925  [32064/60000]\n",
      "loss: 0.222322  [38464/60000]\n",
      "loss: 0.368301  [44864/60000]\n",
      "loss: 0.318417  [51264/60000]\n",
      "loss: 0.432048  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.287371 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.235225  [   64/60000]\n",
      "loss: 0.261391  [ 6464/60000]\n",
      "loss: 0.197565  [12864/60000]\n",
      "loss: 0.322295  [19264/60000]\n",
      "loss: 0.219759  [25664/60000]\n",
      "loss: 0.336425  [32064/60000]\n",
      "loss: 0.222034  [38464/60000]\n",
      "loss: 0.367705  [44864/60000]\n",
      "loss: 0.317636  [51264/60000]\n",
      "loss: 0.431499  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286755 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.233788  [   64/60000]\n",
      "loss: 0.260947  [ 6464/60000]\n",
      "loss: 0.196761  [12864/60000]\n",
      "loss: 0.321329  [19264/60000]\n",
      "loss: 0.219211  [25664/60000]\n",
      "loss: 0.336022  [32064/60000]\n",
      "loss: 0.221756  [38464/60000]\n",
      "loss: 0.367067  [44864/60000]\n",
      "loss: 0.316805  [51264/60000]\n",
      "loss: 0.430895  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286148 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.232377  [   64/60000]\n",
      "loss: 0.260517  [ 6464/60000]\n",
      "loss: 0.195978  [12864/60000]\n",
      "loss: 0.320401  [19264/60000]\n",
      "loss: 0.218679  [25664/60000]\n",
      "loss: 0.335649  [32064/60000]\n",
      "loss: 0.221502  [38464/60000]\n",
      "loss: 0.366469  [44864/60000]\n",
      "loss: 0.316112  [51264/60000]\n",
      "loss: 0.430327  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.285545 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.230961  [   64/60000]\n",
      "loss: 0.260129  [ 6464/60000]\n",
      "loss: 0.195204  [12864/60000]\n",
      "loss: 0.319471  [19264/60000]\n",
      "loss: 0.218125  [25664/60000]\n",
      "loss: 0.335283  [32064/60000]\n",
      "loss: 0.221215  [38464/60000]\n",
      "loss: 0.365885  [44864/60000]\n",
      "loss: 0.315432  [51264/60000]\n",
      "loss: 0.429731  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284949 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.229576  [   64/60000]\n",
      "loss: 0.259770  [ 6464/60000]\n",
      "loss: 0.194432  [12864/60000]\n",
      "loss: 0.318548  [19264/60000]\n",
      "loss: 0.217614  [25664/60000]\n",
      "loss: 0.334922  [32064/60000]\n",
      "loss: 0.220915  [38464/60000]\n",
      "loss: 0.365316  [44864/60000]\n",
      "loss: 0.314743  [51264/60000]\n",
      "loss: 0.429119  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284369 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.228234  [   64/60000]\n",
      "loss: 0.259448  [ 6464/60000]\n",
      "loss: 0.193597  [12864/60000]\n",
      "loss: 0.317639  [19264/60000]\n",
      "loss: 0.217113  [25664/60000]\n",
      "loss: 0.334547  [32064/60000]\n",
      "loss: 0.220682  [38464/60000]\n",
      "loss: 0.364663  [44864/60000]\n",
      "loss: 0.314041  [51264/60000]\n",
      "loss: 0.428491  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283801 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.226927  [   64/60000]\n",
      "loss: 0.259090  [ 6464/60000]\n",
      "loss: 0.192772  [12864/60000]\n",
      "loss: 0.316695  [19264/60000]\n",
      "loss: 0.216646  [25664/60000]\n",
      "loss: 0.334203  [32064/60000]\n",
      "loss: 0.220462  [38464/60000]\n",
      "loss: 0.364028  [44864/60000]\n",
      "loss: 0.313383  [51264/60000]\n",
      "loss: 0.427864  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283241 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.225642  [   64/60000]\n",
      "loss: 0.258741  [ 6464/60000]\n",
      "loss: 0.191990  [12864/60000]\n",
      "loss: 0.315770  [19264/60000]\n",
      "loss: 0.216188  [25664/60000]\n",
      "loss: 0.333843  [32064/60000]\n",
      "loss: 0.220266  [38464/60000]\n",
      "loss: 0.363403  [44864/60000]\n",
      "loss: 0.312758  [51264/60000]\n",
      "loss: 0.427306  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282689 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.224346  [   64/60000]\n",
      "loss: 0.258375  [ 6464/60000]\n",
      "loss: 0.191216  [12864/60000]\n",
      "loss: 0.314831  [19264/60000]\n",
      "loss: 0.215689  [25664/60000]\n",
      "loss: 0.333478  [32064/60000]\n",
      "loss: 0.220074  [38464/60000]\n",
      "loss: 0.362793  [44864/60000]\n",
      "loss: 0.312145  [51264/60000]\n",
      "loss: 0.426694  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282145 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.223062  [   64/60000]\n",
      "loss: 0.258040  [ 6464/60000]\n",
      "loss: 0.190455  [12864/60000]\n",
      "loss: 0.313886  [19264/60000]\n",
      "loss: 0.215225  [25664/60000]\n",
      "loss: 0.333125  [32064/60000]\n",
      "loss: 0.219909  [38464/60000]\n",
      "loss: 0.362203  [44864/60000]\n",
      "loss: 0.311505  [51264/60000]\n",
      "loss: 0.426125  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281614 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.221798  [   64/60000]\n",
      "loss: 0.257677  [ 6464/60000]\n",
      "loss: 0.189701  [12864/60000]\n",
      "loss: 0.313027  [19264/60000]\n",
      "loss: 0.214802  [25664/60000]\n",
      "loss: 0.332849  [32064/60000]\n",
      "loss: 0.219737  [38464/60000]\n",
      "loss: 0.361619  [44864/60000]\n",
      "loss: 0.310867  [51264/60000]\n",
      "loss: 0.425563  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281085 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.220542  [   64/60000]\n",
      "loss: 0.257339  [ 6464/60000]\n",
      "loss: 0.188971  [12864/60000]\n",
      "loss: 0.312171  [19264/60000]\n",
      "loss: 0.214481  [25664/60000]\n",
      "loss: 0.332522  [32064/60000]\n",
      "loss: 0.219591  [38464/60000]\n",
      "loss: 0.361058  [44864/60000]\n",
      "loss: 0.310256  [51264/60000]\n",
      "loss: 0.425009  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280559 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.219335  [   64/60000]\n",
      "loss: 0.256987  [ 6464/60000]\n",
      "loss: 0.188248  [12864/60000]\n",
      "loss: 0.311340  [19264/60000]\n",
      "loss: 0.214234  [25664/60000]\n",
      "loss: 0.332202  [32064/60000]\n",
      "loss: 0.219325  [38464/60000]\n",
      "loss: 0.360568  [44864/60000]\n",
      "loss: 0.309516  [51264/60000]\n",
      "loss: 0.424427  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280039 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.218158  [   64/60000]\n",
      "loss: 0.256574  [ 6464/60000]\n",
      "loss: 0.187533  [12864/60000]\n",
      "loss: 0.310527  [19264/60000]\n",
      "loss: 0.214021  [25664/60000]\n",
      "loss: 0.331889  [32064/60000]\n",
      "loss: 0.219024  [38464/60000]\n",
      "loss: 0.360076  [44864/60000]\n",
      "loss: 0.308773  [51264/60000]\n",
      "loss: 0.423909  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279524 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.216977  [   64/60000]\n",
      "loss: 0.256241  [ 6464/60000]\n",
      "loss: 0.186820  [12864/60000]\n",
      "loss: 0.309717  [19264/60000]\n",
      "loss: 0.213729  [25664/60000]\n",
      "loss: 0.331617  [32064/60000]\n",
      "loss: 0.218760  [38464/60000]\n",
      "loss: 0.359597  [44864/60000]\n",
      "loss: 0.308057  [51264/60000]\n",
      "loss: 0.423351  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279021 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.215834  [   64/60000]\n",
      "loss: 0.255895  [ 6464/60000]\n",
      "loss: 0.186115  [12864/60000]\n",
      "loss: 0.308951  [19264/60000]\n",
      "loss: 0.213431  [25664/60000]\n",
      "loss: 0.331347  [32064/60000]\n",
      "loss: 0.218492  [38464/60000]\n",
      "loss: 0.359052  [44864/60000]\n",
      "loss: 0.307341  [51264/60000]\n",
      "loss: 0.422815  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278522 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.214695  [   64/60000]\n",
      "loss: 0.255576  [ 6464/60000]\n",
      "loss: 0.185400  [12864/60000]\n",
      "loss: 0.308152  [19264/60000]\n",
      "loss: 0.213127  [25664/60000]\n",
      "loss: 0.331066  [32064/60000]\n",
      "loss: 0.218237  [38464/60000]\n",
      "loss: 0.358557  [44864/60000]\n",
      "loss: 0.306624  [51264/60000]\n",
      "loss: 0.422289  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278026 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.213554  [   64/60000]\n",
      "loss: 0.255255  [ 6464/60000]\n",
      "loss: 0.184675  [12864/60000]\n",
      "loss: 0.307374  [19264/60000]\n",
      "loss: 0.212848  [25664/60000]\n",
      "loss: 0.330785  [32064/60000]\n",
      "loss: 0.217965  [38464/60000]\n",
      "loss: 0.358060  [44864/60000]\n",
      "loss: 0.305837  [51264/60000]\n",
      "loss: 0.421818  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277535 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.212421  [   64/60000]\n",
      "loss: 0.254953  [ 6464/60000]\n",
      "loss: 0.183950  [12864/60000]\n",
      "loss: 0.306626  [19264/60000]\n",
      "loss: 0.212568  [25664/60000]\n",
      "loss: 0.330524  [32064/60000]\n",
      "loss: 0.217660  [38464/60000]\n",
      "loss: 0.357608  [44864/60000]\n",
      "loss: 0.305109  [51264/60000]\n",
      "loss: 0.421449  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277049 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I want to make a graph of the training error and the test error after X number of epochs!. But wait, I can just record the test error after each epoch! that will save some time...\n",
    "'''\n",
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_acc = np.array([])\n",
    "test_acc = np.array([])\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_acc = np.hstack((train_acc, train(train_dataloader, model, loss_fn, optimizer)))\n",
    "    test_acc = np.hstack((test_acc, test(test_dataloader, model, loss_fn)))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c3512788-0b3f-47ae-8064-d5b3a8ae22a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtrain_acc, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtest_acc))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:645\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m color \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    643\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _default_color(ax\u001b[38;5;241m.\u001b[39mplot, hue, color, kwargs)\n\u001b[1;32m--> 645\u001b[0m p\u001b[38;5;241m.\u001b[39mplot(ax, kwargs)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:423\u001b[0m, in \u001b[0;36m_LinePlotter.plot\u001b[1;34m(self, ax, kws)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# TODO How to handle NA? We don't want NA to propagate through to the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# estimate/CI when some values are present, but we would also like\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# matplotlib to show \"gaps\" in the line when all values are missing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Loop over the semantic subsets and add to the plot\u001b[39;00m\n\u001b[0;32m    422\u001b[0m grouping_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_vars, sub_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_data(grouping_vars, from_comp_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort:\n\u001b[0;32m    426\u001b[0m         sort_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient, other]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\_oldcore.py:1065\u001b[0m, in \u001b[0;36mVectorPlotter.iter_data\u001b[1;34m(self, grouping_vars, reverse, from_comp_data, by_facet, allow_empty, dropna)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m grouping_vars:\n\u001b[0;32m   1063\u001b[0m     grouping_keys\u001b[38;5;241m.\u001b[39mappend(levels\u001b[38;5;241m.\u001b[39mget(var, []))\n\u001b[1;32m-> 1065\u001b[0m iter_keys \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mgrouping_keys)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reverse:\n\u001b[0;32m   1067\u001b[0m     iter_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(iter_keys))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot((1-train_acc, 1-test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
