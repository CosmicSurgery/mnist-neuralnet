{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ccde100-704b-443e-90b3-100855cd94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f91008-c4e3-4355-b11e-bfae14fa3aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1747e3843b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4cc1f9-00ed-43de-aba1-dbd6540496bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cc97a3-ff6b-488b-83cb-4b86ca414324",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a125aec-b1d8-482a-8198-645ad6657abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ace84846-9e28-4fe3-bd71-8bcef49864c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ad7e7c-aa0f-421d-8625-55ed516e6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 18),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(18, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7591b7bb-a598-48fb-82ed-44ae0b6d74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    correct /= size\n",
    "    print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e96a9e85-3749-4b63-b873-3ca22e3cd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c25197a6-4b17-47a8-bcd0-5c86331f55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=18, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3d5f233-e32a-44a5-bac3-16f85ee389d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc893a84-cffa-443f-b260-62215e7cdcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19906666666666667"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "22794a1b-640b-41d7-905b-9ab0b3d9f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.626"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4d0f7e0b-1a36-4bd9-86dd-6605ab6bd2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      4\u001b[0m     test(test_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[99], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     20\u001b[0m     loss, current \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem(), (batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\optim\\optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\autograd\\profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[0;32m    607\u001b[0m     )\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf90d4-a792-4e0e-a068-8254fd746dcc",
   "metadata": {},
   "source": [
    "## Overfitting testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7ad4c2e2-4a86-49d1-8048-1a12440ac1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311446  [   64/60000]\n",
      "loss: 2.318978  [ 6464/60000]\n",
      "loss: 2.292953  [12864/60000]\n",
      "loss: 2.208490  [19264/60000]\n",
      "loss: 2.216908  [25664/60000]\n",
      "loss: 2.212726  [32064/60000]\n",
      "loss: 2.156218  [38464/60000]\n",
      "loss: 2.183332  [44864/60000]\n",
      "loss: 2.104259  [51264/60000]\n",
      "loss: 2.085171  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 19.9%\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 2.087204 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.072464  [   64/60000]\n",
      "loss: 2.057868  [ 6464/60000]\n",
      "loss: 2.130230  [12864/60000]\n",
      "loss: 1.922219  [19264/60000]\n",
      "loss: 1.982769  [25664/60000]\n",
      "loss: 1.972055  [32064/60000]\n",
      "loss: 1.903299  [38464/60000]\n",
      "loss: 1.992909  [44864/60000]\n",
      "loss: 1.859344  [51264/60000]\n",
      "loss: 1.851977  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 47.5%\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.838495 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.818471  [   64/60000]\n",
      "loss: 1.774000  [ 6464/60000]\n",
      "loss: 1.917660  [12864/60000]\n",
      "loss: 1.640146  [19264/60000]\n",
      "loss: 1.720466  [25664/60000]\n",
      "loss: 1.712880  [32064/60000]\n",
      "loss: 1.629762  [38464/60000]\n",
      "loss: 1.774315  [44864/60000]\n",
      "loss: 1.604300  [51264/60000]\n",
      "loss: 1.609467  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 58.6%\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.572969 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.557949  [   64/60000]\n",
      "loss: 1.491758  [ 6464/60000]\n",
      "loss: 1.660951  [12864/60000]\n",
      "loss: 1.377765  [19264/60000]\n",
      "loss: 1.444292  [25664/60000]\n",
      "loss: 1.447927  [32064/60000]\n",
      "loss: 1.356688  [38464/60000]\n",
      "loss: 1.537228  [44864/60000]\n",
      "loss: 1.358394  [51264/60000]\n",
      "loss: 1.372089  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 66.5%\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.318120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.321148  [   64/60000]\n",
      "loss: 1.245712  [ 6464/60000]\n",
      "loss: 1.394385  [12864/60000]\n",
      "loss: 1.151553  [19264/60000]\n",
      "loss: 1.197075  [25664/60000]\n",
      "loss: 1.215134  [32064/60000]\n",
      "loss: 1.118639  [38464/60000]\n",
      "loss: 1.316757  [44864/60000]\n",
      "loss: 1.153122  [51264/60000]\n",
      "loss: 1.170352  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 72.7%\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 1.109394 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.132133  [   64/60000]\n",
      "loss: 1.050473  [ 6464/60000]\n",
      "loss: 1.166535  [12864/60000]\n",
      "loss: 0.973874  [19264/60000]\n",
      "loss: 1.005295  [25664/60000]\n",
      "loss: 1.031641  [32064/60000]\n",
      "loss: 0.938574  [38464/60000]\n",
      "loss: 1.139525  [44864/60000]\n",
      "loss: 0.997379  [51264/60000]\n",
      "loss: 1.014367  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 76.8%\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.952216 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.989064  [   64/60000]\n",
      "loss: 0.902320  [ 6464/60000]\n",
      "loss: 0.988564  [12864/60000]\n",
      "loss: 0.843259  [19264/60000]\n",
      "loss: 0.862754  [25664/60000]\n",
      "loss: 0.892941  [32064/60000]\n",
      "loss: 0.806622  [38464/60000]\n",
      "loss: 1.004429  [44864/60000]\n",
      "loss: 0.880611  [51264/60000]\n",
      "loss: 0.898888  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 79.5%\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.836298 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.881625  [   64/60000]\n",
      "loss: 0.789953  [ 6464/60000]\n",
      "loss: 0.853873  [12864/60000]\n",
      "loss: 0.748737  [19264/60000]\n",
      "loss: 0.757954  [25664/60000]\n",
      "loss: 0.789745  [32064/60000]\n",
      "loss: 0.709818  [38464/60000]\n",
      "loss: 0.902217  [44864/60000]\n",
      "loss: 0.792498  [51264/60000]\n",
      "loss: 0.814121  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 81.4%\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.749816 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.799808  [   64/60000]\n",
      "loss: 0.703597  [ 6464/60000]\n",
      "loss: 0.751970  [12864/60000]\n",
      "loss: 0.679617  [19264/60000]\n",
      "loss: 0.678745  [25664/60000]\n",
      "loss: 0.712081  [32064/60000]\n",
      "loss: 0.636931  [38464/60000]\n",
      "loss: 0.824047  [44864/60000]\n",
      "loss: 0.724426  [51264/60000]\n",
      "loss: 0.750376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 82.7%\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.683793 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.736062  [   64/60000]\n",
      "loss: 0.636310  [ 6464/60000]\n",
      "loss: 0.674065  [12864/60000]\n",
      "loss: 0.627934  [19264/60000]\n",
      "loss: 0.616891  [25664/60000]\n",
      "loss: 0.652553  [32064/60000]\n",
      "loss: 0.580294  [38464/60000]\n",
      "loss: 0.763228  [44864/60000]\n",
      "loss: 0.670565  [51264/60000]\n",
      "loss: 0.701475  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 83.9%\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.632073 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.685212  [   64/60000]\n",
      "loss: 0.582800  [ 6464/60000]\n",
      "loss: 0.613257  [12864/60000]\n",
      "loss: 0.588443  [19264/60000]\n",
      "loss: 0.567234  [25664/60000]\n",
      "loss: 0.606197  [32064/60000]\n",
      "loss: 0.535110  [38464/60000]\n",
      "loss: 0.714983  [44864/60000]\n",
      "loss: 0.627312  [51264/60000]\n",
      "loss: 0.663166  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 84.7%\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.590623 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.643741  [   64/60000]\n",
      "loss: 0.539546  [ 6464/60000]\n",
      "loss: 0.564672  [12864/60000]\n",
      "loss: 0.557566  [19264/60000]\n",
      "loss: 0.526341  [25664/60000]\n",
      "loss: 0.569478  [32064/60000]\n",
      "loss: 0.498467  [38464/60000]\n",
      "loss: 0.676008  [44864/60000]\n",
      "loss: 0.591738  [51264/60000]\n",
      "loss: 0.632310  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 85.4%\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.556767 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.609224  [   64/60000]\n",
      "loss: 0.504055  [ 6464/60000]\n",
      "loss: 0.525274  [12864/60000]\n",
      "loss: 0.532958  [19264/60000]\n",
      "loss: 0.492195  [25664/60000]\n",
      "loss: 0.539940  [32064/60000]\n",
      "loss: 0.468215  [38464/60000]\n",
      "loss: 0.643991  [44864/60000]\n",
      "loss: 0.562275  [51264/60000]\n",
      "loss: 0.607107  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.0%\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.528659 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.580129  [   64/60000]\n",
      "loss: 0.474578  [ 6464/60000]\n",
      "loss: 0.492751  [12864/60000]\n",
      "loss: 0.512964  [19264/60000]\n",
      "loss: 0.463309  [25664/60000]\n",
      "loss: 0.515848  [32064/60000]\n",
      "loss: 0.442773  [38464/60000]\n",
      "loss: 0.617319  [44864/60000]\n",
      "loss: 0.537686  [51264/60000]\n",
      "loss: 0.586342  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 86.5%\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.505008 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.555234  [   64/60000]\n",
      "loss: 0.449872  [ 6464/60000]\n",
      "loss: 0.465381  [12864/60000]\n",
      "loss: 0.496506  [19264/60000]\n",
      "loss: 0.438663  [25664/60000]\n",
      "loss: 0.495941  [32064/60000]\n",
      "loss: 0.421250  [38464/60000]\n",
      "loss: 0.594771  [44864/60000]\n",
      "loss: 0.517012  [51264/60000]\n",
      "loss: 0.569075  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.0%\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.484894 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.533636  [   64/60000]\n",
      "loss: 0.428864  [ 6464/60000]\n",
      "loss: 0.442080  [12864/60000]\n",
      "loss: 0.482801  [19264/60000]\n",
      "loss: 0.417484  [25664/60000]\n",
      "loss: 0.479340  [32064/60000]\n",
      "loss: 0.402846  [38464/60000]\n",
      "loss: 0.575419  [44864/60000]\n",
      "loss: 0.499393  [51264/60000]\n",
      "loss: 0.554512  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.4%\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.467629 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.514752  [   64/60000]\n",
      "loss: 0.410934  [ 6464/60000]\n",
      "loss: 0.422006  [12864/60000]\n",
      "loss: 0.471228  [19264/60000]\n",
      "loss: 0.399241  [25664/60000]\n",
      "loss: 0.465358  [32064/60000]\n",
      "loss: 0.386991  [38464/60000]\n",
      "loss: 0.558591  [44864/60000]\n",
      "loss: 0.484281  [51264/60000]\n",
      "loss: 0.542164  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 87.8%\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.452679 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.498072  [   64/60000]\n",
      "loss: 0.395538  [ 6464/60000]\n",
      "loss: 0.404541  [12864/60000]\n",
      "loss: 0.461305  [19264/60000]\n",
      "loss: 0.383379  [25664/60000]\n",
      "loss: 0.453440  [32064/60000]\n",
      "loss: 0.373245  [38464/60000]\n",
      "loss: 0.543806  [44864/60000]\n",
      "loss: 0.471273  [51264/60000]\n",
      "loss: 0.531639  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.0%\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.439641 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.483209  [   64/60000]\n",
      "loss: 0.382195  [ 6464/60000]\n",
      "loss: 0.389225  [12864/60000]\n",
      "loss: 0.452666  [19264/60000]\n",
      "loss: 0.369548  [25664/60000]\n",
      "loss: 0.443190  [32064/60000]\n",
      "loss: 0.361243  [38464/60000]\n",
      "loss: 0.530696  [44864/60000]\n",
      "loss: 0.459930  [51264/60000]\n",
      "loss: 0.522511  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.3%\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.428185 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.469908  [   64/60000]\n",
      "loss: 0.370535  [ 6464/60000]\n",
      "loss: 0.375691  [12864/60000]\n",
      "loss: 0.445108  [19264/60000]\n",
      "loss: 0.357461  [25664/60000]\n",
      "loss: 0.434370  [32064/60000]\n",
      "loss: 0.350595  [38464/60000]\n",
      "loss: 0.519020  [44864/60000]\n",
      "loss: 0.449971  [51264/60000]\n",
      "loss: 0.514630  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.5%\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.418057 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.457895  [   64/60000]\n",
      "loss: 0.360293  [ 6464/60000]\n",
      "loss: 0.363641  [12864/60000]\n",
      "loss: 0.438435  [19264/60000]\n",
      "loss: 0.346842  [25664/60000]\n",
      "loss: 0.426683  [32064/60000]\n",
      "loss: 0.341132  [38464/60000]\n",
      "loss: 0.508522  [44864/60000]\n",
      "loss: 0.441188  [51264/60000]\n",
      "loss: 0.507772  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.7%\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.409051 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.446948  [   64/60000]\n",
      "loss: 0.351254  [ 6464/60000]\n",
      "loss: 0.352856  [12864/60000]\n",
      "loss: 0.432484  [19264/60000]\n",
      "loss: 0.337381  [25664/60000]\n",
      "loss: 0.419940  [32064/60000]\n",
      "loss: 0.332631  [38464/60000]\n",
      "loss: 0.499001  [44864/60000]\n",
      "loss: 0.433327  [51264/60000]\n",
      "loss: 0.501830  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 88.8%\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.400999 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.436893  [   64/60000]\n",
      "loss: 0.343207  [ 6464/60000]\n",
      "loss: 0.343160  [12864/60000]\n",
      "loss: 0.427129  [19264/60000]\n",
      "loss: 0.328976  [25664/60000]\n",
      "loss: 0.414029  [32064/60000]\n",
      "loss: 0.324936  [38464/60000]\n",
      "loss: 0.490381  [44864/60000]\n",
      "loss: 0.426376  [51264/60000]\n",
      "loss: 0.496547  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.0%\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.393751 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.427625  [   64/60000]\n",
      "loss: 0.336079  [ 6464/60000]\n",
      "loss: 0.334352  [12864/60000]\n",
      "loss: 0.422218  [19264/60000]\n",
      "loss: 0.321472  [25664/60000]\n",
      "loss: 0.408761  [32064/60000]\n",
      "loss: 0.317975  [38464/60000]\n",
      "loss: 0.482516  [44864/60000]\n",
      "loss: 0.420126  [51264/60000]\n",
      "loss: 0.491938  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.1%\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.387207 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.419028  [   64/60000]\n",
      "loss: 0.329684  [ 6464/60000]\n",
      "loss: 0.326338  [12864/60000]\n",
      "loss: 0.417774  [19264/60000]\n",
      "loss: 0.314757  [25664/60000]\n",
      "loss: 0.404038  [32064/60000]\n",
      "loss: 0.311657  [38464/60000]\n",
      "loss: 0.475334  [44864/60000]\n",
      "loss: 0.414519  [51264/60000]\n",
      "loss: 0.487707  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.2%\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.381264 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.411041  [   64/60000]\n",
      "loss: 0.323946  [ 6464/60000]\n",
      "loss: 0.319070  [12864/60000]\n",
      "loss: 0.413676  [19264/60000]\n",
      "loss: 0.308720  [25664/60000]\n",
      "loss: 0.399793  [32064/60000]\n",
      "loss: 0.305919  [38464/60000]\n",
      "loss: 0.468747  [44864/60000]\n",
      "loss: 0.409420  [51264/60000]\n",
      "loss: 0.483958  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.3%\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.375842 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.403606  [   64/60000]\n",
      "loss: 0.318770  [ 6464/60000]\n",
      "loss: 0.312419  [12864/60000]\n",
      "loss: 0.409868  [19264/60000]\n",
      "loss: 0.303256  [25664/60000]\n",
      "loss: 0.395937  [32064/60000]\n",
      "loss: 0.300664  [38464/60000]\n",
      "loss: 0.462695  [44864/60000]\n",
      "loss: 0.404762  [51264/60000]\n",
      "loss: 0.480620  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.4%\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.370872 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.396618  [   64/60000]\n",
      "loss: 0.314116  [ 6464/60000]\n",
      "loss: 0.306286  [12864/60000]\n",
      "loss: 0.406302  [19264/60000]\n",
      "loss: 0.298277  [25664/60000]\n",
      "loss: 0.392432  [32064/60000]\n",
      "loss: 0.295827  [38464/60000]\n",
      "loss: 0.457135  [44864/60000]\n",
      "loss: 0.400491  [51264/60000]\n",
      "loss: 0.477629  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.5%\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.366304 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.390042  [   64/60000]\n",
      "loss: 0.309918  [ 6464/60000]\n",
      "loss: 0.300610  [12864/60000]\n",
      "loss: 0.402982  [19264/60000]\n",
      "loss: 0.293758  [25664/60000]\n",
      "loss: 0.389216  [32064/60000]\n",
      "loss: 0.291381  [38464/60000]\n",
      "loss: 0.452016  [44864/60000]\n",
      "loss: 0.396515  [51264/60000]\n",
      "loss: 0.474933  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.6%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.362095 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.383802  [   64/60000]\n",
      "loss: 0.306093  [ 6464/60000]\n",
      "loss: 0.295327  [12864/60000]\n",
      "loss: 0.399895  [19264/60000]\n",
      "loss: 0.289599  [25664/60000]\n",
      "loss: 0.386275  [32064/60000]\n",
      "loss: 0.287284  [38464/60000]\n",
      "loss: 0.447272  [44864/60000]\n",
      "loss: 0.392873  [51264/60000]\n",
      "loss: 0.472503  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.358196 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.377893  [   64/60000]\n",
      "loss: 0.302624  [ 6464/60000]\n",
      "loss: 0.290400  [12864/60000]\n",
      "loss: 0.396970  [19264/60000]\n",
      "loss: 0.285810  [25664/60000]\n",
      "loss: 0.383609  [32064/60000]\n",
      "loss: 0.283493  [38464/60000]\n",
      "loss: 0.442835  [44864/60000]\n",
      "loss: 0.389476  [51264/60000]\n",
      "loss: 0.470337  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.7%\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.354576 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.372278  [   64/60000]\n",
      "loss: 0.299469  [ 6464/60000]\n",
      "loss: 0.285788  [12864/60000]\n",
      "loss: 0.394212  [19264/60000]\n",
      "loss: 0.282320  [25664/60000]\n",
      "loss: 0.381166  [32064/60000]\n",
      "loss: 0.279971  [38464/60000]\n",
      "loss: 0.438735  [44864/60000]\n",
      "loss: 0.386328  [51264/60000]\n",
      "loss: 0.468376  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.351200 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.366960  [   64/60000]\n",
      "loss: 0.296603  [ 6464/60000]\n",
      "loss: 0.281467  [12864/60000]\n",
      "loss: 0.391569  [19264/60000]\n",
      "loss: 0.279109  [25664/60000]\n",
      "loss: 0.378893  [32064/60000]\n",
      "loss: 0.276695  [38464/60000]\n",
      "loss: 0.434914  [44864/60000]\n",
      "loss: 0.383394  [51264/60000]\n",
      "loss: 0.466559  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.8%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.348050 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.361929  [   64/60000]\n",
      "loss: 0.293956  [ 6464/60000]\n",
      "loss: 0.277419  [12864/60000]\n",
      "loss: 0.389016  [19264/60000]\n",
      "loss: 0.276153  [25664/60000]\n",
      "loss: 0.376759  [32064/60000]\n",
      "loss: 0.273651  [38464/60000]\n",
      "loss: 0.431349  [44864/60000]\n",
      "loss: 0.380643  [51264/60000]\n",
      "loss: 0.464907  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 89.9%\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.345101 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.357106  [   64/60000]\n",
      "loss: 0.291519  [ 6464/60000]\n",
      "loss: 0.273610  [12864/60000]\n",
      "loss: 0.386581  [19264/60000]\n",
      "loss: 0.273380  [25664/60000]\n",
      "loss: 0.374776  [32064/60000]\n",
      "loss: 0.270817  [38464/60000]\n",
      "loss: 0.428014  [44864/60000]\n",
      "loss: 0.378020  [51264/60000]\n",
      "loss: 0.463414  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.342331 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.352486  [   64/60000]\n",
      "loss: 0.289283  [ 6464/60000]\n",
      "loss: 0.270017  [12864/60000]\n",
      "loss: 0.384253  [19264/60000]\n",
      "loss: 0.270773  [25664/60000]\n",
      "loss: 0.372907  [32064/60000]\n",
      "loss: 0.268161  [38464/60000]\n",
      "loss: 0.424887  [44864/60000]\n",
      "loss: 0.375553  [51264/60000]\n",
      "loss: 0.462046  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.0%\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.339719 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.348055  [   64/60000]\n",
      "loss: 0.287234  [ 6464/60000]\n",
      "loss: 0.266590  [12864/60000]\n",
      "loss: 0.381983  [19264/60000]\n",
      "loss: 0.268346  [25664/60000]\n",
      "loss: 0.371140  [32064/60000]\n",
      "loss: 0.265664  [38464/60000]\n",
      "loss: 0.421982  [44864/60000]\n",
      "loss: 0.373270  [51264/60000]\n",
      "loss: 0.460792  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.337255 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.343811  [   64/60000]\n",
      "loss: 0.285360  [ 6464/60000]\n",
      "loss: 0.263343  [12864/60000]\n",
      "loss: 0.379786  [19264/60000]\n",
      "loss: 0.266061  [25664/60000]\n",
      "loss: 0.369476  [32064/60000]\n",
      "loss: 0.263358  [38464/60000]\n",
      "loss: 0.419190  [44864/60000]\n",
      "loss: 0.371083  [51264/60000]\n",
      "loss: 0.459650  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.1%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.334935 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.339731  [   64/60000]\n",
      "loss: 0.283560  [ 6464/60000]\n",
      "loss: 0.260268  [12864/60000]\n",
      "loss: 0.377673  [19264/60000]\n",
      "loss: 0.263876  [25664/60000]\n",
      "loss: 0.368027  [32064/60000]\n",
      "loss: 0.261131  [38464/60000]\n",
      "loss: 0.416569  [44864/60000]\n",
      "loss: 0.369035  [51264/60000]\n",
      "loss: 0.458614  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.332736 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.335784  [   64/60000]\n",
      "loss: 0.281895  [ 6464/60000]\n",
      "loss: 0.257351  [12864/60000]\n",
      "loss: 0.375694  [19264/60000]\n",
      "loss: 0.261864  [25664/60000]\n",
      "loss: 0.366651  [32064/60000]\n",
      "loss: 0.259001  [38464/60000]\n",
      "loss: 0.414089  [44864/60000]\n",
      "loss: 0.367049  [51264/60000]\n",
      "loss: 0.457645  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.2%\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.330648 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.331939  [   64/60000]\n",
      "loss: 0.280379  [ 6464/60000]\n",
      "loss: 0.254579  [12864/60000]\n",
      "loss: 0.373776  [19264/60000]\n",
      "loss: 0.259961  [25664/60000]\n",
      "loss: 0.365373  [32064/60000]\n",
      "loss: 0.257010  [38464/60000]\n",
      "loss: 0.411737  [44864/60000]\n",
      "loss: 0.365146  [51264/60000]\n",
      "loss: 0.456740  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.328663 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.328240  [   64/60000]\n",
      "loss: 0.278977  [ 6464/60000]\n",
      "loss: 0.251932  [12864/60000]\n",
      "loss: 0.371922  [19264/60000]\n",
      "loss: 0.258162  [25664/60000]\n",
      "loss: 0.364164  [32064/60000]\n",
      "loss: 0.255108  [38464/60000]\n",
      "loss: 0.409504  [44864/60000]\n",
      "loss: 0.363297  [51264/60000]\n",
      "loss: 0.455878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.3%\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.326776 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.324579  [   64/60000]\n",
      "loss: 0.277670  [ 6464/60000]\n",
      "loss: 0.249412  [12864/60000]\n",
      "loss: 0.370111  [19264/60000]\n",
      "loss: 0.256447  [25664/60000]\n",
      "loss: 0.363039  [32064/60000]\n",
      "loss: 0.253300  [38464/60000]\n",
      "loss: 0.407395  [44864/60000]\n",
      "loss: 0.361510  [51264/60000]\n",
      "loss: 0.454985  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.324981 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.321034  [   64/60000]\n",
      "loss: 0.276453  [ 6464/60000]\n",
      "loss: 0.247065  [12864/60000]\n",
      "loss: 0.368344  [19264/60000]\n",
      "loss: 0.254840  [25664/60000]\n",
      "loss: 0.361960  [32064/60000]\n",
      "loss: 0.251577  [38464/60000]\n",
      "loss: 0.405412  [44864/60000]\n",
      "loss: 0.359763  [51264/60000]\n",
      "loss: 0.454153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.4%\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.323264 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.317589  [   64/60000]\n",
      "loss: 0.275312  [ 6464/60000]\n",
      "loss: 0.244839  [12864/60000]\n",
      "loss: 0.366690  [19264/60000]\n",
      "loss: 0.253297  [25664/60000]\n",
      "loss: 0.360966  [32064/60000]\n",
      "loss: 0.249923  [38464/60000]\n",
      "loss: 0.403501  [44864/60000]\n",
      "loss: 0.358088  [51264/60000]\n",
      "loss: 0.453323  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.321621 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.314243  [   64/60000]\n",
      "loss: 0.274265  [ 6464/60000]\n",
      "loss: 0.242711  [12864/60000]\n",
      "loss: 0.365063  [19264/60000]\n",
      "loss: 0.251779  [25664/60000]\n",
      "loss: 0.359882  [32064/60000]\n",
      "loss: 0.248368  [38464/60000]\n",
      "loss: 0.401665  [44864/60000]\n",
      "loss: 0.356466  [51264/60000]\n",
      "loss: 0.452569  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.5%\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.320046 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.310971  [   64/60000]\n",
      "loss: 0.273295  [ 6464/60000]\n",
      "loss: 0.240691  [12864/60000]\n",
      "loss: 0.363443  [19264/60000]\n",
      "loss: 0.250335  [25664/60000]\n",
      "loss: 0.358840  [32064/60000]\n",
      "loss: 0.246891  [38464/60000]\n",
      "loss: 0.399927  [44864/60000]\n",
      "loss: 0.354897  [51264/60000]\n",
      "loss: 0.451816  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.318538 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.307802  [   64/60000]\n",
      "loss: 0.272382  [ 6464/60000]\n",
      "loss: 0.238761  [12864/60000]\n",
      "loss: 0.361861  [19264/60000]\n",
      "loss: 0.248977  [25664/60000]\n",
      "loss: 0.357790  [32064/60000]\n",
      "loss: 0.245520  [38464/60000]\n",
      "loss: 0.398207  [44864/60000]\n",
      "loss: 0.353408  [51264/60000]\n",
      "loss: 0.451085  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.6%\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.317088 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.304733  [   64/60000]\n",
      "loss: 0.271497  [ 6464/60000]\n",
      "loss: 0.236913  [12864/60000]\n",
      "loss: 0.360322  [19264/60000]\n",
      "loss: 0.247720  [25664/60000]\n",
      "loss: 0.356810  [32064/60000]\n",
      "loss: 0.244182  [38464/60000]\n",
      "loss: 0.396583  [44864/60000]\n",
      "loss: 0.351987  [51264/60000]\n",
      "loss: 0.450442  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.315697 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.301729  [   64/60000]\n",
      "loss: 0.270657  [ 6464/60000]\n",
      "loss: 0.235146  [12864/60000]\n",
      "loss: 0.358837  [19264/60000]\n",
      "loss: 0.246505  [25664/60000]\n",
      "loss: 0.355904  [32064/60000]\n",
      "loss: 0.242935  [38464/60000]\n",
      "loss: 0.395000  [44864/60000]\n",
      "loss: 0.350593  [51264/60000]\n",
      "loss: 0.449746  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.314361 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.298826  [   64/60000]\n",
      "loss: 0.269848  [ 6464/60000]\n",
      "loss: 0.233430  [12864/60000]\n",
      "loss: 0.357380  [19264/60000]\n",
      "loss: 0.245385  [25664/60000]\n",
      "loss: 0.355014  [32064/60000]\n",
      "loss: 0.241703  [38464/60000]\n",
      "loss: 0.393487  [44864/60000]\n",
      "loss: 0.349199  [51264/60000]\n",
      "loss: 0.449051  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.7%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.313068 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.295966  [   64/60000]\n",
      "loss: 0.269110  [ 6464/60000]\n",
      "loss: 0.231742  [12864/60000]\n",
      "loss: 0.355931  [19264/60000]\n",
      "loss: 0.244334  [25664/60000]\n",
      "loss: 0.354139  [32064/60000]\n",
      "loss: 0.240559  [38464/60000]\n",
      "loss: 0.392049  [44864/60000]\n",
      "loss: 0.347825  [51264/60000]\n",
      "loss: 0.448461  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.311820 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.293166  [   64/60000]\n",
      "loss: 0.268428  [ 6464/60000]\n",
      "loss: 0.230137  [12864/60000]\n",
      "loss: 0.354495  [19264/60000]\n",
      "loss: 0.243302  [25664/60000]\n",
      "loss: 0.353278  [32064/60000]\n",
      "loss: 0.239469  [38464/60000]\n",
      "loss: 0.390675  [44864/60000]\n",
      "loss: 0.346472  [51264/60000]\n",
      "loss: 0.447874  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.310615 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.290458  [   64/60000]\n",
      "loss: 0.267775  [ 6464/60000]\n",
      "loss: 0.228563  [12864/60000]\n",
      "loss: 0.353103  [19264/60000]\n",
      "loss: 0.242335  [25664/60000]\n",
      "loss: 0.352485  [32064/60000]\n",
      "loss: 0.238412  [38464/60000]\n",
      "loss: 0.389451  [44864/60000]\n",
      "loss: 0.345149  [51264/60000]\n",
      "loss: 0.447307  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.8%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.309445 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.287826  [   64/60000]\n",
      "loss: 0.267161  [ 6464/60000]\n",
      "loss: 0.227018  [12864/60000]\n",
      "loss: 0.351709  [19264/60000]\n",
      "loss: 0.241392  [25664/60000]\n",
      "loss: 0.351708  [32064/60000]\n",
      "loss: 0.237416  [38464/60000]\n",
      "loss: 0.388272  [44864/60000]\n",
      "loss: 0.343855  [51264/60000]\n",
      "loss: 0.446767  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.308315 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.285238  [   64/60000]\n",
      "loss: 0.266577  [ 6464/60000]\n",
      "loss: 0.225517  [12864/60000]\n",
      "loss: 0.350297  [19264/60000]\n",
      "loss: 0.240477  [25664/60000]\n",
      "loss: 0.350938  [32064/60000]\n",
      "loss: 0.236480  [38464/60000]\n",
      "loss: 0.387129  [44864/60000]\n",
      "loss: 0.342626  [51264/60000]\n",
      "loss: 0.446221  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.307221 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.282732  [   64/60000]\n",
      "loss: 0.266016  [ 6464/60000]\n",
      "loss: 0.224058  [12864/60000]\n",
      "loss: 0.348947  [19264/60000]\n",
      "loss: 0.239578  [25664/60000]\n",
      "loss: 0.350212  [32064/60000]\n",
      "loss: 0.235580  [38464/60000]\n",
      "loss: 0.386025  [44864/60000]\n",
      "loss: 0.341384  [51264/60000]\n",
      "loss: 0.445606  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.4%, Avg loss: 0.306167 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.280269  [   64/60000]\n",
      "loss: 0.265480  [ 6464/60000]\n",
      "loss: 0.222645  [12864/60000]\n",
      "loss: 0.347616  [19264/60000]\n",
      "loss: 0.238733  [25664/60000]\n",
      "loss: 0.349494  [32064/60000]\n",
      "loss: 0.234751  [38464/60000]\n",
      "loss: 0.384915  [44864/60000]\n",
      "loss: 0.340146  [51264/60000]\n",
      "loss: 0.445001  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 90.9%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.305146 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.277882  [   64/60000]\n",
      "loss: 0.264972  [ 6464/60000]\n",
      "loss: 0.221269  [12864/60000]\n",
      "loss: 0.346307  [19264/60000]\n",
      "loss: 0.237827  [25664/60000]\n",
      "loss: 0.348813  [32064/60000]\n",
      "loss: 0.233939  [38464/60000]\n",
      "loss: 0.383880  [44864/60000]\n",
      "loss: 0.338978  [51264/60000]\n",
      "loss: 0.444390  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.304157 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.275552  [   64/60000]\n",
      "loss: 0.264638  [ 6464/60000]\n",
      "loss: 0.219939  [12864/60000]\n",
      "loss: 0.345044  [19264/60000]\n",
      "loss: 0.236835  [25664/60000]\n",
      "loss: 0.348143  [32064/60000]\n",
      "loss: 0.233142  [38464/60000]\n",
      "loss: 0.382895  [44864/60000]\n",
      "loss: 0.337906  [51264/60000]\n",
      "loss: 0.443790  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.303189 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.273331  [   64/60000]\n",
      "loss: 0.264376  [ 6464/60000]\n",
      "loss: 0.218615  [12864/60000]\n",
      "loss: 0.343765  [19264/60000]\n",
      "loss: 0.235863  [25664/60000]\n",
      "loss: 0.347487  [32064/60000]\n",
      "loss: 0.232413  [38464/60000]\n",
      "loss: 0.381935  [44864/60000]\n",
      "loss: 0.336823  [51264/60000]\n",
      "loss: 0.443205  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.0%\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.302251 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.271162  [   64/60000]\n",
      "loss: 0.264144  [ 6464/60000]\n",
      "loss: 0.217355  [12864/60000]\n",
      "loss: 0.342524  [19264/60000]\n",
      "loss: 0.234907  [25664/60000]\n",
      "loss: 0.346829  [32064/60000]\n",
      "loss: 0.231697  [38464/60000]\n",
      "loss: 0.381001  [44864/60000]\n",
      "loss: 0.335749  [51264/60000]\n",
      "loss: 0.442647  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.301339 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.269055  [   64/60000]\n",
      "loss: 0.263907  [ 6464/60000]\n",
      "loss: 0.216140  [12864/60000]\n",
      "loss: 0.341370  [19264/60000]\n",
      "loss: 0.233978  [25664/60000]\n",
      "loss: 0.346199  [32064/60000]\n",
      "loss: 0.231012  [38464/60000]\n",
      "loss: 0.380151  [44864/60000]\n",
      "loss: 0.334724  [51264/60000]\n",
      "loss: 0.442129  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.300454 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.266978  [   64/60000]\n",
      "loss: 0.263679  [ 6464/60000]\n",
      "loss: 0.214988  [12864/60000]\n",
      "loss: 0.340233  [19264/60000]\n",
      "loss: 0.233095  [25664/60000]\n",
      "loss: 0.345587  [32064/60000]\n",
      "loss: 0.230390  [38464/60000]\n",
      "loss: 0.379328  [44864/60000]\n",
      "loss: 0.333704  [51264/60000]\n",
      "loss: 0.441612  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.1%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.299589 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.264926  [   64/60000]\n",
      "loss: 0.263483  [ 6464/60000]\n",
      "loss: 0.213876  [12864/60000]\n",
      "loss: 0.339137  [19264/60000]\n",
      "loss: 0.232241  [25664/60000]\n",
      "loss: 0.345002  [32064/60000]\n",
      "loss: 0.229791  [38464/60000]\n",
      "loss: 0.378541  [44864/60000]\n",
      "loss: 0.332726  [51264/60000]\n",
      "loss: 0.441093  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.298742 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.262905  [   64/60000]\n",
      "loss: 0.263294  [ 6464/60000]\n",
      "loss: 0.212792  [12864/60000]\n",
      "loss: 0.338017  [19264/60000]\n",
      "loss: 0.231363  [25664/60000]\n",
      "loss: 0.344402  [32064/60000]\n",
      "loss: 0.229191  [38464/60000]\n",
      "loss: 0.377770  [44864/60000]\n",
      "loss: 0.331777  [51264/60000]\n",
      "loss: 0.440520  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.297916 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.260919  [   64/60000]\n",
      "loss: 0.263119  [ 6464/60000]\n",
      "loss: 0.211727  [12864/60000]\n",
      "loss: 0.336916  [19264/60000]\n",
      "loss: 0.230520  [25664/60000]\n",
      "loss: 0.343846  [32064/60000]\n",
      "loss: 0.228613  [38464/60000]\n",
      "loss: 0.377023  [44864/60000]\n",
      "loss: 0.330816  [51264/60000]\n",
      "loss: 0.439982  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.297113 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.258944  [   64/60000]\n",
      "loss: 0.262949  [ 6464/60000]\n",
      "loss: 0.210702  [12864/60000]\n",
      "loss: 0.335893  [19264/60000]\n",
      "loss: 0.229731  [25664/60000]\n",
      "loss: 0.343300  [32064/60000]\n",
      "loss: 0.228051  [38464/60000]\n",
      "loss: 0.376340  [44864/60000]\n",
      "loss: 0.329899  [51264/60000]\n",
      "loss: 0.439411  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.2%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.296323 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.257028  [   64/60000]\n",
      "loss: 0.262809  [ 6464/60000]\n",
      "loss: 0.209655  [12864/60000]\n",
      "loss: 0.334821  [19264/60000]\n",
      "loss: 0.228915  [25664/60000]\n",
      "loss: 0.342755  [32064/60000]\n",
      "loss: 0.227524  [38464/60000]\n",
      "loss: 0.375705  [44864/60000]\n",
      "loss: 0.328947  [51264/60000]\n",
      "loss: 0.438878  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.295557 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.255128  [   64/60000]\n",
      "loss: 0.262701  [ 6464/60000]\n",
      "loss: 0.208637  [12864/60000]\n",
      "loss: 0.333826  [19264/60000]\n",
      "loss: 0.228120  [25664/60000]\n",
      "loss: 0.342272  [32064/60000]\n",
      "loss: 0.226980  [38464/60000]\n",
      "loss: 0.375117  [44864/60000]\n",
      "loss: 0.328021  [51264/60000]\n",
      "loss: 0.438305  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294802 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.253281  [   64/60000]\n",
      "loss: 0.262628  [ 6464/60000]\n",
      "loss: 0.207632  [12864/60000]\n",
      "loss: 0.332828  [19264/60000]\n",
      "loss: 0.227327  [25664/60000]\n",
      "loss: 0.341767  [32064/60000]\n",
      "loss: 0.226461  [38464/60000]\n",
      "loss: 0.374522  [44864/60000]\n",
      "loss: 0.327119  [51264/60000]\n",
      "loss: 0.437756  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.294062 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.251477  [   64/60000]\n",
      "loss: 0.262552  [ 6464/60000]\n",
      "loss: 0.206644  [12864/60000]\n",
      "loss: 0.331845  [19264/60000]\n",
      "loss: 0.226513  [25664/60000]\n",
      "loss: 0.341234  [32064/60000]\n",
      "loss: 0.225962  [38464/60000]\n",
      "loss: 0.373926  [44864/60000]\n",
      "loss: 0.326273  [51264/60000]\n",
      "loss: 0.437202  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.3%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.293337 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.249717  [   64/60000]\n",
      "loss: 0.262479  [ 6464/60000]\n",
      "loss: 0.205671  [12864/60000]\n",
      "loss: 0.330861  [19264/60000]\n",
      "loss: 0.225713  [25664/60000]\n",
      "loss: 0.340694  [32064/60000]\n",
      "loss: 0.225486  [38464/60000]\n",
      "loss: 0.373268  [44864/60000]\n",
      "loss: 0.325382  [51264/60000]\n",
      "loss: 0.436692  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.292631 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.247951  [   64/60000]\n",
      "loss: 0.262412  [ 6464/60000]\n",
      "loss: 0.204745  [12864/60000]\n",
      "loss: 0.329921  [19264/60000]\n",
      "loss: 0.224945  [25664/60000]\n",
      "loss: 0.340214  [32064/60000]\n",
      "loss: 0.225020  [38464/60000]\n",
      "loss: 0.372637  [44864/60000]\n",
      "loss: 0.324445  [51264/60000]\n",
      "loss: 0.436153  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291939 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.246218  [   64/60000]\n",
      "loss: 0.262327  [ 6464/60000]\n",
      "loss: 0.203799  [12864/60000]\n",
      "loss: 0.329014  [19264/60000]\n",
      "loss: 0.224217  [25664/60000]\n",
      "loss: 0.339700  [32064/60000]\n",
      "loss: 0.224583  [38464/60000]\n",
      "loss: 0.372048  [44864/60000]\n",
      "loss: 0.323575  [51264/60000]\n",
      "loss: 0.435601  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.291256 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.244550  [   64/60000]\n",
      "loss: 0.262262  [ 6464/60000]\n",
      "loss: 0.202847  [12864/60000]\n",
      "loss: 0.328070  [19264/60000]\n",
      "loss: 0.223528  [25664/60000]\n",
      "loss: 0.339159  [32064/60000]\n",
      "loss: 0.224138  [38464/60000]\n",
      "loss: 0.371467  [44864/60000]\n",
      "loss: 0.322684  [51264/60000]\n",
      "loss: 0.435070  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.4%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.290580 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.242908  [   64/60000]\n",
      "loss: 0.262179  [ 6464/60000]\n",
      "loss: 0.201914  [12864/60000]\n",
      "loss: 0.327168  [19264/60000]\n",
      "loss: 0.222848  [25664/60000]\n",
      "loss: 0.338631  [32064/60000]\n",
      "loss: 0.223747  [38464/60000]\n",
      "loss: 0.370874  [44864/60000]\n",
      "loss: 0.321823  [51264/60000]\n",
      "loss: 0.434495  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289920 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.241296  [   64/60000]\n",
      "loss: 0.262064  [ 6464/60000]\n",
      "loss: 0.200993  [12864/60000]\n",
      "loss: 0.326226  [19264/60000]\n",
      "loss: 0.222183  [25664/60000]\n",
      "loss: 0.338156  [32064/60000]\n",
      "loss: 0.223360  [38464/60000]\n",
      "loss: 0.370214  [44864/60000]\n",
      "loss: 0.320909  [51264/60000]\n",
      "loss: 0.433892  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.289270 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.239696  [   64/60000]\n",
      "loss: 0.261996  [ 6464/60000]\n",
      "loss: 0.200119  [12864/60000]\n",
      "loss: 0.325246  [19264/60000]\n",
      "loss: 0.221546  [25664/60000]\n",
      "loss: 0.337743  [32064/60000]\n",
      "loss: 0.223004  [38464/60000]\n",
      "loss: 0.369534  [44864/60000]\n",
      "loss: 0.320051  [51264/60000]\n",
      "loss: 0.433291  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.288631 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.238151  [   64/60000]\n",
      "loss: 0.261889  [ 6464/60000]\n",
      "loss: 0.199259  [12864/60000]\n",
      "loss: 0.324294  [19264/60000]\n",
      "loss: 0.220953  [25664/60000]\n",
      "loss: 0.337353  [32064/60000]\n",
      "loss: 0.222659  [38464/60000]\n",
      "loss: 0.368885  [44864/60000]\n",
      "loss: 0.319209  [51264/60000]\n",
      "loss: 0.432661  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.287999 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.236660  [   64/60000]\n",
      "loss: 0.261770  [ 6464/60000]\n",
      "loss: 0.198410  [12864/60000]\n",
      "loss: 0.323337  [19264/60000]\n",
      "loss: 0.220353  [25664/60000]\n",
      "loss: 0.336925  [32064/60000]\n",
      "loss: 0.222322  [38464/60000]\n",
      "loss: 0.368301  [44864/60000]\n",
      "loss: 0.318417  [51264/60000]\n",
      "loss: 0.432048  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.287371 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.235225  [   64/60000]\n",
      "loss: 0.261391  [ 6464/60000]\n",
      "loss: 0.197565  [12864/60000]\n",
      "loss: 0.322295  [19264/60000]\n",
      "loss: 0.219759  [25664/60000]\n",
      "loss: 0.336425  [32064/60000]\n",
      "loss: 0.222034  [38464/60000]\n",
      "loss: 0.367705  [44864/60000]\n",
      "loss: 0.317636  [51264/60000]\n",
      "loss: 0.431499  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.5%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286755 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.233788  [   64/60000]\n",
      "loss: 0.260947  [ 6464/60000]\n",
      "loss: 0.196761  [12864/60000]\n",
      "loss: 0.321329  [19264/60000]\n",
      "loss: 0.219211  [25664/60000]\n",
      "loss: 0.336022  [32064/60000]\n",
      "loss: 0.221756  [38464/60000]\n",
      "loss: 0.367067  [44864/60000]\n",
      "loss: 0.316805  [51264/60000]\n",
      "loss: 0.430895  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.286148 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.232377  [   64/60000]\n",
      "loss: 0.260517  [ 6464/60000]\n",
      "loss: 0.195978  [12864/60000]\n",
      "loss: 0.320401  [19264/60000]\n",
      "loss: 0.218679  [25664/60000]\n",
      "loss: 0.335649  [32064/60000]\n",
      "loss: 0.221502  [38464/60000]\n",
      "loss: 0.366469  [44864/60000]\n",
      "loss: 0.316112  [51264/60000]\n",
      "loss: 0.430327  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.285545 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.230961  [   64/60000]\n",
      "loss: 0.260129  [ 6464/60000]\n",
      "loss: 0.195204  [12864/60000]\n",
      "loss: 0.319471  [19264/60000]\n",
      "loss: 0.218125  [25664/60000]\n",
      "loss: 0.335283  [32064/60000]\n",
      "loss: 0.221215  [38464/60000]\n",
      "loss: 0.365885  [44864/60000]\n",
      "loss: 0.315432  [51264/60000]\n",
      "loss: 0.429731  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284949 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.229576  [   64/60000]\n",
      "loss: 0.259770  [ 6464/60000]\n",
      "loss: 0.194432  [12864/60000]\n",
      "loss: 0.318548  [19264/60000]\n",
      "loss: 0.217614  [25664/60000]\n",
      "loss: 0.334922  [32064/60000]\n",
      "loss: 0.220915  [38464/60000]\n",
      "loss: 0.365316  [44864/60000]\n",
      "loss: 0.314743  [51264/60000]\n",
      "loss: 0.429119  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.6%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.284369 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.228234  [   64/60000]\n",
      "loss: 0.259448  [ 6464/60000]\n",
      "loss: 0.193597  [12864/60000]\n",
      "loss: 0.317639  [19264/60000]\n",
      "loss: 0.217113  [25664/60000]\n",
      "loss: 0.334547  [32064/60000]\n",
      "loss: 0.220682  [38464/60000]\n",
      "loss: 0.364663  [44864/60000]\n",
      "loss: 0.314041  [51264/60000]\n",
      "loss: 0.428491  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283801 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.226927  [   64/60000]\n",
      "loss: 0.259090  [ 6464/60000]\n",
      "loss: 0.192772  [12864/60000]\n",
      "loss: 0.316695  [19264/60000]\n",
      "loss: 0.216646  [25664/60000]\n",
      "loss: 0.334203  [32064/60000]\n",
      "loss: 0.220462  [38464/60000]\n",
      "loss: 0.364028  [44864/60000]\n",
      "loss: 0.313383  [51264/60000]\n",
      "loss: 0.427864  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.283241 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.225642  [   64/60000]\n",
      "loss: 0.258741  [ 6464/60000]\n",
      "loss: 0.191990  [12864/60000]\n",
      "loss: 0.315770  [19264/60000]\n",
      "loss: 0.216188  [25664/60000]\n",
      "loss: 0.333843  [32064/60000]\n",
      "loss: 0.220266  [38464/60000]\n",
      "loss: 0.363403  [44864/60000]\n",
      "loss: 0.312758  [51264/60000]\n",
      "loss: 0.427306  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282689 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.224346  [   64/60000]\n",
      "loss: 0.258375  [ 6464/60000]\n",
      "loss: 0.191216  [12864/60000]\n",
      "loss: 0.314831  [19264/60000]\n",
      "loss: 0.215689  [25664/60000]\n",
      "loss: 0.333478  [32064/60000]\n",
      "loss: 0.220074  [38464/60000]\n",
      "loss: 0.362793  [44864/60000]\n",
      "loss: 0.312145  [51264/60000]\n",
      "loss: 0.426694  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.282145 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.223062  [   64/60000]\n",
      "loss: 0.258040  [ 6464/60000]\n",
      "loss: 0.190455  [12864/60000]\n",
      "loss: 0.313886  [19264/60000]\n",
      "loss: 0.215225  [25664/60000]\n",
      "loss: 0.333125  [32064/60000]\n",
      "loss: 0.219909  [38464/60000]\n",
      "loss: 0.362203  [44864/60000]\n",
      "loss: 0.311505  [51264/60000]\n",
      "loss: 0.426125  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.7%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281614 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.221798  [   64/60000]\n",
      "loss: 0.257677  [ 6464/60000]\n",
      "loss: 0.189701  [12864/60000]\n",
      "loss: 0.313027  [19264/60000]\n",
      "loss: 0.214802  [25664/60000]\n",
      "loss: 0.332849  [32064/60000]\n",
      "loss: 0.219737  [38464/60000]\n",
      "loss: 0.361619  [44864/60000]\n",
      "loss: 0.310867  [51264/60000]\n",
      "loss: 0.425563  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.281085 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.220542  [   64/60000]\n",
      "loss: 0.257339  [ 6464/60000]\n",
      "loss: 0.188971  [12864/60000]\n",
      "loss: 0.312171  [19264/60000]\n",
      "loss: 0.214481  [25664/60000]\n",
      "loss: 0.332522  [32064/60000]\n",
      "loss: 0.219591  [38464/60000]\n",
      "loss: 0.361058  [44864/60000]\n",
      "loss: 0.310256  [51264/60000]\n",
      "loss: 0.425009  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280559 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.219335  [   64/60000]\n",
      "loss: 0.256987  [ 6464/60000]\n",
      "loss: 0.188248  [12864/60000]\n",
      "loss: 0.311340  [19264/60000]\n",
      "loss: 0.214234  [25664/60000]\n",
      "loss: 0.332202  [32064/60000]\n",
      "loss: 0.219325  [38464/60000]\n",
      "loss: 0.360568  [44864/60000]\n",
      "loss: 0.309516  [51264/60000]\n",
      "loss: 0.424427  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.280039 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.218158  [   64/60000]\n",
      "loss: 0.256574  [ 6464/60000]\n",
      "loss: 0.187533  [12864/60000]\n",
      "loss: 0.310527  [19264/60000]\n",
      "loss: 0.214021  [25664/60000]\n",
      "loss: 0.331889  [32064/60000]\n",
      "loss: 0.219024  [38464/60000]\n",
      "loss: 0.360076  [44864/60000]\n",
      "loss: 0.308773  [51264/60000]\n",
      "loss: 0.423909  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279524 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.216977  [   64/60000]\n",
      "loss: 0.256241  [ 6464/60000]\n",
      "loss: 0.186820  [12864/60000]\n",
      "loss: 0.309717  [19264/60000]\n",
      "loss: 0.213729  [25664/60000]\n",
      "loss: 0.331617  [32064/60000]\n",
      "loss: 0.218760  [38464/60000]\n",
      "loss: 0.359597  [44864/60000]\n",
      "loss: 0.308057  [51264/60000]\n",
      "loss: 0.423351  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.279021 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.215834  [   64/60000]\n",
      "loss: 0.255895  [ 6464/60000]\n",
      "loss: 0.186115  [12864/60000]\n",
      "loss: 0.308951  [19264/60000]\n",
      "loss: 0.213431  [25664/60000]\n",
      "loss: 0.331347  [32064/60000]\n",
      "loss: 0.218492  [38464/60000]\n",
      "loss: 0.359052  [44864/60000]\n",
      "loss: 0.307341  [51264/60000]\n",
      "loss: 0.422815  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.8%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278522 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.214695  [   64/60000]\n",
      "loss: 0.255576  [ 6464/60000]\n",
      "loss: 0.185400  [12864/60000]\n",
      "loss: 0.308152  [19264/60000]\n",
      "loss: 0.213127  [25664/60000]\n",
      "loss: 0.331066  [32064/60000]\n",
      "loss: 0.218237  [38464/60000]\n",
      "loss: 0.358557  [44864/60000]\n",
      "loss: 0.306624  [51264/60000]\n",
      "loss: 0.422289  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.278026 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.213554  [   64/60000]\n",
      "loss: 0.255255  [ 6464/60000]\n",
      "loss: 0.184675  [12864/60000]\n",
      "loss: 0.307374  [19264/60000]\n",
      "loss: 0.212848  [25664/60000]\n",
      "loss: 0.330785  [32064/60000]\n",
      "loss: 0.217965  [38464/60000]\n",
      "loss: 0.358060  [44864/60000]\n",
      "loss: 0.305837  [51264/60000]\n",
      "loss: 0.421818  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277535 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.212421  [   64/60000]\n",
      "loss: 0.254953  [ 6464/60000]\n",
      "loss: 0.183950  [12864/60000]\n",
      "loss: 0.306626  [19264/60000]\n",
      "loss: 0.212568  [25664/60000]\n",
      "loss: 0.330524  [32064/60000]\n",
      "loss: 0.217660  [38464/60000]\n",
      "loss: 0.357608  [44864/60000]\n",
      "loss: 0.305109  [51264/60000]\n",
      "loss: 0.421449  [57664/60000]\n",
      "Training Error: \n",
      " Accuracy: 91.9%\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.277049 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I want to make a graph of the training error and the test error after X number of epochs!. But wait, I can just record the test error after each epoch! that will save some time...\n",
    "'''\n",
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_acc = np.array([])\n",
    "test_acc = np.array([])\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_acc = np.hstack((train_acc, train(train_dataloader, model, loss_fn, optimizer)))\n",
    "    test_acc = np.hstack((test_acc, test(test_dataloader, model, loss_fn)))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c3512788-0b3f-47ae-8064-d5b3a8ae22a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtrain_acc, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtest_acc))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:645\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m color \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    643\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _default_color(ax\u001b[38;5;241m.\u001b[39mplot, hue, color, kwargs)\n\u001b[1;32m--> 645\u001b[0m p\u001b[38;5;241m.\u001b[39mplot(ax, kwargs)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\relational.py:423\u001b[0m, in \u001b[0;36m_LinePlotter.plot\u001b[1;34m(self, ax, kws)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# TODO How to handle NA? We don't want NA to propagate through to the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# estimate/CI when some values are present, but we would also like\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# matplotlib to show \"gaps\" in the line when all values are missing.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# Loop over the semantic subsets and add to the plot\u001b[39;00m\n\u001b[0;32m    422\u001b[0m grouping_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_vars, sub_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_data(grouping_vars, from_comp_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort:\n\u001b[0;32m    426\u001b[0m         sort_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, orient, other]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fpga\\Lib\\site-packages\\seaborn\\_oldcore.py:1065\u001b[0m, in \u001b[0;36mVectorPlotter.iter_data\u001b[1;34m(self, grouping_vars, reverse, from_comp_data, by_facet, allow_empty, dropna)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m grouping_vars:\n\u001b[0;32m   1063\u001b[0m     grouping_keys\u001b[38;5;241m.\u001b[39mappend(levels\u001b[38;5;241m.\u001b[39mget(var, []))\n\u001b[1;32m-> 1065\u001b[0m iter_keys \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mgrouping_keys)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reverse:\n\u001b[0;32m   1067\u001b[0m     iter_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(iter_keys))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot((1-train_acc, 1-test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
