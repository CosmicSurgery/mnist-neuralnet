{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d5877b-90a5-45c9-aba0-d76efb60c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8001e10b-4efd-44aa-bbc2-ae407c02d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Shape of X [N, C, H, W]: torch.Size([2, 1, 28, 28])\n",
      "Shape of y: torch.Size([2]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "LAYER_ONE_SIZE = 784\n",
    "LAYER_TWO_SIZE = 200\n",
    "LAYER_THREE_SIZE = 80\n",
    "LAYER_FOUR_SIZE = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a66e538-00fe-4127-9529-598b93c2541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork1(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=200, bias=False)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=200, out_features=80, bias=False)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=80, out_features=10, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, LAYER_TWO_SIZE, bias = False),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(LAYER_TWO_SIZE, LAYER_THREE_SIZE, bias = False),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(LAYER_THREE_SIZE, LAYER_FOUR_SIZE, bias = False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return (100*correct)\n",
    "\n",
    "model1 = NeuralNetwork1().to(device)\n",
    "# model2 = NeuralNetwork2().to(device)\n",
    "print(model1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=1e-2)\n",
    "# optimizer2 = torch.optim.SGD(model2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cc5fcd8-dd49-479c-aeb8-50b5a5766cf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.190320  [    2/60000]\n",
      "loss: 0.011296  [  202/60000]\n",
      "loss: 0.013661  [  402/60000]\n",
      "loss: 0.098771  [  602/60000]\n",
      "loss: 0.482596  [  802/60000]\n",
      "loss: 0.019149  [ 1002/60000]\n",
      "loss: 0.801377  [ 1202/60000]\n",
      "loss: 0.039846  [ 1402/60000]\n",
      "loss: 0.127609  [ 1602/60000]\n",
      "loss: 0.005249  [ 1802/60000]\n",
      "loss: 0.806561  [ 2002/60000]\n",
      "loss: 0.029430  [ 2202/60000]\n",
      "loss: 0.006587  [ 2402/60000]\n",
      "loss: 0.562397  [ 2602/60000]\n",
      "loss: 0.424573  [ 2802/60000]\n",
      "loss: 0.088213  [ 3002/60000]\n",
      "loss: 0.031906  [ 3202/60000]\n",
      "loss: 0.015859  [ 3402/60000]\n",
      "loss: 0.117567  [ 3602/60000]\n",
      "loss: 0.016858  [ 3802/60000]\n",
      "loss: 0.008090  [ 4002/60000]\n",
      "loss: 0.148585  [ 4202/60000]\n",
      "loss: 0.006214  [ 4402/60000]\n",
      "loss: 0.345707  [ 4602/60000]\n",
      "loss: 1.357677  [ 4802/60000]\n",
      "loss: 0.143221  [ 5002/60000]\n",
      "loss: 0.182180  [ 5202/60000]\n",
      "loss: 0.008501  [ 5402/60000]\n",
      "loss: 0.183328  [ 5602/60000]\n",
      "loss: 1.003575  [ 5802/60000]\n",
      "loss: 0.001594  [ 6002/60000]\n",
      "loss: 0.022815  [ 6202/60000]\n",
      "loss: 0.010673  [ 6402/60000]\n",
      "loss: 1.447415  [ 6602/60000]\n",
      "loss: 0.038559  [ 6802/60000]\n",
      "loss: 0.154637  [ 7002/60000]\n",
      "loss: 0.145308  [ 7202/60000]\n",
      "loss: 0.059891  [ 7402/60000]\n",
      "loss: 0.014700  [ 7602/60000]\n",
      "loss: 0.053080  [ 7802/60000]\n",
      "loss: 0.004427  [ 8002/60000]\n",
      "loss: 4.065157  [ 8202/60000]\n",
      "loss: 0.066518  [ 8402/60000]\n",
      "loss: 0.244234  [ 8602/60000]\n",
      "loss: 0.024452  [ 8802/60000]\n",
      "loss: 0.011215  [ 9002/60000]\n",
      "loss: 0.050546  [ 9202/60000]\n",
      "loss: 0.011541  [ 9402/60000]\n",
      "loss: 0.200442  [ 9602/60000]\n",
      "loss: 0.077557  [ 9802/60000]\n",
      "loss: 0.002812  [10002/60000]\n",
      "loss: 0.028329  [10202/60000]\n",
      "loss: 0.054498  [10402/60000]\n",
      "loss: 0.032362  [10602/60000]\n",
      "loss: 0.284605  [10802/60000]\n",
      "loss: 0.017959  [11002/60000]\n",
      "loss: 0.008173  [11202/60000]\n",
      "loss: 0.013539  [11402/60000]\n",
      "loss: 1.803945  [11602/60000]\n",
      "loss: 0.003465  [11802/60000]\n",
      "loss: 0.129518  [12002/60000]\n",
      "loss: 0.127088  [12202/60000]\n",
      "loss: 0.011727  [12402/60000]\n",
      "loss: 0.154879  [12602/60000]\n",
      "loss: 0.049438  [12802/60000]\n",
      "loss: 0.069403  [13002/60000]\n",
      "loss: 0.029980  [13202/60000]\n",
      "loss: 0.123407  [13402/60000]\n",
      "loss: 0.003267  [13602/60000]\n",
      "loss: 0.091161  [13802/60000]\n",
      "loss: 0.011277  [14002/60000]\n",
      "loss: 0.788917  [14202/60000]\n",
      "loss: 0.002923  [14402/60000]\n",
      "loss: 0.018083  [14602/60000]\n",
      "loss: 0.645845  [14802/60000]\n",
      "loss: 0.019270  [15002/60000]\n",
      "loss: 0.035167  [15202/60000]\n",
      "loss: 0.019318  [15402/60000]\n",
      "loss: 0.053633  [15602/60000]\n",
      "loss: 0.427847  [15802/60000]\n",
      "loss: 0.010326  [16002/60000]\n",
      "loss: 0.171440  [16202/60000]\n",
      "loss: 0.081212  [16402/60000]\n",
      "loss: 0.039595  [16602/60000]\n",
      "loss: 0.010722  [16802/60000]\n",
      "loss: 0.069964  [17002/60000]\n",
      "loss: 0.300276  [17202/60000]\n",
      "loss: 1.917778  [17402/60000]\n",
      "loss: 0.058765  [17602/60000]\n",
      "loss: 0.101931  [17802/60000]\n",
      "loss: 0.795653  [18002/60000]\n",
      "loss: 0.003190  [18202/60000]\n",
      "loss: 0.040569  [18402/60000]\n",
      "loss: 0.223528  [18602/60000]\n",
      "loss: 0.007302  [18802/60000]\n",
      "loss: 0.039219  [19002/60000]\n",
      "loss: 0.004331  [19202/60000]\n",
      "loss: 0.007650  [19402/60000]\n",
      "loss: 0.056350  [19602/60000]\n",
      "loss: 0.004159  [19802/60000]\n",
      "loss: 0.074180  [20002/60000]\n",
      "loss: 0.002162  [20202/60000]\n",
      "loss: 0.002898  [20402/60000]\n",
      "loss: 0.924648  [20602/60000]\n",
      "loss: 0.069885  [20802/60000]\n",
      "loss: 0.279439  [21002/60000]\n",
      "loss: 0.014254  [21202/60000]\n",
      "loss: 0.024312  [21402/60000]\n",
      "loss: 5.472639  [21602/60000]\n",
      "loss: 0.008306  [21802/60000]\n",
      "loss: 0.043705  [22002/60000]\n",
      "loss: 0.800299  [22202/60000]\n",
      "loss: 0.173545  [22402/60000]\n",
      "loss: 0.001203  [22602/60000]\n",
      "loss: 0.445987  [22802/60000]\n",
      "loss: 0.006609  [23002/60000]\n",
      "loss: 0.110756  [23202/60000]\n",
      "loss: 0.854469  [23402/60000]\n",
      "loss: 0.017619  [23602/60000]\n",
      "loss: 0.002489  [23802/60000]\n",
      "loss: 0.018090  [24002/60000]\n",
      "loss: 0.005562  [24202/60000]\n",
      "loss: 0.009023  [24402/60000]\n",
      "loss: 0.035124  [24602/60000]\n",
      "loss: 0.003926  [24802/60000]\n",
      "loss: 0.076213  [25002/60000]\n",
      "loss: 0.031765  [25202/60000]\n",
      "loss: 0.044524  [25402/60000]\n",
      "loss: 0.012735  [25602/60000]\n",
      "loss: 2.997260  [25802/60000]\n",
      "loss: 0.022445  [26002/60000]\n",
      "loss: 0.003020  [26202/60000]\n",
      "loss: 0.030148  [26402/60000]\n",
      "loss: 0.592635  [26602/60000]\n",
      "loss: 0.015749  [26802/60000]\n",
      "loss: 0.183638  [27002/60000]\n",
      "loss: 0.145039  [27202/60000]\n",
      "loss: 0.009093  [27402/60000]\n",
      "loss: 0.289951  [27602/60000]\n",
      "loss: 0.421291  [27802/60000]\n",
      "loss: 0.004249  [28002/60000]\n",
      "loss: 0.066946  [28202/60000]\n",
      "loss: 0.004460  [28402/60000]\n",
      "loss: 0.058415  [28602/60000]\n",
      "loss: 0.075897  [28802/60000]\n",
      "loss: 0.265311  [29002/60000]\n",
      "loss: 0.858424  [29202/60000]\n",
      "loss: 0.054113  [29402/60000]\n",
      "loss: 0.014494  [29602/60000]\n",
      "loss: 0.106073  [29802/60000]\n",
      "loss: 0.002909  [30002/60000]\n",
      "loss: 0.015569  [30202/60000]\n",
      "loss: 0.060256  [30402/60000]\n",
      "loss: 0.918825  [30602/60000]\n",
      "loss: 0.208630  [30802/60000]\n",
      "loss: 0.314687  [31002/60000]\n",
      "loss: 1.374131  [31202/60000]\n",
      "loss: 0.027433  [31402/60000]\n",
      "loss: 1.284879  [31602/60000]\n",
      "loss: 0.050353  [31802/60000]\n",
      "loss: 0.015544  [32002/60000]\n",
      "loss: 0.026001  [32202/60000]\n",
      "loss: 0.022829  [32402/60000]\n",
      "loss: 0.012394  [32602/60000]\n",
      "loss: 0.005202  [32802/60000]\n",
      "loss: 0.652084  [33002/60000]\n",
      "loss: 0.042369  [33202/60000]\n",
      "loss: 0.015336  [33402/60000]\n",
      "loss: 0.015155  [33602/60000]\n",
      "loss: 0.004314  [33802/60000]\n",
      "loss: 0.118424  [34002/60000]\n",
      "loss: 0.005450  [34202/60000]\n",
      "loss: 0.006398  [34402/60000]\n",
      "loss: 0.022607  [34602/60000]\n",
      "loss: 1.815929  [34802/60000]\n",
      "loss: 0.004311  [35002/60000]\n",
      "loss: 0.002214  [35202/60000]\n",
      "loss: 1.319204  [35402/60000]\n",
      "loss: 0.005045  [35602/60000]\n",
      "loss: 0.003906  [35802/60000]\n",
      "loss: 0.224529  [36002/60000]\n",
      "loss: 0.018884  [36202/60000]\n",
      "loss: 0.018864  [36402/60000]\n",
      "loss: 0.025304  [36602/60000]\n",
      "loss: 0.068688  [36802/60000]\n",
      "loss: 0.002662  [37002/60000]\n",
      "loss: 0.012874  [37202/60000]\n",
      "loss: 0.110879  [37402/60000]\n",
      "loss: 0.033258  [37602/60000]\n",
      "loss: 0.065195  [37802/60000]\n",
      "loss: 0.036898  [38002/60000]\n",
      "loss: 0.000763  [38202/60000]\n",
      "loss: 0.023867  [38402/60000]\n",
      "loss: 0.039638  [38602/60000]\n",
      "loss: 0.194364  [38802/60000]\n",
      "loss: 0.325852  [39002/60000]\n",
      "loss: 0.005153  [39202/60000]\n",
      "loss: 0.007128  [39402/60000]\n",
      "loss: 1.763894  [39602/60000]\n",
      "loss: 0.008028  [39802/60000]\n",
      "loss: 0.007886  [40002/60000]\n",
      "loss: 0.008165  [40202/60000]\n",
      "loss: 0.115551  [40402/60000]\n",
      "loss: 0.016061  [40602/60000]\n",
      "loss: 0.071532  [40802/60000]\n",
      "loss: 0.036993  [41002/60000]\n",
      "loss: 0.039419  [41202/60000]\n",
      "loss: 0.382225  [41402/60000]\n",
      "loss: 0.028638  [41602/60000]\n",
      "loss: 0.264820  [41802/60000]\n",
      "loss: 0.001408  [42002/60000]\n",
      "loss: 0.031470  [42202/60000]\n",
      "loss: 0.119274  [42402/60000]\n",
      "loss: 0.009053  [42602/60000]\n",
      "loss: 0.020190  [42802/60000]\n",
      "loss: 0.180787  [43002/60000]\n",
      "loss: 0.407687  [43202/60000]\n",
      "loss: 0.063865  [43402/60000]\n",
      "loss: 0.075974  [43602/60000]\n",
      "loss: 0.092359  [43802/60000]\n",
      "loss: 0.017645  [44002/60000]\n",
      "loss: 0.035257  [44202/60000]\n",
      "loss: 0.003607  [44402/60000]\n",
      "loss: 0.028027  [44602/60000]\n",
      "loss: 0.056096  [44802/60000]\n",
      "loss: 0.019224  [45002/60000]\n",
      "loss: 0.494383  [45202/60000]\n",
      "loss: 0.002064  [45402/60000]\n",
      "loss: 0.010775  [45602/60000]\n",
      "loss: 5.559150  [45802/60000]\n",
      "loss: 0.115294  [46002/60000]\n",
      "loss: 0.085463  [46202/60000]\n",
      "loss: 0.072614  [46402/60000]\n",
      "loss: 0.131905  [46602/60000]\n",
      "loss: 0.027404  [46802/60000]\n",
      "loss: 0.003677  [47002/60000]\n",
      "loss: 0.042797  [47202/60000]\n",
      "loss: 0.265423  [47402/60000]\n",
      "loss: 1.715203  [47602/60000]\n",
      "loss: 0.001956  [47802/60000]\n",
      "loss: 0.083733  [48002/60000]\n",
      "loss: 0.040733  [48202/60000]\n",
      "loss: 0.007337  [48402/60000]\n",
      "loss: 0.009204  [48602/60000]\n",
      "loss: 0.508529  [48802/60000]\n",
      "loss: 0.571391  [49002/60000]\n",
      "loss: 0.585754  [49202/60000]\n",
      "loss: 0.042273  [49402/60000]\n",
      "loss: 0.014555  [49602/60000]\n",
      "loss: 0.239868  [49802/60000]\n",
      "loss: 0.046787  [50002/60000]\n",
      "loss: 0.027094  [50202/60000]\n",
      "loss: 0.009986  [50402/60000]\n",
      "loss: 0.107958  [50602/60000]\n",
      "loss: 0.084054  [50802/60000]\n",
      "loss: 0.015392  [51002/60000]\n",
      "loss: 1.215411  [51202/60000]\n",
      "loss: 0.007006  [51402/60000]\n",
      "loss: 1.701610  [51602/60000]\n",
      "loss: 0.008283  [51802/60000]\n",
      "loss: 0.015611  [52002/60000]\n",
      "loss: 0.097850  [52202/60000]\n",
      "loss: 0.011028  [52402/60000]\n",
      "loss: 0.186264  [52602/60000]\n",
      "loss: 2.488796  [52802/60000]\n",
      "loss: 0.025109  [53002/60000]\n",
      "loss: 0.007179  [53202/60000]\n",
      "loss: 0.012910  [53402/60000]\n",
      "loss: 0.031066  [53602/60000]\n",
      "loss: 0.032324  [53802/60000]\n",
      "loss: 0.124501  [54002/60000]\n",
      "loss: 0.022437  [54202/60000]\n",
      "loss: 0.622246  [54402/60000]\n",
      "loss: 0.076458  [54602/60000]\n",
      "loss: 0.034053  [54802/60000]\n",
      "loss: 0.061791  [55002/60000]\n",
      "loss: 0.016612  [55202/60000]\n",
      "loss: 0.133866  [55402/60000]\n",
      "loss: 0.016856  [55602/60000]\n",
      "loss: 0.012127  [55802/60000]\n",
      "loss: 0.143220  [56002/60000]\n",
      "loss: 0.014086  [56202/60000]\n",
      "loss: 0.070470  [56402/60000]\n",
      "loss: 0.601468  [56602/60000]\n",
      "loss: 1.833099  [56802/60000]\n",
      "loss: 0.009977  [57002/60000]\n",
      "loss: 0.005609  [57202/60000]\n",
      "loss: 0.048679  [57402/60000]\n",
      "loss: 0.029393  [57602/60000]\n",
      "loss: 0.026146  [57802/60000]\n",
      "loss: 0.007184  [58002/60000]\n",
      "loss: 0.001998  [58202/60000]\n",
      "loss: 0.350363  [58402/60000]\n",
      "loss: 0.011325  [58602/60000]\n",
      "loss: 0.004007  [58802/60000]\n",
      "loss: 0.105017  [59002/60000]\n",
      "loss: 0.048938  [59202/60000]\n",
      "loss: 1.670998  [59402/60000]\n",
      "loss: 0.004444  [59602/60000]\n",
      "loss: 0.003237  [59802/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.201335 \n",
      "\n",
      "Model 1 Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.099524  [    2/60000]\n",
      "loss: 0.009614  [  202/60000]\n",
      "loss: 0.011457  [  402/60000]\n",
      "loss: 0.051729  [  602/60000]\n",
      "loss: 0.195315  [  802/60000]\n",
      "loss: 0.012387  [ 1002/60000]\n",
      "loss: 0.702497  [ 1202/60000]\n",
      "loss: 0.040457  [ 1402/60000]\n",
      "loss: 0.086386  [ 1602/60000]\n",
      "loss: 0.003770  [ 1802/60000]\n",
      "loss: 0.749319  [ 2002/60000]\n",
      "loss: 0.020235  [ 2202/60000]\n",
      "loss: 0.003347  [ 2402/60000]\n",
      "loss: 0.720824  [ 2602/60000]\n",
      "loss: 0.375511  [ 2802/60000]\n",
      "loss: 0.051342  [ 3002/60000]\n",
      "loss: 0.017925  [ 3202/60000]\n",
      "loss: 0.009304  [ 3402/60000]\n",
      "loss: 0.071376  [ 3602/60000]\n",
      "loss: 0.011815  [ 3802/60000]\n",
      "loss: 0.005700  [ 4002/60000]\n",
      "loss: 0.170504  [ 4202/60000]\n",
      "loss: 0.005332  [ 4402/60000]\n",
      "loss: 0.188466  [ 4602/60000]\n",
      "loss: 0.811028  [ 4802/60000]\n",
      "loss: 0.117756  [ 5002/60000]\n",
      "loss: 0.113378  [ 5202/60000]\n",
      "loss: 0.004990  [ 5402/60000]\n",
      "loss: 0.258058  [ 5602/60000]\n",
      "loss: 0.563047  [ 5802/60000]\n",
      "loss: 0.001175  [ 6002/60000]\n",
      "loss: 0.010968  [ 6202/60000]\n",
      "loss: 0.007715  [ 6402/60000]\n",
      "loss: 1.001313  [ 6602/60000]\n",
      "loss: 0.019126  [ 6802/60000]\n",
      "loss: 0.112849  [ 7002/60000]\n",
      "loss: 0.109170  [ 7202/60000]\n",
      "loss: 0.050839  [ 7402/60000]\n",
      "loss: 0.005870  [ 7602/60000]\n",
      "loss: 0.055928  [ 7802/60000]\n",
      "loss: 0.003100  [ 8002/60000]\n",
      "loss: 4.030754  [ 8202/60000]\n",
      "loss: 0.078049  [ 8402/60000]\n",
      "loss: 0.195060  [ 8602/60000]\n",
      "loss: 0.019326  [ 8802/60000]\n",
      "loss: 0.006870  [ 9002/60000]\n",
      "loss: 0.046833  [ 9202/60000]\n",
      "loss: 0.010194  [ 9402/60000]\n",
      "loss: 0.094501  [ 9602/60000]\n",
      "loss: 0.029452  [ 9802/60000]\n",
      "loss: 0.001469  [10002/60000]\n",
      "loss: 0.013548  [10202/60000]\n",
      "loss: 0.026895  [10402/60000]\n",
      "loss: 0.018799  [10602/60000]\n",
      "loss: 0.274240  [10802/60000]\n",
      "loss: 0.015219  [11002/60000]\n",
      "loss: 0.005977  [11202/60000]\n",
      "loss: 0.009485  [11402/60000]\n",
      "loss: 1.165147  [11602/60000]\n",
      "loss: 0.004723  [11802/60000]\n",
      "loss: 0.122806  [12002/60000]\n",
      "loss: 0.133518  [12202/60000]\n",
      "loss: 0.009975  [12402/60000]\n",
      "loss: 0.122535  [12602/60000]\n",
      "loss: 0.016674  [12802/60000]\n",
      "loss: 0.043154  [13002/60000]\n",
      "loss: 0.017661  [13202/60000]\n",
      "loss: 0.062825  [13402/60000]\n",
      "loss: 0.002904  [13602/60000]\n",
      "loss: 0.055036  [13802/60000]\n",
      "loss: 0.010077  [14002/60000]\n",
      "loss: 0.766910  [14202/60000]\n",
      "loss: 0.002125  [14402/60000]\n",
      "loss: 0.014915  [14602/60000]\n",
      "loss: 0.517517  [14802/60000]\n",
      "loss: 0.006997  [15002/60000]\n",
      "loss: 0.019968  [15202/60000]\n",
      "loss: 0.016513  [15402/60000]\n",
      "loss: 0.033340  [15602/60000]\n",
      "loss: 0.359815  [15802/60000]\n",
      "loss: 0.007108  [16002/60000]\n",
      "loss: 0.106827  [16202/60000]\n",
      "loss: 0.030390  [16402/60000]\n",
      "loss: 0.024480  [16602/60000]\n",
      "loss: 0.005595  [16802/60000]\n",
      "loss: 0.084584  [17002/60000]\n",
      "loss: 0.320103  [17202/60000]\n",
      "loss: 1.763325  [17402/60000]\n",
      "loss: 0.064383  [17602/60000]\n",
      "loss: 0.082814  [17802/60000]\n",
      "loss: 0.535792  [18002/60000]\n",
      "loss: 0.002096  [18202/60000]\n",
      "loss: 0.033648  [18402/60000]\n",
      "loss: 0.138072  [18602/60000]\n",
      "loss: 0.005455  [18802/60000]\n",
      "loss: 0.025839  [19002/60000]\n",
      "loss: 0.004964  [19202/60000]\n",
      "loss: 0.006229  [19402/60000]\n",
      "loss: 0.038450  [19602/60000]\n",
      "loss: 0.002527  [19802/60000]\n",
      "loss: 0.057028  [20002/60000]\n",
      "loss: 0.001141  [20202/60000]\n",
      "loss: 0.002341  [20402/60000]\n",
      "loss: 0.846542  [20602/60000]\n",
      "loss: 0.052021  [20802/60000]\n",
      "loss: 0.245291  [21002/60000]\n",
      "loss: 0.009427  [21202/60000]\n",
      "loss: 0.020094  [21402/60000]\n",
      "loss: 5.567694  [21602/60000]\n",
      "loss: 0.011833  [21802/60000]\n",
      "loss: 0.041180  [22002/60000]\n",
      "loss: 0.528074  [22202/60000]\n",
      "loss: 0.144445  [22402/60000]\n",
      "loss: 0.001076  [22602/60000]\n",
      "loss: 0.276662  [22802/60000]\n",
      "loss: 0.002788  [23002/60000]\n",
      "loss: 0.047126  [23202/60000]\n",
      "loss: 0.986133  [23402/60000]\n",
      "loss: 0.013677  [23602/60000]\n",
      "loss: 0.002296  [23802/60000]\n",
      "loss: 0.015278  [24002/60000]\n",
      "loss: 0.002705  [24202/60000]\n",
      "loss: 0.005943  [24402/60000]\n",
      "loss: 0.013185  [24602/60000]\n",
      "loss: 0.003030  [24802/60000]\n",
      "loss: 0.053585  [25002/60000]\n",
      "loss: 0.031391  [25202/60000]\n",
      "loss: 0.052331  [25402/60000]\n",
      "loss: 0.011444  [25602/60000]\n",
      "loss: 2.892918  [25802/60000]\n",
      "loss: 0.018831  [26002/60000]\n",
      "loss: 0.001809  [26202/60000]\n",
      "loss: 0.024981  [26402/60000]\n",
      "loss: 0.427826  [26602/60000]\n",
      "loss: 0.017963  [26802/60000]\n",
      "loss: 0.129494  [27002/60000]\n",
      "loss: 0.134742  [27202/60000]\n",
      "loss: 0.005548  [27402/60000]\n",
      "loss: 0.200942  [27602/60000]\n",
      "loss: 0.281549  [27802/60000]\n",
      "loss: 0.003397  [28002/60000]\n",
      "loss: 0.050260  [28202/60000]\n",
      "loss: 0.002769  [28402/60000]\n",
      "loss: 0.045501  [28602/60000]\n",
      "loss: 0.090310  [28802/60000]\n",
      "loss: 0.205780  [29002/60000]\n",
      "loss: 0.713508  [29202/60000]\n",
      "loss: 0.020762  [29402/60000]\n",
      "loss: 0.007233  [29602/60000]\n",
      "loss: 0.062421  [29802/60000]\n",
      "loss: 0.002228  [30002/60000]\n",
      "loss: 0.013923  [30202/60000]\n",
      "loss: 0.048885  [30402/60000]\n",
      "loss: 0.604747  [30602/60000]\n",
      "loss: 0.156040  [30802/60000]\n",
      "loss: 0.269898  [31002/60000]\n",
      "loss: 0.878616  [31202/60000]\n",
      "loss: 0.012498  [31402/60000]\n",
      "loss: 1.513770  [31602/60000]\n",
      "loss: 0.025004  [31802/60000]\n",
      "loss: 0.008299  [32002/60000]\n",
      "loss: 0.018380  [32202/60000]\n",
      "loss: 0.016230  [32402/60000]\n",
      "loss: 0.004966  [32602/60000]\n",
      "loss: 0.003400  [32802/60000]\n",
      "loss: 0.507560  [33002/60000]\n",
      "loss: 0.017241  [33202/60000]\n",
      "loss: 0.013332  [33402/60000]\n",
      "loss: 0.011977  [33602/60000]\n",
      "loss: 0.003669  [33802/60000]\n",
      "loss: 0.066697  [34002/60000]\n",
      "loss: 0.004369  [34202/60000]\n",
      "loss: 0.002279  [34402/60000]\n",
      "loss: 0.014488  [34602/60000]\n",
      "loss: 1.636181  [34802/60000]\n",
      "loss: 0.003019  [35002/60000]\n",
      "loss: 0.001171  [35202/60000]\n",
      "loss: 1.154003  [35402/60000]\n",
      "loss: 0.003795  [35602/60000]\n",
      "loss: 0.002484  [35802/60000]\n",
      "loss: 0.185228  [36002/60000]\n",
      "loss: 0.015537  [36202/60000]\n",
      "loss: 0.013695  [36402/60000]\n",
      "loss: 0.017831  [36602/60000]\n",
      "loss: 0.067840  [36802/60000]\n",
      "loss: 0.001810  [37002/60000]\n",
      "loss: 0.010493  [37202/60000]\n",
      "loss: 0.084966  [37402/60000]\n",
      "loss: 0.016159  [37602/60000]\n",
      "loss: 0.074288  [37802/60000]\n",
      "loss: 0.031564  [38002/60000]\n",
      "loss: 0.000555  [38202/60000]\n",
      "loss: 0.023821  [38402/60000]\n",
      "loss: 0.032367  [38602/60000]\n",
      "loss: 0.117645  [38802/60000]\n",
      "loss: 0.229210  [39002/60000]\n",
      "loss: 0.003883  [39202/60000]\n",
      "loss: 0.003155  [39402/60000]\n",
      "loss: 1.837147  [39602/60000]\n",
      "loss: 0.006656  [39802/60000]\n",
      "loss: 0.005045  [40002/60000]\n",
      "loss: 0.004898  [40202/60000]\n",
      "loss: 0.097011  [40402/60000]\n",
      "loss: 0.013480  [40602/60000]\n",
      "loss: 0.050273  [40802/60000]\n",
      "loss: 0.028237  [41002/60000]\n",
      "loss: 0.014374  [41202/60000]\n",
      "loss: 0.411693  [41402/60000]\n",
      "loss: 0.019108  [41602/60000]\n",
      "loss: 0.156604  [41802/60000]\n",
      "loss: 0.001156  [42002/60000]\n",
      "loss: 0.022546  [42202/60000]\n",
      "loss: 0.096069  [42402/60000]\n",
      "loss: 0.005612  [42602/60000]\n",
      "loss: 0.012541  [42802/60000]\n",
      "loss: 0.147806  [43002/60000]\n",
      "loss: 0.340733  [43202/60000]\n",
      "loss: 0.058819  [43402/60000]\n",
      "loss: 0.049215  [43602/60000]\n",
      "loss: 0.074946  [43802/60000]\n",
      "loss: 0.011760  [44002/60000]\n",
      "loss: 0.019981  [44202/60000]\n",
      "loss: 0.002504  [44402/60000]\n",
      "loss: 0.017181  [44602/60000]\n",
      "loss: 0.029730  [44802/60000]\n",
      "loss: 0.018866  [45002/60000]\n",
      "loss: 0.671250  [45202/60000]\n",
      "loss: 0.001455  [45402/60000]\n",
      "loss: 0.006672  [45602/60000]\n",
      "loss: 5.074508  [45802/60000]\n",
      "loss: 0.059634  [46002/60000]\n",
      "loss: 0.051887  [46202/60000]\n",
      "loss: 0.061252  [46402/60000]\n",
      "loss: 0.166871  [46602/60000]\n",
      "loss: 0.015959  [46802/60000]\n",
      "loss: 0.002195  [47002/60000]\n",
      "loss: 0.033108  [47202/60000]\n",
      "loss: 0.170798  [47402/60000]\n",
      "loss: 1.927293  [47602/60000]\n",
      "loss: 0.002064  [47802/60000]\n",
      "loss: 0.088922  [48002/60000]\n",
      "loss: 0.023172  [48202/60000]\n",
      "loss: 0.008356  [48402/60000]\n",
      "loss: 0.006759  [48602/60000]\n",
      "loss: 0.390757  [48802/60000]\n",
      "loss: 0.648288  [49002/60000]\n",
      "loss: 0.595070  [49202/60000]\n",
      "loss: 0.032363  [49402/60000]\n",
      "loss: 0.015943  [49602/60000]\n",
      "loss: 0.238466  [49802/60000]\n",
      "loss: 0.025669  [50002/60000]\n",
      "loss: 0.011223  [50202/60000]\n",
      "loss: 0.007596  [50402/60000]\n",
      "loss: 0.084630  [50602/60000]\n",
      "loss: 0.050706  [50802/60000]\n",
      "loss: 0.012713  [51002/60000]\n",
      "loss: 1.197777  [51202/60000]\n",
      "loss: 0.004499  [51402/60000]\n",
      "loss: 1.592677  [51602/60000]\n",
      "loss: 0.005537  [51802/60000]\n",
      "loss: 0.009874  [52002/60000]\n",
      "loss: 0.063239  [52202/60000]\n",
      "loss: 0.005414  [52402/60000]\n",
      "loss: 0.181836  [52602/60000]\n",
      "loss: 2.592549  [52802/60000]\n",
      "loss: 0.014708  [53002/60000]\n",
      "loss: 0.003166  [53202/60000]\n",
      "loss: 0.017581  [53402/60000]\n",
      "loss: 0.027000  [53602/60000]\n",
      "loss: 0.017935  [53802/60000]\n",
      "loss: 0.069493  [54002/60000]\n",
      "loss: 0.014982  [54202/60000]\n",
      "loss: 0.458753  [54402/60000]\n",
      "loss: 0.078976  [54602/60000]\n",
      "loss: 0.022216  [54802/60000]\n",
      "loss: 0.028976  [55002/60000]\n",
      "loss: 0.014615  [55202/60000]\n",
      "loss: 0.080445  [55402/60000]\n",
      "loss: 0.012115  [55602/60000]\n",
      "loss: 0.011502  [55802/60000]\n",
      "loss: 0.109009  [56002/60000]\n",
      "loss: 0.012199  [56202/60000]\n",
      "loss: 0.056496  [56402/60000]\n",
      "loss: 0.415463  [56602/60000]\n",
      "loss: 1.773859  [56802/60000]\n",
      "loss: 0.005521  [57002/60000]\n",
      "loss: 0.003424  [57202/60000]\n",
      "loss: 0.035468  [57402/60000]\n",
      "loss: 0.022192  [57602/60000]\n",
      "loss: 0.020286  [57802/60000]\n",
      "loss: 0.004030  [58002/60000]\n",
      "loss: 0.001520  [58202/60000]\n",
      "loss: 0.304286  [58402/60000]\n",
      "loss: 0.009006  [58602/60000]\n",
      "loss: 0.001934  [58802/60000]\n",
      "loss: 0.038534  [59002/60000]\n",
      "loss: 0.037879  [59202/60000]\n",
      "loss: 1.199655  [59402/60000]\n",
      "loss: 0.002928  [59602/60000]\n",
      "loss: 0.001682  [59802/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.173138 \n",
      "\n",
      "Done! 89.74926519393921\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "start1 = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Model 1 Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model1, loss_fn, optimizer1)\n",
    "    test(test_dataloader, model1, loss_fn)\n",
    "end1 = time.time()\n",
    "print(\"Done!\", end1-start1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f37980-65e3-4aec-bc2b-81a71b64b6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
