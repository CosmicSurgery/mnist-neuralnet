{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8681f488-e79b-454c-acc2-e83f791b7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2763bd-e1b1-4821-93a8-eac32b899c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4271ef60-f0ea-4f67-b80e-8161e856cc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "bs = 1\n",
    "epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1151e913-2145-4afc-ba93-bfeee7a969f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "def get_model():\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "def get_data(training_data, test_data, bs):\n",
    "    return (\n",
    "        DataLoader(training_data, batch_size=bs), #, shuffle=True),\n",
    "        DataLoader(test_data, batch_size=bs),\n",
    "    )\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    correct = 0\n",
    "    size = len(train_dl.dataset)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch, (xb, yb) in enumerate(train_dl):\n",
    "            # xb, yb = xb.to(device), yb.to(device)\n",
    "            loss, _ = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            # correct += bool(model(xb).argmax() == yb)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                current = (batch + 1) * len(xb)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        \n",
    "\n",
    "        # correct /= len(train_dl.dataset)\n",
    "\n",
    "        # print(f\"Training Error: \\n Accuracy: {(100*correct):>0.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40075769-4896-4b8d-a0aa-20bb86aa1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(training_data, test_data, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04026ee1-e6a8-4ae4-8290-04f619c6bb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.399653  [    1/60000]\n",
      "loss: 2.501658  [  101/60000]\n",
      "loss: 2.463152  [  201/60000]\n",
      "loss: 2.052140  [  301/60000]\n",
      "loss: 2.062002  [  401/60000]\n",
      "loss: 2.140464  [  501/60000]\n",
      "loss: 2.372984  [  601/60000]\n",
      "loss: 2.181933  [  701/60000]\n",
      "loss: 2.231670  [  801/60000]\n",
      "loss: 2.377624  [  901/60000]\n",
      "loss: 2.330267  [ 1001/60000]\n",
      "loss: 2.336244  [ 1101/60000]\n",
      "loss: 2.223290  [ 1201/60000]\n",
      "loss: 2.095239  [ 1301/60000]\n",
      "loss: 2.088407  [ 1401/60000]\n",
      "loss: 2.224381  [ 1501/60000]\n",
      "loss: 2.407632  [ 1601/60000]\n",
      "loss: 2.197903  [ 1701/60000]\n",
      "loss: 1.949502  [ 1801/60000]\n",
      "loss: 2.035360  [ 1901/60000]\n",
      "loss: 2.712936  [ 2001/60000]\n",
      "loss: 1.718874  [ 2101/60000]\n",
      "loss: 1.546966  [ 2201/60000]\n",
      "loss: 2.099437  [ 2301/60000]\n",
      "loss: 1.999518  [ 2401/60000]\n",
      "loss: 1.646725  [ 2501/60000]\n",
      "loss: 2.030183  [ 2601/60000]\n",
      "loss: 1.635978  [ 2701/60000]\n",
      "loss: 1.797469  [ 2801/60000]\n",
      "loss: 2.535216  [ 2901/60000]\n",
      "loss: 2.357742  [ 3001/60000]\n",
      "loss: 1.245815  [ 3101/60000]\n",
      "loss: 1.886808  [ 3201/60000]\n",
      "loss: 1.857645  [ 3301/60000]\n",
      "loss: 1.317118  [ 3401/60000]\n",
      "loss: 2.474105  [ 3501/60000]\n",
      "loss: 2.080749  [ 3601/60000]\n",
      "loss: 2.146750  [ 3701/60000]\n",
      "loss: 1.824239  [ 3801/60000]\n",
      "loss: 1.670927  [ 3901/60000]\n",
      "loss: 1.477537  [ 4001/60000]\n",
      "loss: 1.805615  [ 4101/60000]\n",
      "loss: 1.802143  [ 4201/60000]\n",
      "loss: 1.322692  [ 4301/60000]\n",
      "loss: 1.074519  [ 4401/60000]\n",
      "loss: 1.733799  [ 4501/60000]\n",
      "loss: 2.487252  [ 4601/60000]\n",
      "loss: 1.970451  [ 4701/60000]\n",
      "loss: 2.096550  [ 4801/60000]\n",
      "loss: 1.782357  [ 4901/60000]\n",
      "loss: 1.230931  [ 5001/60000]\n",
      "loss: 0.992010  [ 5101/60000]\n",
      "loss: 1.855284  [ 5201/60000]\n",
      "loss: 0.672718  [ 5301/60000]\n",
      "loss: 1.775013  [ 5401/60000]\n",
      "loss: 1.037364  [ 5501/60000]\n",
      "loss: 1.519436  [ 5601/60000]\n",
      "loss: 1.815870  [ 5701/60000]\n",
      "loss: 1.676224  [ 5801/60000]\n",
      "loss: 1.104862  [ 5901/60000]\n",
      "loss: 0.831079  [ 6001/60000]\n",
      "loss: 1.097087  [ 6101/60000]\n",
      "loss: 1.832000  [ 6201/60000]\n",
      "loss: 1.903916  [ 6301/60000]\n",
      "loss: 0.293180  [ 6401/60000]\n",
      "loss: 1.094437  [ 6501/60000]\n",
      "loss: 1.109300  [ 6601/60000]\n",
      "loss: 0.101540  [ 6701/60000]\n",
      "loss: 0.969290  [ 6801/60000]\n",
      "loss: 0.537288  [ 6901/60000]\n",
      "loss: 1.956707  [ 7001/60000]\n",
      "loss: 0.643509  [ 7101/60000]\n",
      "loss: 1.396074  [ 7201/60000]\n",
      "loss: 2.677383  [ 7301/60000]\n",
      "loss: 1.604929  [ 7401/60000]\n",
      "loss: 0.811715  [ 7501/60000]\n",
      "loss: 1.266056  [ 7601/60000]\n",
      "loss: 1.164005  [ 7701/60000]\n",
      "loss: 1.222153  [ 7801/60000]\n",
      "loss: 1.759812  [ 7901/60000]\n",
      "loss: 0.114509  [ 8001/60000]\n",
      "loss: 0.664261  [ 8101/60000]\n",
      "loss: 3.676215  [ 8201/60000]\n",
      "loss: 1.328023  [ 8301/60000]\n",
      "loss: 0.897721  [ 8401/60000]\n",
      "loss: 0.338270  [ 8501/60000]\n",
      "loss: 1.711863  [ 8601/60000]\n",
      "loss: 1.553795  [ 8701/60000]\n",
      "loss: 0.472627  [ 8801/60000]\n",
      "loss: 1.344985  [ 8901/60000]\n",
      "loss: 0.734352  [ 9001/60000]\n",
      "loss: 0.255698  [ 9101/60000]\n",
      "loss: 0.442415  [ 9201/60000]\n",
      "loss: 0.614379  [ 9301/60000]\n",
      "loss: 0.373775  [ 9401/60000]\n",
      "loss: 0.315815  [ 9501/60000]\n",
      "loss: 0.215428  [ 9601/60000]\n",
      "loss: 0.147157  [ 9701/60000]\n",
      "loss: 1.168159  [ 9801/60000]\n",
      "loss: 0.814028  [ 9901/60000]\n",
      "loss: 0.196807  [10001/60000]\n",
      "loss: 0.407403  [10101/60000]\n",
      "loss: 0.622246  [10201/60000]\n",
      "loss: 0.379100  [10301/60000]\n",
      "loss: 1.391181  [10401/60000]\n",
      "loss: 0.132699  [10501/60000]\n",
      "loss: 0.225664  [10601/60000]\n",
      "loss: 0.499541  [10701/60000]\n",
      "loss: 1.196697  [10801/60000]\n",
      "loss: 0.203046  [10901/60000]\n",
      "loss: 0.086218  [11001/60000]\n",
      "loss: 0.870234  [11101/60000]\n",
      "loss: 0.308804  [11201/60000]\n",
      "loss: 1.433577  [11301/60000]\n",
      "loss: 0.425638  [11401/60000]\n",
      "loss: 1.887674  [11501/60000]\n",
      "loss: 2.888432  [11601/60000]\n",
      "loss: 0.413822  [11701/60000]\n",
      "loss: 0.161610  [11801/60000]\n",
      "loss: 0.829770  [11901/60000]\n",
      "loss: 0.363267  [12001/60000]\n",
      "loss: 0.075099  [12101/60000]\n",
      "loss: 0.798625  [12201/60000]\n",
      "loss: 0.399443  [12301/60000]\n",
      "loss: 0.504135  [12401/60000]\n",
      "loss: 0.075074  [12501/60000]\n",
      "loss: 1.117054  [12601/60000]\n",
      "loss: 0.301538  [12701/60000]\n",
      "loss: 1.030288  [12801/60000]\n",
      "loss: 1.391145  [12901/60000]\n",
      "loss: 0.556539  [13001/60000]\n",
      "loss: 0.794237  [13101/60000]\n",
      "loss: 0.753025  [13201/60000]\n",
      "loss: 0.807214  [13301/60000]\n",
      "loss: 0.982221  [13401/60000]\n",
      "loss: 0.654983  [13501/60000]\n",
      "loss: 0.038299  [13601/60000]\n",
      "loss: 0.379812  [13701/60000]\n",
      "loss: 1.041994  [13801/60000]\n",
      "loss: 0.740023  [13901/60000]\n",
      "loss: 0.114997  [14001/60000]\n",
      "loss: 3.550425  [14101/60000]\n",
      "loss: 0.174782  [14201/60000]\n",
      "loss: 0.726691  [14301/60000]\n",
      "loss: 0.107084  [14401/60000]\n",
      "loss: 2.402642  [14501/60000]\n",
      "loss: 0.441472  [14601/60000]\n",
      "loss: 0.823295  [14701/60000]\n",
      "loss: 0.526524  [14801/60000]\n",
      "loss: 0.140489  [14901/60000]\n",
      "loss: 0.867073  [15001/60000]\n",
      "loss: 0.368478  [15101/60000]\n",
      "loss: 0.054406  [15201/60000]\n",
      "loss: 0.166033  [15301/60000]\n",
      "loss: 0.336315  [15401/60000]\n",
      "loss: 0.578300  [15501/60000]\n",
      "loss: 0.015287  [15601/60000]\n",
      "loss: 0.013835  [15701/60000]\n",
      "loss: 0.274641  [15801/60000]\n",
      "loss: 0.027505  [15901/60000]\n",
      "loss: 0.062347  [16001/60000]\n",
      "loss: 2.843605  [16101/60000]\n",
      "loss: 2.412984  [16201/60000]\n",
      "loss: 0.087431  [16301/60000]\n",
      "loss: 1.033873  [16401/60000]\n",
      "loss: 0.117312  [16501/60000]\n",
      "loss: 0.943451  [16601/60000]\n",
      "loss: 0.556617  [16701/60000]\n",
      "loss: 0.066250  [16801/60000]\n",
      "loss: 0.534506  [16901/60000]\n",
      "loss: 0.333005  [17001/60000]\n",
      "loss: 1.210650  [17101/60000]\n",
      "loss: 0.174162  [17201/60000]\n",
      "loss: 0.364385  [17301/60000]\n",
      "loss: 2.039260  [17401/60000]\n",
      "loss: 0.434114  [17501/60000]\n",
      "loss: 0.045475  [17601/60000]\n",
      "loss: 1.616849  [17701/60000]\n",
      "loss: 0.398131  [17801/60000]\n",
      "loss: 0.438233  [17901/60000]\n",
      "loss: 0.211893  [18001/60000]\n",
      "loss: 0.307828  [18101/60000]\n",
      "loss: 0.263954  [18201/60000]\n",
      "loss: 0.095220  [18301/60000]\n",
      "loss: 0.257680  [18401/60000]\n",
      "loss: 1.134519  [18501/60000]\n",
      "loss: 0.415140  [18601/60000]\n",
      "loss: 0.992537  [18701/60000]\n",
      "loss: 0.079207  [18801/60000]\n",
      "loss: 1.752079  [18901/60000]\n",
      "loss: 0.513849  [19001/60000]\n",
      "loss: 0.038687  [19101/60000]\n",
      "loss: 0.045344  [19201/60000]\n",
      "loss: 0.330833  [19301/60000]\n",
      "loss: 0.110255  [19401/60000]\n",
      "loss: 0.008104  [19501/60000]\n",
      "loss: 0.815704  [19601/60000]\n",
      "loss: 0.489350  [19701/60000]\n",
      "loss: 0.090176  [19801/60000]\n",
      "loss: 0.002969  [19901/60000]\n",
      "loss: 0.008754  [20001/60000]\n",
      "loss: 3.553015  [20101/60000]\n",
      "loss: 0.155123  [20201/60000]\n",
      "loss: 0.235218  [20301/60000]\n",
      "loss: 0.086605  [20401/60000]\n",
      "loss: 0.204981  [20501/60000]\n",
      "loss: 0.775884  [20601/60000]\n",
      "loss: 0.505006  [20701/60000]\n",
      "loss: 0.130415  [20801/60000]\n",
      "loss: 0.523590  [20901/60000]\n",
      "loss: 0.783907  [21001/60000]\n",
      "loss: 0.223042  [21101/60000]\n",
      "loss: 0.061202  [21201/60000]\n",
      "loss: 0.125405  [21301/60000]\n",
      "loss: 0.032683  [21401/60000]\n",
      "loss: 0.107774  [21501/60000]\n",
      "loss: 0.026528  [21601/60000]\n",
      "loss: 1.759921  [21701/60000]\n",
      "loss: 0.027047  [21801/60000]\n",
      "loss: 0.106784  [21901/60000]\n",
      "loss: 0.189741  [22001/60000]\n",
      "loss: 0.211663  [22101/60000]\n",
      "loss: 1.645426  [22201/60000]\n",
      "loss: 0.527530  [22301/60000]\n",
      "loss: 1.279164  [22401/60000]\n",
      "loss: 0.075763  [22501/60000]\n",
      "loss: 0.007150  [22601/60000]\n",
      "loss: 0.027772  [22701/60000]\n",
      "loss: 2.125586  [22801/60000]\n",
      "loss: 0.052291  [22901/60000]\n",
      "loss: 0.011704  [23001/60000]\n",
      "loss: 2.613809  [23101/60000]\n",
      "loss: 0.510291  [23201/60000]\n",
      "loss: 0.249734  [23301/60000]\n",
      "loss: 0.845130  [23401/60000]\n",
      "loss: 0.084477  [23501/60000]\n",
      "loss: 0.126066  [23601/60000]\n",
      "loss: 0.497894  [23701/60000]\n",
      "loss: 0.024846  [23801/60000]\n",
      "loss: 0.135324  [23901/60000]\n",
      "loss: 0.214129  [24001/60000]\n",
      "loss: 0.062007  [24101/60000]\n",
      "loss: 0.043679  [24201/60000]\n",
      "loss: 0.977866  [24301/60000]\n",
      "loss: 0.009909  [24401/60000]\n",
      "loss: 0.032452  [24501/60000]\n",
      "loss: 0.084919  [24601/60000]\n",
      "loss: 0.003832  [24701/60000]\n",
      "loss: 0.062660  [24801/60000]\n",
      "loss: 0.165002  [24901/60000]\n",
      "loss: 1.446226  [25001/60000]\n",
      "loss: 0.416827  [25101/60000]\n",
      "loss: 0.946655  [25201/60000]\n",
      "loss: 0.267659  [25301/60000]\n",
      "loss: 0.566839  [25401/60000]\n",
      "loss: 0.007808  [25501/60000]\n",
      "loss: 0.043891  [25601/60000]\n",
      "loss: 0.085917  [25701/60000]\n",
      "loss: 6.303459  [25801/60000]\n",
      "loss: 1.025491  [25901/60000]\n",
      "loss: 0.262589  [26001/60000]\n",
      "loss: 0.246331  [26101/60000]\n",
      "loss: 0.109916  [26201/60000]\n",
      "loss: 3.444045  [26301/60000]\n",
      "loss: 0.340872  [26401/60000]\n",
      "loss: 0.173933  [26501/60000]\n",
      "loss: 1.231728  [26601/60000]\n",
      "loss: 0.216234  [26701/60000]\n",
      "loss: 0.006302  [26801/60000]\n",
      "loss: 2.761884  [26901/60000]\n",
      "loss: 0.837122  [27001/60000]\n",
      "loss: 1.749321  [27101/60000]\n",
      "loss: 0.413925  [27201/60000]\n",
      "loss: 0.040900  [27301/60000]\n",
      "loss: 0.050511  [27401/60000]\n",
      "loss: 0.118071  [27501/60000]\n",
      "loss: 0.026188  [27601/60000]\n",
      "loss: 0.674635  [27701/60000]\n",
      "loss: 0.197456  [27801/60000]\n",
      "loss: 0.143213  [27901/60000]\n",
      "loss: 0.044474  [28001/60000]\n",
      "loss: 0.069043  [28101/60000]\n",
      "loss: 1.080810  [28201/60000]\n",
      "loss: 0.044863  [28301/60000]\n",
      "loss: 0.019816  [28401/60000]\n",
      "loss: 0.156770  [28501/60000]\n",
      "loss: 0.315144  [28601/60000]\n",
      "loss: 0.204154  [28701/60000]\n",
      "loss: 0.164597  [28801/60000]\n",
      "loss: 0.009835  [28901/60000]\n",
      "loss: 0.127537  [29001/60000]\n",
      "loss: 0.460924  [29101/60000]\n",
      "loss: 0.041019  [29201/60000]\n",
      "loss: 0.874928  [29301/60000]\n",
      "loss: 1.739452  [29401/60000]\n",
      "loss: 0.084409  [29501/60000]\n",
      "loss: 0.182139  [29601/60000]\n",
      "loss: 0.417843  [29701/60000]\n",
      "loss: 1.148804  [29801/60000]\n",
      "loss: 1.453968  [29901/60000]\n",
      "loss: 0.009463  [30001/60000]\n",
      "loss: 0.191591  [30101/60000]\n",
      "loss: 0.119365  [30201/60000]\n",
      "loss: 0.008358  [30301/60000]\n",
      "loss: 0.230499  [30401/60000]\n",
      "loss: 0.327186  [30501/60000]\n",
      "loss: 2.514298  [30601/60000]\n",
      "loss: 0.114859  [30701/60000]\n",
      "loss: 0.200156  [30801/60000]\n",
      "loss: 6.452875  [30901/60000]\n",
      "loss: 1.633116  [31001/60000]\n",
      "loss: 0.442812  [31101/60000]\n",
      "loss: 4.721250  [31201/60000]\n",
      "loss: 0.275067  [31301/60000]\n",
      "loss: 0.582086  [31401/60000]\n",
      "loss: 1.031687  [31501/60000]\n",
      "loss: 3.335533  [31601/60000]\n",
      "loss: 0.432204  [31701/60000]\n",
      "loss: 0.543015  [31801/60000]\n",
      "loss: 0.401230  [31901/60000]\n",
      "loss: 0.118802  [32001/60000]\n",
      "loss: 0.497567  [32101/60000]\n",
      "loss: 0.120478  [32201/60000]\n",
      "loss: 0.046661  [32301/60000]\n",
      "loss: 0.617326  [32401/60000]\n",
      "loss: 0.150419  [32501/60000]\n",
      "loss: 0.101139  [32601/60000]\n",
      "loss: 0.020465  [32701/60000]\n",
      "loss: 0.016442  [32801/60000]\n",
      "loss: 0.085613  [32901/60000]\n",
      "loss: 1.219688  [33001/60000]\n",
      "loss: 0.001823  [33101/60000]\n",
      "loss: 0.756598  [33201/60000]\n",
      "loss: 0.262377  [33301/60000]\n",
      "loss: 0.232624  [33401/60000]\n",
      "loss: 0.185272  [33501/60000]\n",
      "loss: 0.063661  [33601/60000]\n",
      "loss: 0.190920  [33701/60000]\n",
      "loss: 0.019274  [33801/60000]\n",
      "loss: 0.410418  [33901/60000]\n",
      "loss: 0.354783  [34001/60000]\n",
      "loss: 0.274955  [34101/60000]\n",
      "loss: 0.024829  [34201/60000]\n",
      "loss: 0.640410  [34301/60000]\n",
      "loss: 0.225445  [34401/60000]\n",
      "loss: 4.404297  [34501/60000]\n",
      "loss: 0.398376  [34601/60000]\n",
      "loss: 0.554557  [34701/60000]\n",
      "loss: 4.014215  [34801/60000]\n",
      "loss: 0.181451  [34901/60000]\n",
      "loss: 0.012834  [35001/60000]\n",
      "loss: 0.887768  [35101/60000]\n",
      "loss: 0.032250  [35201/60000]\n",
      "loss: 0.420194  [35301/60000]\n",
      "loss: 0.000753  [35401/60000]\n",
      "loss: 1.011065  [35501/60000]\n",
      "loss: 0.018733  [35601/60000]\n",
      "loss: 0.000579  [35701/60000]\n",
      "loss: 0.003234  [35801/60000]\n",
      "loss: 0.010237  [35901/60000]\n",
      "loss: 0.871888  [36001/60000]\n",
      "loss: 0.340875  [36101/60000]\n",
      "loss: 0.217931  [36201/60000]\n",
      "loss: 0.262030  [36301/60000]\n",
      "loss: 0.065737  [36401/60000]\n",
      "loss: 0.314265  [36501/60000]\n",
      "loss: 0.191699  [36601/60000]\n",
      "loss: 0.007893  [36701/60000]\n",
      "loss: 0.318620  [36801/60000]\n",
      "loss: 0.502133  [36901/60000]\n",
      "loss: 0.046133  [37001/60000]\n",
      "loss: 2.364560  [37101/60000]\n",
      "loss: 0.004643  [37201/60000]\n",
      "loss: 0.145342  [37301/60000]\n",
      "loss: 0.528786  [37401/60000]\n",
      "loss: 0.017860  [37501/60000]\n",
      "loss: 0.211907  [37601/60000]\n",
      "loss: 0.028289  [37701/60000]\n",
      "loss: 0.044465  [37801/60000]\n",
      "loss: 0.168904  [37901/60000]\n",
      "loss: 0.169913  [38001/60000]\n",
      "loss: 0.009340  [38101/60000]\n",
      "loss: 0.000145  [38201/60000]\n",
      "loss: 0.843483  [38301/60000]\n",
      "loss: 0.013087  [38401/60000]\n",
      "loss: 0.131729  [38501/60000]\n",
      "loss: 0.268556  [38601/60000]\n",
      "loss: 1.297548  [38701/60000]\n",
      "loss: 1.035113  [38801/60000]\n",
      "loss: 0.048775  [38901/60000]\n",
      "loss: 0.914265  [39001/60000]\n",
      "loss: 0.246040  [39101/60000]\n",
      "loss: 0.001138  [39201/60000]\n",
      "loss: 0.020113  [39301/60000]\n",
      "loss: 0.190397  [39401/60000]\n",
      "loss: 0.398572  [39501/60000]\n",
      "loss: 3.296789  [39601/60000]\n",
      "loss: 2.192066  [39701/60000]\n",
      "loss: 0.005104  [39801/60000]\n",
      "loss: 0.064794  [39901/60000]\n",
      "loss: 0.003150  [40001/60000]\n",
      "loss: 0.247816  [40101/60000]\n",
      "loss: 0.022065  [40201/60000]\n",
      "loss: 0.947939  [40301/60000]\n",
      "loss: 0.001604  [40401/60000]\n",
      "loss: 0.038472  [40501/60000]\n",
      "loss: 0.083462  [40601/60000]\n",
      "loss: 0.447091  [40701/60000]\n",
      "loss: 0.251032  [40801/60000]\n",
      "loss: 0.182339  [40901/60000]\n",
      "loss: 0.108965  [41001/60000]\n",
      "loss: 0.102858  [41101/60000]\n",
      "loss: 0.994507  [41201/60000]\n",
      "loss: 0.096261  [41301/60000]\n",
      "loss: 0.825883  [41401/60000]\n",
      "loss: 0.822818  [41501/60000]\n",
      "loss: 0.031030  [41601/60000]\n",
      "loss: 1.307487  [41701/60000]\n",
      "loss: 0.011643  [41801/60000]\n",
      "loss: 1.902378  [41901/60000]\n",
      "loss: 0.012286  [42001/60000]\n",
      "loss: 0.083896  [42101/60000]\n",
      "loss: 0.123585  [42201/60000]\n",
      "loss: 0.001958  [42301/60000]\n",
      "loss: 0.591511  [42401/60000]\n",
      "loss: 0.056060  [42501/60000]\n",
      "loss: 0.110542  [42601/60000]\n",
      "loss: 0.016023  [42701/60000]\n",
      "loss: 0.142985  [42801/60000]\n",
      "loss: 0.030802  [42901/60000]\n",
      "loss: 0.540199  [43001/60000]\n",
      "loss: 0.213548  [43101/60000]\n",
      "loss: 0.635748  [43201/60000]\n",
      "loss: 0.028653  [43301/60000]\n",
      "loss: 0.246100  [43401/60000]\n",
      "loss: 0.074222  [43501/60000]\n",
      "loss: 0.251306  [43601/60000]\n",
      "loss: 0.141965  [43701/60000]\n",
      "loss: 0.135828  [43801/60000]\n",
      "loss: 0.432037  [43901/60000]\n",
      "loss: 0.012869  [44001/60000]\n",
      "loss: 1.248521  [44101/60000]\n",
      "loss: 0.474166  [44201/60000]\n",
      "loss: 1.003164  [44301/60000]\n",
      "loss: 0.032585  [44401/60000]\n",
      "loss: 0.361990  [44501/60000]\n",
      "loss: 0.187266  [44601/60000]\n",
      "loss: 0.003850  [44701/60000]\n",
      "loss: 0.628850  [44801/60000]\n",
      "loss: 0.005631  [44901/60000]\n",
      "loss: 0.250599  [45001/60000]\n",
      "loss: 0.044677  [45101/60000]\n",
      "loss: 0.002992  [45201/60000]\n",
      "loss: 0.152919  [45301/60000]\n",
      "loss: 0.014808  [45401/60000]\n",
      "loss: 0.449799  [45501/60000]\n",
      "loss: 0.113392  [45601/60000]\n",
      "loss: 0.356634  [45701/60000]\n",
      "loss: 5.209809  [45801/60000]\n",
      "loss: 0.056529  [45901/60000]\n",
      "loss: 0.247943  [46001/60000]\n",
      "loss: 0.258750  [46101/60000]\n",
      "loss: 0.819099  [46201/60000]\n",
      "loss: 4.006463  [46301/60000]\n",
      "loss: 0.223002  [46401/60000]\n",
      "loss: 0.005136  [46501/60000]\n",
      "loss: 0.278136  [46601/60000]\n",
      "loss: 0.238097  [46701/60000]\n",
      "loss: 0.284361  [46801/60000]\n",
      "loss: 0.010469  [46901/60000]\n",
      "loss: 0.030667  [47001/60000]\n",
      "loss: 1.103137  [47101/60000]\n",
      "loss: 0.058063  [47201/60000]\n",
      "loss: 0.082689  [47301/60000]\n",
      "loss: 0.040623  [47401/60000]\n",
      "loss: 0.052704  [47501/60000]\n",
      "loss: 2.553356  [47601/60000]\n",
      "loss: 0.004194  [47701/60000]\n",
      "loss: 0.001726  [47801/60000]\n",
      "loss: 0.001224  [47901/60000]\n",
      "loss: 0.111910  [48001/60000]\n",
      "loss: 0.001068  [48101/60000]\n",
      "loss: 0.368843  [48201/60000]\n",
      "loss: 0.092433  [48301/60000]\n",
      "loss: 0.081612  [48401/60000]\n",
      "loss: 0.088785  [48501/60000]\n",
      "loss: 0.013561  [48601/60000]\n",
      "loss: 0.096016  [48701/60000]\n",
      "loss: 0.971909  [48801/60000]\n",
      "loss: 0.217232  [48901/60000]\n",
      "loss: 0.049927  [49001/60000]\n",
      "loss: 0.103786  [49101/60000]\n",
      "loss: 0.462370  [49201/60000]\n",
      "loss: 2.688341  [49301/60000]\n",
      "loss: 0.441402  [49401/60000]\n",
      "loss: 0.522715  [49501/60000]\n",
      "loss: 0.029604  [49601/60000]\n",
      "loss: 0.246946  [49701/60000]\n",
      "loss: 0.133382  [49801/60000]\n",
      "loss: 0.114068  [49901/60000]\n",
      "loss: 0.182200  [50001/60000]\n",
      "loss: 0.007731  [50101/60000]\n",
      "loss: 0.003480  [50201/60000]\n",
      "loss: 0.047150  [50301/60000]\n",
      "loss: 0.034638  [50401/60000]\n",
      "loss: 0.173972  [50501/60000]\n",
      "loss: 0.006302  [50601/60000]\n",
      "loss: 0.909643  [50701/60000]\n",
      "loss: 0.053938  [50801/60000]\n",
      "loss: 0.035798  [50901/60000]\n",
      "loss: 0.019004  [51001/60000]\n",
      "loss: 0.305032  [51101/60000]\n",
      "loss: 2.041286  [51201/60000]\n",
      "loss: 3.506476  [51301/60000]\n",
      "loss: 0.124865  [51401/60000]\n",
      "loss: 0.015129  [51501/60000]\n",
      "loss: 2.200779  [51601/60000]\n",
      "loss: 0.018729  [51701/60000]\n",
      "loss: 0.094399  [51801/60000]\n",
      "loss: 0.020081  [51901/60000]\n",
      "loss: 0.111335  [52001/60000]\n",
      "loss: 0.758830  [52101/60000]\n",
      "loss: 0.059178  [52201/60000]\n",
      "loss: 0.003194  [52301/60000]\n",
      "loss: 0.081903  [52401/60000]\n",
      "loss: 0.088870  [52501/60000]\n",
      "loss: 0.300502  [52601/60000]\n",
      "loss: 0.031147  [52701/60000]\n",
      "loss: 3.373211  [52801/60000]\n",
      "loss: 0.202900  [52901/60000]\n",
      "loss: 0.025411  [53001/60000]\n",
      "loss: 0.012759  [53101/60000]\n",
      "loss: 0.061973  [53201/60000]\n",
      "loss: 0.389593  [53301/60000]\n",
      "loss: 0.004588  [53401/60000]\n",
      "loss: 0.079305  [53501/60000]\n",
      "loss: 0.276277  [53601/60000]\n",
      "loss: 0.079897  [53701/60000]\n",
      "loss: 0.036088  [53801/60000]\n",
      "loss: 0.846370  [53901/60000]\n",
      "loss: 1.068782  [54001/60000]\n",
      "loss: 0.463792  [54101/60000]\n",
      "loss: 0.219128  [54201/60000]\n",
      "loss: 0.118960  [54301/60000]\n",
      "loss: 0.320792  [54401/60000]\n",
      "loss: 0.016212  [54501/60000]\n",
      "loss: 0.079822  [54601/60000]\n",
      "loss: 0.013590  [54701/60000]\n",
      "loss: 0.006823  [54801/60000]\n",
      "loss: 0.017749  [54901/60000]\n",
      "loss: 0.038125  [55001/60000]\n",
      "loss: 0.038830  [55101/60000]\n",
      "loss: 0.041116  [55201/60000]\n",
      "loss: 0.001109  [55301/60000]\n",
      "loss: 1.647811  [55401/60000]\n",
      "loss: 0.060401  [55501/60000]\n",
      "loss: 0.113123  [55601/60000]\n",
      "loss: 0.006343  [55701/60000]\n",
      "loss: 0.062247  [55801/60000]\n",
      "loss: 0.009001  [55901/60000]\n",
      "loss: 0.018709  [56001/60000]\n",
      "loss: 0.004913  [56101/60000]\n",
      "loss: 0.025452  [56201/60000]\n",
      "loss: 3.250094  [56301/60000]\n",
      "loss: 0.069921  [56401/60000]\n",
      "loss: 0.011417  [56501/60000]\n",
      "loss: 2.186286  [56601/60000]\n",
      "loss: 0.046636  [56701/60000]\n",
      "loss: 3.517839  [56801/60000]\n",
      "loss: 0.014851  [56901/60000]\n",
      "loss: 0.000066  [57001/60000]\n",
      "loss: 0.087373  [57101/60000]\n",
      "loss: 0.166557  [57201/60000]\n",
      "loss: 0.015983  [57301/60000]\n",
      "loss: 0.242425  [57401/60000]\n",
      "loss: 0.000513  [57501/60000]\n",
      "loss: 0.006008  [57601/60000]\n",
      "loss: 0.034379  [57701/60000]\n",
      "loss: 0.092712  [57801/60000]\n",
      "loss: 0.134931  [57901/60000]\n",
      "loss: 0.281889  [58001/60000]\n",
      "loss: 0.585253  [58101/60000]\n",
      "loss: 0.015936  [58201/60000]\n",
      "loss: 0.015861  [58301/60000]\n",
      "loss: 0.040192  [58401/60000]\n",
      "loss: 0.010722  [58501/60000]\n",
      "loss: 0.019501  [58601/60000]\n",
      "loss: 0.103393  [58701/60000]\n",
      "loss: 0.142300  [58801/60000]\n",
      "loss: 0.309931  [58901/60000]\n",
      "loss: 0.012057  [59001/60000]\n",
      "loss: 0.024333  [59101/60000]\n",
      "loss: 0.023852  [59201/60000]\n",
      "loss: 0.005406  [59301/60000]\n",
      "loss: 3.666957  [59401/60000]\n",
      "loss: 0.019756  [59501/60000]\n",
      "loss: 0.139096  [59601/60000]\n",
      "loss: 0.002019  [59701/60000]\n",
      "loss: 0.051596  [59801/60000]\n",
      "loss: 0.028424  [59901/60000]\n",
      "45.88908624649048\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dl, valid_dl = get_data(training_data, test_data, bs)\n",
    "model, loss_func, opt = get_model()\n",
    "# Move the model to the device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "start_time = time.time()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "stop_time = time.time()\n",
    "print(stop_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "905f6503-ac2d-4447-8cf8-eecd9f63a347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "803aeb0c-102a-4caa-b716-8017a4189658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1032554-6291-45d5-9cac-a04458388f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug = iter(valid_dl)\n",
    "next(debug)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf51fb1c-1f0e-488c-94a1-914a9ef1cd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: 10.9\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m     stop_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m     times[i] \u001b[38;5;241m=\u001b[39m stop_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(times\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "start_time = time.time()\n",
    "times = np.zeros(10)\n",
    "for i in range(10):\n",
    "    for batch, (xb, yb) in enumerate(valid_dl):\n",
    "        pred = model(xb).argmax()\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"Complete: {batch/len(valid_dl)+i}\", end=\"\\r\")\n",
    "        if pred == yb:\n",
    "            correct +=1\n",
    "    stop_time = time.time()\n",
    "    times[i] = stop_time - start_time\n",
    "print(np.diff(times).mean())\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57d767d4-31d9-4ec7-a3b8-d4928f6cb20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7625641557905407"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(times).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a4757017-e489-4686-a61f-e598e3321466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "045ca23c-8ab6-4292-a4c5-92fb540f947e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.reshape(60000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "645607b7-358a-4bb5-834b-eed941172f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5a6e2735-4deb-484b-b6db-6577cdf243d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img in training_data.data.reshape(60000,784):\n",
    "    plt.figure()\n",
    "    plt.imshow(img.reshape(28,28))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "effa5e7d-ae77-4c22-b942-55b2a7e6d0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(int(training_data.targets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "47167eeb-7c7c-4585-ba30-a8c7420c62bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "concat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mconcat(training_data\u001b[38;5;241m.\u001b[39mtargets, training_data\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m60000\u001b[39m, \u001b[38;5;241m784\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: concat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "torch.concat(training_data.targets, training_data.data.reshape(60000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41c75d1-b0b8-4930-99d0-02a11d6f85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_h.h', 'w') as f:\n",
    "    # Write the initial static declaration\n",
    "    f.write('#ifndef OUTPUT_H\\n#define OUTPUT_H\\n\\n')\n",
    "    f.write(f'static double img[{10000}][{784}] = {{\\n')\n",
    "    \n",
    "    for i,img in enumerate(test_data.data.reshape(10000, 784)[:-1]):  # Flatten images except for the last one\n",
    "        img_values = ','.join(map(str, img.tolist()))  # Convert tensor to comma-separated string\n",
    "        f.write(\"{\"+f\"{img_values}\"+\"},\\n\")  # Write each image as a line in the text file\n",
    "    img_values = ','.join(map(str, test_data.data.reshape(10000, 784)[-1].tolist()))  # Convert tensor to comma-separated string\n",
    "    f.write(\"{\"+f\"{img_values}\"+\"}\\n\")  # Write each image as a line in the text file\n",
    "        \n",
    "    # Close the matrix declaration\n",
    "    f.write('};\\n\\n')\n",
    "    f.write('#endif // OUTPUT_H\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0f876d-d088-41b0-904e-5837689bd968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10000, 784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frmt = training_data.data.reshape(6,10000, 784)\n",
    "train_frmt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75ff293b-8250-4d32-a9b3-616c830e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,train_set in enumerate(train_frmt):\n",
    "    with open(f'train_{i}_h.h', 'w') as f:\n",
    "        # Write the initial static declaration\n",
    "        f.write(f'#ifndef TRAIN_{i}_H\\n#define TRAIN_{i}_H\\n\\n')\n",
    "        f.write(f'static double img_set_{i}[{10000}][{784}] = {{\\n')\n",
    "        \n",
    "        for i,img in enumerate(train_set[:-1]):  # Flatten images except for the last one\n",
    "            img_values = ','.join(map(str, img.tolist()))  # Convert tensor to comma-separated string\n",
    "            f.write(\"{\"+f\"{img_values}\"+\"},\\n\")  # Write each image as a line in the text file\n",
    "        img_values = ','.join(map(str, train_set[-1].tolist()))  # Convert tensor to comma-separated string\n",
    "        f.write(\"{\"+f\"{img_values}\"+\"}\\n\")  # Write each image as a line in the text file\n",
    "            \n",
    "        # Close the matrix declaration\n",
    "        f.write('};\\n\\n')\n",
    "        f.write('#endif // TRAIN_{I}_H\\n')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
